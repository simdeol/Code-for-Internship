{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c84f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!java -mx4g -cp \"C:\\Users\\simra\\Downloads\\stanford-corenlp-4.4.0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc7c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip https://nlp.stanford.edu/software/stanford-english-corenlp-2018-10-05-models.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb24eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd \"C:\\Users\\simra\\Downloads\\stanford-corenlp-4.4.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2836306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stanfordnlp\n",
    "#stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
    "#nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d21085d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graph4nlp\n",
      "  Downloading graph4nlp-0.5.5-py2.py3-none-any.whl (32.2 MB)\n",
      "Collecting pythonds\n",
      "  Downloading pythonds-1.2.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: networkx>=2.5 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from graph4nlp) (2.6.2)\n",
      "Requirement already satisfied: scipy>=1.5.2 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from graph4nlp) (1.7.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from graph4nlp) (5.4.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\simra\\appdata\\roaming\\python\\python39\\site-packages (from graph4nlp) (4.19.4)\n",
      "Requirement already satisfied: scikit-learn>=0.23.2 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from graph4nlp) (0.24.2)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from graph4nlp) (4.64.0)\n",
      "Collecting stanfordcorenlp\n",
      "  Downloading stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: nltk>=3.5 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from graph4nlp) (3.6.2)\n",
      "Collecting dgl>=0.4\n",
      "  Downloading dgl-0.6.1-cp39-cp39-win_amd64.whl (3.1 MB)\n",
      "Collecting ogb\n",
      "  Downloading ogb-1.3.3-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from dgl>=0.4->graph4nlp) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from dgl>=0.4->graph4nlp) (1.22.3)\n",
      "Requirement already satisfied: regex in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from nltk>=3.5->graph4nlp) (2021.8.21)\n",
      "Requirement already satisfied: click in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from nltk>=3.5->graph4nlp) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from nltk>=3.5->graph4nlp) (1.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from requests>=2.19.0->dgl>=0.4->graph4nlp) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from requests>=2.19.0->dgl>=0.4->graph4nlp) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from requests>=2.19.0->dgl>=0.4->graph4nlp) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from requests>=2.19.0->dgl>=0.4->graph4nlp) (2021.10.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from scikit-learn>=0.23.2->graph4nlp) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from tqdm>=4.29.0->graph4nlp) (0.4.4)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from ogb->graph4nlp) (1.16.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from ogb->graph4nlp) (1.3.1)\n",
      "Collecting outdated>=0.2.0\n",
      "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from ogb->graph4nlp) (1.9.0)\n",
      "Collecting littleutils\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from pandas>=0.24.0->ogb->graph4nlp) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from pandas>=0.24.0->ogb->graph4nlp) (2022.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from torch>=1.6.0->ogb->graph4nlp) (4.1.1)\n",
      "Collecting psutil\n",
      "  Downloading psutil-5.9.1-cp39-cp39-win_amd64.whl (245 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from transformers->graph4nlp) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from transformers->graph4nlp) (3.0.12)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from transformers->graph4nlp) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\simra\\appdata\\roaming\\python\\python39\\site-packages (from transformers->graph4nlp) (0.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from packaging>=20.0->transformers->graph4nlp) (3.0.4)\n",
      "Building wheels for collected packages: littleutils\n",
      "  Building wheel for littleutils (setup.py): started\n",
      "  Building wheel for littleutils (setup.py): finished with status 'done'\n",
      "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=c94f6b6d46c54d893ec07c1bf4101def99371835168fe1e0b699ac2ee5877986\n",
      "  Stored in directory: c:\\users\\simra\\appdata\\local\\pip\\cache\\wheels\\04\\bb\\0d\\2d02ec45f29c48d6192476bfb59c5a0e64b605e7212374dd15\n",
      "Successfully built littleutils\n",
      "Installing collected packages: littleutils, psutil, outdated, stanfordcorenlp, pythonds, ogb, dgl, graph4nlp\n",
      "Successfully installed dgl-0.6.1 graph4nlp-0.5.5 littleutils-0.2.2 ogb-1.3.3 outdated-0.2.1 psutil-5.9.1 pythonds-1.2.1 stanfordcorenlp-3.9.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install graph4nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe4b59c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.8.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts convert-caffe2-to-onnx.exe and convert-onnx-to-caffe2.exe are installed in 'C:\\Users\\simra\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\n",
      "pysentimiento 0.3.2 requires torch<2.0.0,>=1.9.0, but you have torch 1.8.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading torch-1.8.0-cp39-cp39-win_amd64.whl (190.5 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from torch==1.8.0) (4.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from torch==1.8.0) (1.22.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.8.0\n",
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from torchtext==0.9.0) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from torchtext==0.9.0) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from torchtext==0.9.0) (1.22.3)\n",
      "Requirement already satisfied: torch==1.8.0 in c:\\users\\simra\\appdata\\roaming\\python\\python39\\site-packages (from torchtext==0.9.0) (1.8.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from torch==1.8.0->torchtext==0.9.0) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from requests->torchtext==0.9.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from requests->torchtext==0.9.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from requests->torchtext==0.9.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from requests->torchtext==0.9.0) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from tqdm->torchtext==0.9.0) (0.4.4)\n",
      "Installing collected packages: torchtext\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.12.0\n",
      "    Uninstalling torchtext-0.12.0:\n",
      "      Successfully uninstalled torchtext-0.12.0\n",
      "Successfully installed torchtext-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.8.0 --user\n",
    "!pip install torchtext==0.9.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48f5521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.0\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torchtext; print(torchtext.__version__)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6006564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import graph4nlp\n",
    "import torch\n",
    "import torchtext\n",
    "import nltk\n",
    "from graph4nlp.pytorch.modules.graph_construction.dependency_graph_construction import DependencyBasedGraphConstruction\n",
    "from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa9047ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Edges': [(1, 0), (4, 2), (4, 3), (1, 4), (1, 5)]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphdata.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12bc33c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_feat': None, 'node_emb': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphdata.node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97e7dd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<graph4nlp.pytorch.data.views.NodeView at 0x1adb8b6c4c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphdata.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd6934ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simra\\Downloads\n"
     ]
    }
   ],
   "source": [
    "cd \"C:\\Users\\simra\\Downloads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9931f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph4nlp.pytorch.data.dataset import Text2LabelDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58edb2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimileDataset(Text2LabelDataset):\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return {\"train\":\"train.txt\",\"test\":\"test.txt\",\"val\":\"val.txt\"}\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return {\"vocab\": \"vocab.pt\", \"data\": \"data.pt\", \"label\": \"label.pt\"}\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        topology_subdir,\n",
    "        graph_name,\n",
    "        static_or_dynamic=\"static\",\n",
    "        topology_builder=None,\n",
    "        merge_strategy=None, edge_strategy=None,\n",
    "        dynamic_init_graph_name=None,\n",
    "        dynamic_init_topology_builder=None,\n",
    "        dynamic_init_topology_aux_args=None,\n",
    "        port=9000,\n",
    "        timeout=15000,\n",
    "        tokenizer=nltk.RegexpTokenizer(\" \", gaps=True).tokenize,\n",
    "        word_emb_size=None,\n",
    "                    #self.topology_builder,\n",
    "                    #self.static_or_dynamic,\n",
    "                    #self.graph_name,\n",
    "                    #self.dynamic_init_topology_builder,\n",
    "                    #self.merge_strategy,\n",
    "                    #self.edge_strategy,\n",
    "                    #self.dynamic_init_topology_aux_args,\n",
    "                    #self.lower_case\n",
    "                    #elf.tokenizer,\n",
    "        pretrained_word_emb_name=\"840B\",\n",
    "        pretrained_word_emb_url=None,\n",
    "        pretrained_word_emb_cache_dir=r\"C:\\Users\\simra\\Downloads\",\n",
    "        max_word_vocab_size=300,\n",
    "        min_word_vocab_freq=1,\n",
    "        **kwargs\n",
    "    ):\n",
    "            super(SimileDataset, self).__init__(\n",
    "                graph_name,\n",
    "                root_dir=root_dir,\n",
    "                static_or_dynamic=static_or_dynamic,\n",
    "                topology_builder=topology_builder,\n",
    "                topology_subdir=topology_subdir,\n",
    "                dynamic_init_graph_name=dynamic_init_graph_name,\n",
    "                dynamic_init_topology_builder=dynamic_init_topology_builder,\n",
    "                dynamic_init_topology_aux_args=dynamic_init_topology_aux_args,\n",
    "                merge_strategy=None, edge_strategy=None,\n",
    "                pretrained_word_emb_name=pretrained_word_emb_name,\n",
    "                pretrained_word_emb_url=pretrained_word_emb_url,\n",
    "                pretrained_word_emb_cache_dir=pretrained_word_emb_cache_dir,\n",
    "                max_word_vocab_size=max_word_vocab_size,\n",
    "                min_word_vocab_freq=min_word_vocab_freq,\n",
    "                tokenizer=tokenizer,\n",
    "                word_emb_size=word_emb_size,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fec8b12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████▉| 2196016/2196017 [04:31<00:00, 8079.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained word embeddings hit ratio: 0.930921052631579\n",
      "Using pretrained word embeddings\n",
      "[ Initialized word embeddings: (304, 300) ]\n",
      "Saving vocab model to C:\\Users\\simra\\Downloads\\processed_files_1\\vocab.pt\n",
      "Saving label mappings to C:\\Users\\simra\\Downloads\\processed_files_1\\label.pt\n"
     ]
    }
   ],
   "source": [
    "dataset=SimileDataset(graph_name='dependency',\n",
    "        root_dir=r\"C:\\Users\\simra\\Downloads\\processed_files_1\",\n",
    "        static_or_dynamic= \"static\",\n",
    "        topology_builder=DependencyBasedGraphConstruction,\n",
    "        topology_subdir = r\"C:\\Users\\simra\\Downloads\\processed_files_1\",\n",
    "        dynamic_init_graph_name ='dependency',\n",
    "        dynamic_init_topology_builder=None,\n",
    "        dynamic_init_topology_aux_args=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63298d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "764a7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset.train, batch_size=32, shuffle=True,\n",
    "                                           num_workers=0,\n",
    "                                           collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3118e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1f263b555e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be7852ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "            dataset.val,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=dataset.collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c2c45b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset.test, batch_size=32, shuffle=False,\n",
    "                                          num_workers=0,\n",
    "                                          collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "149dac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_model = dataset.label_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd3d4846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2798"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0128f093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93718e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1313"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e5d703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab,label_model,config):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.config = config\n",
    "        self.vocab_model = vocab\n",
    "        self.label_model = label_model\n",
    "        self.graph_name = 'dependency'\n",
    "        assert not (\n",
    "            self.graph_name in (\"node_emb\", \"node_emb_refined\") and config[\"gnn\"] == \"gat\"\n",
    "        ), \"dynamic graph construction does not support GAT\"\n",
    "\n",
    "        embedding_style = {\n",
    "            \"single_token_item\": True if self.graph_name != \"ie\" else False,\n",
    "            \"emb_strategy\": config.get(\"emb_strategy\", \"w2v_bilstm\"),\n",
    "            \"num_rnn_layers\": 1,\n",
    "            \"bert_model_name\": config.get(\"bert_model_name\", \"bert-base-uncased\"),\n",
    "            \"bert_lower_case\": True,\n",
    "        }\n",
    "\n",
    "        self.graph_initializer = GraphEmbeddingInitialization(\n",
    "            word_vocab=self.vocab_model.in_word_vocab,\n",
    "            embedding_style=embedding_style,\n",
    "            hidden_size=config[\"num_hidden\"],\n",
    "            word_dropout=config[\"word_dropout\"],\n",
    "            rnn_dropout=config[\"rnn_dropout\"],\n",
    "            fix_word_emb=not config[\"no_fix_word_emb\"],\n",
    "            fix_bert_emb=not config.get(\"no_fix_bert_emb\", False),\n",
    "        )\n",
    "\n",
    "        use_edge_weight = False\n",
    "        if self.graph_name == \"node_emb\":\n",
    "            self.graph_topology = NodeEmbeddingBasedGraphConstruction(\n",
    "                sim_metric_type=config[\"gl_metric_type\"],\n",
    "                num_heads=config[\"gl_num_heads\"],\n",
    "                top_k_neigh=config[\"gl_top_k\"],\n",
    "                epsilon_neigh=config[\"gl_epsilon\"],\n",
    "                smoothness_ratio=config[\"gl_smoothness_ratio\"],\n",
    "                connectivity_ratio=config[\"gl_connectivity_ratio\"],\n",
    "                sparsity_ratio=config[\"gl_sparsity_ratio\"],\n",
    "                input_size=config[\"num_hidden\"],\n",
    "                hidden_size=config[\"gl_num_hidden\"],\n",
    "            )\n",
    "            use_edge_weight = True\n",
    "        elif self.graph_name == \"node_emb_refined\":\n",
    "            self.graph_topology = NodeEmbeddingBasedRefinedGraphConstruction(\n",
    "                config[\"init_adj_alpha\"],\n",
    "                sim_metric_type=config[\"gl_metric_type\"],\n",
    "                num_heads=config[\"gl_num_heads\"],\n",
    "                top_k_neigh=config[\"gl_top_k\"],\n",
    "                epsilon_neigh=config[\"gl_epsilon\"],\n",
    "                smoothness_ratio=config[\"gl_smoothness_ratio\"],\n",
    "                connectivity_ratio=config[\"gl_connectivity_ratio\"],\n",
    "                sparsity_ratio=config[\"gl_sparsity_ratio\"],\n",
    "                input_size=config[\"num_hidden\"],\n",
    "                hidden_size=config[\"gl_num_hidden\"],\n",
    "            )\n",
    "            use_edge_weight = True\n",
    "\n",
    "        if \"w2v\" in self.graph_initializer.embedding_layer.word_emb_layers:\n",
    "            self.word_emb = self.graph_initializer.embedding_layer.word_emb_layers[\n",
    "                \"w2v\"\n",
    "            ].word_emb_layer\n",
    "        else:\n",
    "            self.word_emb = WordEmbedding(\n",
    "                self.vocab_model.in_word_vocab.embeddings.shape[0],\n",
    "                self.vocab_model.in_word_vocab.embeddings.shape[1],\n",
    "                pretrained_word_emb=self.vocab_model.in_word_vocab.embeddings,\n",
    "                fix_emb=not config[\"no_fix_word_emb\"],\n",
    "            ).word_emb_layer\n",
    "\n",
    "        if config[\"gnn\"] == \"gat\":\n",
    "            heads = [config[\"gat_num_heads\"]] * (config[\"gnn_num_layers\"] - 1) + [\n",
    "                config[\"gat_num_out_heads\"]\n",
    "            ]\n",
    "            self.gnn = GAT(\n",
    "                config[\"gnn_num_layers\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"num_hidden\"],\n",
    "                heads,\n",
    "                direction_option=config[\"gnn_direction_option\"],\n",
    "                feat_drop=config[\"gnn_dropout\"],\n",
    "                attn_drop=config[\"gat_attn_dropout\"],\n",
    "                negative_slope=config[\"gat_negative_slope\"],\n",
    "                residual=config[\"gat_residual\"],\n",
    "                activation=F.elu,\n",
    "                allow_zero_in_degree=True,\n",
    "            )\n",
    "        elif config[\"gnn\"] == \"graphsage\":\n",
    "            self.gnn = GraphSAGE(\n",
    "                config[\"gnn_num_layers\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"graphsage_aggreagte_type\"],\n",
    "                direction_option=config[\"gnn_direction_option\"],\n",
    "                feat_drop=config[\"gnn_dropout\"],\n",
    "                bias=True,\n",
    "                norm=None,\n",
    "                activation=F.relu,\n",
    "                use_edge_weight=use_edge_weight,\n",
    "            )\n",
    "        elif config[\"gnn\"] == \"ggnn\":\n",
    "            self.gnn = GGNN(\n",
    "                config[\"gnn_num_layers\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"num_hidden\"],\n",
    "                config[\"num_hidden\"],\n",
    "                feat_drop=config[\"gnn_dropout\"],\n",
    "                direction_option=config[\"gnn_direction_option\"],\n",
    "                bias=True,\n",
    "                use_edge_weight=use_edge_weight,\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown gnn type: {}\".format(config[\"gnn\"]))\n",
    "\n",
    "        self.clf = FeedForwardNN(\n",
    "            2 * config[\"num_hidden\"]\n",
    "            if config[\"gnn_direction_option\"] == \"bi_sep\"\n",
    "            else config[\"num_hidden\"],\n",
    "            2,\n",
    "            [config[\"num_hidden\"]],\n",
    "            graph_pool_type=config[\"graph_pooling\"],\n",
    "            dim=config[\"num_hidden\"],\n",
    "            use_linear_proj=config[\"max_pool_linear_proj\"],\n",
    "        )\n",
    "\n",
    "        self.loss = GeneralLoss(\"CrossEntropy\")\n",
    "\n",
    "    def forward(self, graph_list, tgt=None, require_loss=True):\n",
    "        # graph embedding initialization\n",
    "        batch_gd = self.graph_initializer(graph_list)\n",
    "\n",
    "        # run dynamic graph construction if turned on\n",
    "        if hasattr(self, \"graph_topology\") and hasattr(self.graph_topology, \"dynamic_topology\"):\n",
    "            batch_gd = self.graph_topology.dynamic_topology(batch_gd)\n",
    "\n",
    "        # run GNN\n",
    "        self.gnn(batch_gd)\n",
    "\n",
    "        # run graph classifier\n",
    "        self.clf(batch_gd)\n",
    "        logits = batch_gd.graph_attributes[\"logits\"]\n",
    "\n",
    "        if require_loss:\n",
    "            loss = self.loss(logits, tgt)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def inference_forward(self, collate_data):\n",
    "        return self.forward(collate_data[\"graph_data\"], require_loss=False)\n",
    "\n",
    "    def post_process(self, logits, label_names):\n",
    "        logits_list = []\n",
    "\n",
    "        for idx in range(len(logits)):\n",
    "            logits_list.append(logits[idx].cpu().clone().numpy())\n",
    "\n",
    "        pred_tags = [label_names[pred.argmax()] for pred in logits_list]\n",
    "        return pred_tags\n",
    "\n",
    "    @classmethod\n",
    "    def load_checkpoint(cls, model_path):\n",
    "        \"\"\"The API to load the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_path : str\n",
    "            The saved model path.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Class\n",
    "        \"\"\"\n",
    "        return torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aa1b7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = r\"C:\\Users\\simra\\Downloads\\simile.yaml\"\n",
    "config = yaml.load(open(config_file, 'r'), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c4c491a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"device\"]=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3e75046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"num_classes\"]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3259236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Fix word embeddings ]\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier(dataset.vocab_model,dataset.label_model,config).to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0cec70ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(parameters, lr=config['lr'])\n",
    "#stopper = EarlyStopping(os.path.join(config['out_dir'], Constants._SAVED_WEIGHTS_FILE), patience=config['patience'])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=config['lr_reduce_factor'], \\\n",
    "    patience=config['lr_patience'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9b0008ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = Accuracy(['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4edd93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "        dur = []\n",
    "        for epoch in range(config['epochs']):\n",
    "            model.train()\n",
    "            train_loss = []\n",
    "            train_acc = []\n",
    "            val_loss=[]\n",
    "            t0 = time.time()\n",
    "            for i, data in enumerate(train_dataloader):\n",
    "                tgt = data['tgt_tensor'].to(config['device'])\n",
    "                data['graph_data'] = data['graph_data'].to(config['device'])\n",
    "                logits, loss = model(data['graph_data'], tgt, require_loss=True)\n",
    "\n",
    "                # add graph regularization loss if available\n",
    "                if data['graph_data'].graph_attributes.get('graph_reg', None) is not None:\n",
    "                    loss = loss + data['graph_data'].graph_attributes['graph_reg']\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                pred = torch.max(logits, dim=-1)[1].cpu()\n",
    "                train_acc.append(metric.calculate_scores(ground_truth=tgt.cpu(), predict=pred.cpu(), zero_division=0)[0])\n",
    "                dur.append(time.time() - t0)\n",
    "            val_acc,val_loss= evaluate_val(val_dataloader)\n",
    "            scheduler.step(val_acc)\n",
    "            print('Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} | Train Acc: {:.4f} | Val Acc: {:.4f} | Val Loss:{:.4f}'.\n",
    "              format(epoch + 1, config['epochs'], np.mean(dur), np.mean(train_loss), np.mean(train_acc), val_acc,np.mean(val_loss)))\n",
    "            logger.write('Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} | Train Acc: {:.4f} | Val Acc: {:.4f}'.\n",
    "                        format(epoch + 1, config['epochs'], np.mean(dur), np.mean(train_loss), np.mean(train_acc), val_acc))\n",
    "\n",
    "            #if stopper.step(val_acc, model):\n",
    "             #   break\n",
    "\n",
    "        #return stopper.best_score\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_collect = []\n",
    "        gt_collect = []\n",
    "        for i, data in enumerate(dataloader):\n",
    "            tgt = data['tgt_tensor'].to(config['device'])\n",
    "            data['graph_data'] = data['graph_data'].to(config[\"device\"])\n",
    "            logits= model(data['graph_data'], require_loss=False)\n",
    "            pred_collect.append(logits)\n",
    "            gt_collect.append(tgt)\n",
    "        #print(\"pred_collect: \",pred_collect)\n",
    "        pred_collect = torch.max(torch.cat(pred_collect, 0), dim=-1)[1].cpu()\n",
    "        #print(\"pred_collect: \",pred_collect)\n",
    "        #print(\"length of pred_collect:\",len(pred_collect))\n",
    "        gt_collect = torch.cat(gt_collect, 0).cpu()\n",
    "        score = metric.calculate_scores(ground_truth=gt_collect, predict=pred_collect, zero_division=0)[0]\n",
    "\n",
    "        return score\n",
    "def evaluate_val(dataloader):\n",
    "    valid_loss = []\n",
    "    model.eval()\n",
    "    #with torch.no_grad():\n",
    "    pred_collect = []\n",
    "    gt_collect = []\n",
    "    for i, data in enumerate(dataloader):\n",
    "        loss_val=0\n",
    "        tgt = data['tgt_tensor'].to(config['device'])\n",
    "        data['graph_data'] = data['graph_data'].to(config[\"device\"])\n",
    "        logits,loss_val= model(data['graph_data'],tgt, require_loss=True)\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        valid_loss.append(loss_val.item())\n",
    "        pred_collect.append(logits)\n",
    "        gt_collect.append(tgt)\n",
    "        #print(\"pred_collect: \",pred_collect)\n",
    "        pred_collect = torch.max(torch.cat(pred_collect, 0), dim=-1)[1].cpu()\n",
    "        #print(\"pred_collect: \",pred_collect)\n",
    "        #print(\"length of pred_collect:\",len(pred_collect))\n",
    "        gt_collect = torch.cat(gt_collect, 0).cpu()\n",
    "        score = metric.calculate_scores(ground_truth=gt_collect, predict=pred_collect, zero_division=0)[0]\n",
    "\n",
    "        return score,valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6f5e90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def test():\n",
    "        # restored best saved model\n",
    "        #stopper.load_checkpoint(model)\n",
    "\n",
    "        t0 = time.time()\n",
    "        acc = evaluate(test_dataloader)\n",
    "        dur = time.time() - t0\n",
    "        print('Test examples: {} | Time: {:.2f}s |  Test Acc: {:.4f}'.\n",
    "          format(len(dataset.test), dur, acc))\n",
    "        logger.write('Test examples: {} | Time: {:.2f}s |  Test Acc: {:.4f}'.\n",
    "          format(len(dataset.test), dur, acc))\n",
    "\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f2281887",
   "metadata": {},
   "outputs": [],
   "source": [
    " config['device'] = torch.device('cpu')\n",
    "logger = Logger(\n",
    "        config[\"out_dir\"],\n",
    "        config={k: v for k, v in config.items() if k != \"device\"},\n",
    "        overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "00b5584d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1 / 100] | Time: 8.63s | Loss: 0.6087 | Train Acc: 0.6884 | Val Acc: 0.8438 | Val Loss:0.2820\n",
      "Epoch: [2 / 100] | Time: 10.37s | Loss: 0.3704 | Train Acc: 0.8504 | Val Acc: 0.9062 | Val Loss:0.2140\n",
      "Epoch: [3 / 100] | Time: 10.77s | Loss: 0.3440 | Train Acc: 0.8531 | Val Acc: 0.8438 | Val Loss:0.1816\n",
      "Epoch: [4 / 100] | Time: 10.86s | Loss: 0.2908 | Train Acc: 0.8748 | Val Acc: 0.9375 | Val Loss:0.1718\n",
      "Epoch: [5 / 100] | Time: 10.92s | Loss: 0.2910 | Train Acc: 0.8720 | Val Acc: 0.9062 | Val Loss:0.1479\n",
      "Epoch: [6 / 100] | Time: 10.84s | Loss: 0.2712 | Train Acc: 0.8823 | Val Acc: 0.9375 | Val Loss:0.1837\n",
      "Epoch: [7 / 100] | Time: 10.86s | Loss: 0.2477 | Train Acc: 0.8927 | Val Acc: 0.9688 | Val Loss:0.0863\n",
      "Epoch: [8 / 100] | Time: 10.86s | Loss: 0.2347 | Train Acc: 0.8940 | Val Acc: 0.9062 | Val Loss:0.1090\n",
      "Epoch: [9 / 100] | Time: 10.87s | Loss: 0.2151 | Train Acc: 0.9034 | Val Acc: 0.9688 | Val Loss:0.0722\n",
      "Epoch: [10 / 100] | Time: 10.93s | Loss: 0.2153 | Train Acc: 0.9119 | Val Acc: 1.0000 | Val Loss:0.0558\n",
      "Epoch: [11 / 100] | Time: 10.90s | Loss: 0.2204 | Train Acc: 0.9047 | Val Acc: 1.0000 | Val Loss:0.0832\n",
      "Epoch: [12 / 100] | Time: 10.89s | Loss: 0.2088 | Train Acc: 0.9141 | Val Acc: 1.0000 | Val Loss:0.0484\n",
      "Epoch    13: reducing learning rate of group 0 to 5.0000e-03.\n",
      "Epoch: [13 / 100] | Time: 10.92s | Loss: 0.1857 | Train Acc: 0.9195 | Val Acc: 1.0000 | Val Loss:0.0389\n",
      "Epoch: [14 / 100] | Time: 10.92s | Loss: 0.1617 | Train Acc: 0.9257 | Val Acc: 0.9688 | Val Loss:0.0481\n",
      "Epoch: [15 / 100] | Time: 10.94s | Loss: 0.1297 | Train Acc: 0.9464 | Val Acc: 1.0000 | Val Loss:0.0238\n",
      "Epoch    16: reducing learning rate of group 0 to 2.5000e-03.\n",
      "Epoch: [16 / 100] | Time: 10.94s | Loss: 0.1191 | Train Acc: 0.9515 | Val Acc: 1.0000 | Val Loss:0.0152\n",
      "Epoch: [17 / 100] | Time: 10.95s | Loss: 0.0950 | Train Acc: 0.9641 | Val Acc: 1.0000 | Val Loss:0.0066\n",
      "Epoch: [18 / 100] | Time: 10.95s | Loss: 0.0881 | Train Acc: 0.9641 | Val Acc: 1.0000 | Val Loss:0.0112\n",
      "Epoch    19: reducing learning rate of group 0 to 1.2500e-03.\n",
      "Epoch: [19 / 100] | Time: 10.98s | Loss: 0.0769 | Train Acc: 0.9709 | Val Acc: 1.0000 | Val Loss:0.0051\n",
      "Epoch: [20 / 100] | Time: 11.03s | Loss: 0.0635 | Train Acc: 0.9740 | Val Acc: 1.0000 | Val Loss:0.0042\n",
      "Epoch: [21 / 100] | Time: 11.03s | Loss: 0.0621 | Train Acc: 0.9726 | Val Acc: 1.0000 | Val Loss:0.0063\n",
      "Epoch    22: reducing learning rate of group 0 to 6.2500e-04.\n",
      "Epoch: [22 / 100] | Time: 11.04s | Loss: 0.0541 | Train Acc: 0.9798 | Val Acc: 1.0000 | Val Loss:0.0029\n",
      "Epoch: [23 / 100] | Time: 11.04s | Loss: 0.0505 | Train Acc: 0.9794 | Val Acc: 1.0000 | Val Loss:0.0027\n",
      "Epoch: [24 / 100] | Time: 11.07s | Loss: 0.0490 | Train Acc: 0.9776 | Val Acc: 1.0000 | Val Loss:0.0034\n",
      "Epoch    25: reducing learning rate of group 0 to 3.1250e-04.\n",
      "Epoch: [25 / 100] | Time: 11.07s | Loss: 0.0499 | Train Acc: 0.9805 | Val Acc: 1.0000 | Val Loss:0.0029\n",
      "Epoch: [26 / 100] | Time: 11.08s | Loss: 0.0412 | Train Acc: 0.9833 | Val Acc: 1.0000 | Val Loss:0.0030\n",
      "Epoch: [27 / 100] | Time: 11.08s | Loss: 0.0429 | Train Acc: 0.9812 | Val Acc: 1.0000 | Val Loss:0.0022\n",
      "Epoch    28: reducing learning rate of group 0 to 1.5625e-04.\n",
      "Epoch: [28 / 100] | Time: 11.07s | Loss: 0.0391 | Train Acc: 0.9840 | Val Acc: 1.0000 | Val Loss:0.0019\n",
      "Epoch: [29 / 100] | Time: 11.08s | Loss: 0.0385 | Train Acc: 0.9830 | Val Acc: 1.0000 | Val Loss:0.0014\n",
      "Epoch: [30 / 100] | Time: 11.09s | Loss: 0.0410 | Train Acc: 0.9825 | Val Acc: 1.0000 | Val Loss:0.0013\n",
      "Epoch    31: reducing learning rate of group 0 to 7.8125e-05.\n",
      "Epoch: [31 / 100] | Time: 11.10s | Loss: 0.0360 | Train Acc: 0.9840 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [32 / 100] | Time: 11.11s | Loss: 0.0410 | Train Acc: 0.9822 | Val Acc: 1.0000 | Val Loss:0.0014\n",
      "Epoch: [33 / 100] | Time: 11.11s | Loss: 0.0364 | Train Acc: 0.9847 | Val Acc: 1.0000 | Val Loss:0.0014\n",
      "Epoch    34: reducing learning rate of group 0 to 3.9063e-05.\n",
      "Epoch: [34 / 100] | Time: 11.10s | Loss: 0.0336 | Train Acc: 0.9851 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [35 / 100] | Time: 11.10s | Loss: 0.0402 | Train Acc: 0.9826 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [36 / 100] | Time: 11.11s | Loss: 0.0406 | Train Acc: 0.9829 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch    37: reducing learning rate of group 0 to 1.9531e-05.\n",
      "Epoch: [37 / 100] | Time: 11.10s | Loss: 0.0336 | Train Acc: 0.9837 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [38 / 100] | Time: 11.10s | Loss: 0.0392 | Train Acc: 0.9829 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [39 / 100] | Time: 11.12s | Loss: 0.0419 | Train Acc: 0.9840 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch    40: reducing learning rate of group 0 to 9.7656e-06.\n",
      "Epoch: [40 / 100] | Time: 11.12s | Loss: 0.0368 | Train Acc: 0.9836 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [41 / 100] | Time: 11.13s | Loss: 0.0413 | Train Acc: 0.9851 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [42 / 100] | Time: 11.13s | Loss: 0.0370 | Train Acc: 0.9840 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch    43: reducing learning rate of group 0 to 4.8828e-06.\n",
      "Epoch: [43 / 100] | Time: 11.12s | Loss: 0.0366 | Train Acc: 0.9840 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [44 / 100] | Time: 11.12s | Loss: 0.0411 | Train Acc: 0.9847 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [45 / 100] | Time: 11.13s | Loss: 0.0399 | Train Acc: 0.9836 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch    46: reducing learning rate of group 0 to 2.4414e-06.\n",
      "Epoch: [46 / 100] | Time: 11.13s | Loss: 0.0393 | Train Acc: 0.9844 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [47 / 100] | Time: 11.13s | Loss: 0.0373 | Train Acc: 0.9844 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [48 / 100] | Time: 11.13s | Loss: 0.0404 | Train Acc: 0.9858 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch    49: reducing learning rate of group 0 to 1.2207e-06.\n",
      "Epoch: [49 / 100] | Time: 11.13s | Loss: 0.0330 | Train Acc: 0.9865 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [50 / 100] | Time: 11.13s | Loss: 0.0349 | Train Acc: 0.9851 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [51 / 100] | Time: 11.12s | Loss: 0.0410 | Train Acc: 0.9822 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch    52: reducing learning rate of group 0 to 6.1035e-07.\n",
      "Epoch: [52 / 100] | Time: 11.12s | Loss: 0.0314 | Train Acc: 0.9878 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [53 / 100] | Time: 11.12s | Loss: 0.0391 | Train Acc: 0.9830 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [54 / 100] | Time: 11.12s | Loss: 0.0364 | Train Acc: 0.9862 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch    55: reducing learning rate of group 0 to 3.0518e-07.\n",
      "Epoch: [55 / 100] | Time: 11.12s | Loss: 0.0361 | Train Acc: 0.9826 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [56 / 100] | Time: 11.12s | Loss: 0.0340 | Train Acc: 0.9851 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [57 / 100] | Time: 11.12s | Loss: 0.0386 | Train Acc: 0.9839 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch    58: reducing learning rate of group 0 to 1.5259e-07.\n",
      "Epoch: [58 / 100] | Time: 11.11s | Loss: 0.0382 | Train Acc: 0.9814 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [59 / 100] | Time: 11.11s | Loss: 0.0394 | Train Acc: 0.9830 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [60 / 100] | Time: 11.10s | Loss: 0.0353 | Train Acc: 0.9854 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch    61: reducing learning rate of group 0 to 7.6294e-08.\n",
      "Epoch: [61 / 100] | Time: 11.11s | Loss: 0.0366 | Train Acc: 0.9837 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [62 / 100] | Time: 11.11s | Loss: 0.0324 | Train Acc: 0.9862 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [63 / 100] | Time: 11.11s | Loss: 0.0361 | Train Acc: 0.9858 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch    64: reducing learning rate of group 0 to 3.8147e-08.\n",
      "Epoch: [64 / 100] | Time: 11.11s | Loss: 0.0375 | Train Acc: 0.9851 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [65 / 100] | Time: 11.11s | Loss: 0.0372 | Train Acc: 0.9847 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [66 / 100] | Time: 11.10s | Loss: 0.0355 | Train Acc: 0.9864 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch    67: reducing learning rate of group 0 to 1.9073e-08.\n",
      "Epoch: [67 / 100] | Time: 11.10s | Loss: 0.0413 | Train Acc: 0.9844 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [68 / 100] | Time: 11.09s | Loss: 0.0370 | Train Acc: 0.9872 | Val Acc: 1.0000 | Val Loss:0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [69 / 100] | Time: 11.08s | Loss: 0.0403 | Train Acc: 0.9869 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [70 / 100] | Time: 11.08s | Loss: 0.0403 | Train Acc: 0.9830 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [71 / 100] | Time: 11.08s | Loss: 0.0344 | Train Acc: 0.9844 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [72 / 100] | Time: 11.08s | Loss: 0.0421 | Train Acc: 0.9857 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [73 / 100] | Time: 11.07s | Loss: 0.0361 | Train Acc: 0.9851 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [74 / 100] | Time: 11.07s | Loss: 0.0403 | Train Acc: 0.9822 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [75 / 100] | Time: 11.07s | Loss: 0.0414 | Train Acc: 0.9822 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [76 / 100] | Time: 11.07s | Loss: 0.0391 | Train Acc: 0.9822 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [77 / 100] | Time: 11.07s | Loss: 0.0405 | Train Acc: 0.9862 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [78 / 100] | Time: 11.07s | Loss: 0.0366 | Train Acc: 0.9869 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [79 / 100] | Time: 11.07s | Loss: 0.0469 | Train Acc: 0.9836 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [80 / 100] | Time: 11.07s | Loss: 0.0373 | Train Acc: 0.9865 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [81 / 100] | Time: 11.06s | Loss: 0.0372 | Train Acc: 0.9830 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [82 / 100] | Time: 11.06s | Loss: 0.0369 | Train Acc: 0.9854 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [83 / 100] | Time: 11.07s | Loss: 0.0372 | Train Acc: 0.9847 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [84 / 100] | Time: 11.07s | Loss: 0.0363 | Train Acc: 0.9858 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [85 / 100] | Time: 11.06s | Loss: 0.0366 | Train Acc: 0.9872 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [86 / 100] | Time: 11.06s | Loss: 0.0315 | Train Acc: 0.9865 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [87 / 100] | Time: 11.06s | Loss: 0.0322 | Train Acc: 0.9862 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [88 / 100] | Time: 11.07s | Loss: 0.0351 | Train Acc: 0.9840 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [89 / 100] | Time: 11.06s | Loss: 0.0412 | Train Acc: 0.9843 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [90 / 100] | Time: 11.06s | Loss: 0.0357 | Train Acc: 0.9847 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [91 / 100] | Time: 11.06s | Loss: 0.0385 | Train Acc: 0.9837 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [92 / 100] | Time: 11.06s | Loss: 0.0383 | Train Acc: 0.9829 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [93 / 100] | Time: 11.06s | Loss: 0.0308 | Train Acc: 0.9876 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [94 / 100] | Time: 11.07s | Loss: 0.0389 | Train Acc: 0.9837 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [95 / 100] | Time: 11.07s | Loss: 0.0344 | Train Acc: 0.9844 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [96 / 100] | Time: 11.06s | Loss: 0.0335 | Train Acc: 0.9833 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [97 / 100] | Time: 11.06s | Loss: 0.0348 | Train Acc: 0.9847 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [98 / 100] | Time: 11.07s | Loss: 0.0388 | Train Acc: 0.9833 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [99 / 100] | Time: 11.07s | Loss: 0.0475 | Train Acc: 0.9818 | Val Acc: 1.0000 | Val Loss:0.0012\n",
      "Epoch: [100 / 100] | Time: 11.07s | Loss: 0.0444 | Train Acc: 0.9822 | Val Acc: 1.0000 | Val Loss:0.0012\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3edca87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test examples: 812 | Time: 1.96s |  Test Acc: 0.9926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9926108374384236"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c5c74c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (5.4.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\simra\\anaconda3\\envs\\py39\\lib\\site-packages (from h5py) (1.22.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyyaml h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a37181f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eb24df2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simra\\Downloads\\saved_model\n"
     ]
    }
   ],
   "source": [
    "cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "54de5d97",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3725518214.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [85]\u001b[1;36m\u001b[0m\n\u001b[1;33m    chown simra:admin saved_model\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "chown simra:admin saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b80207d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=r'C:/Users/simra/Downloads/saved_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0246bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,os.path.join(PATH,\"similegraph\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9e277643",
   "metadata": {},
   "outputs": [],
   "source": [
    "m= torch.load(os.path.join(PATH,\"similegraph\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de502b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (graph_initializer): GraphEmbeddingInitialization(\n",
       "    (embedding_layer): EmbeddingConstruction(\n",
       "      (word_emb_layers): ModuleDict(\n",
       "        (w2v): WordEmbedding(\n",
       "          (word_emb_layer): Embedding(304, 300, padding_idx=0)\n",
       "        )\n",
       "      )\n",
       "      (seq_info_encode_layer): RNNEmbedding(\n",
       "        (model): LSTM(300, 150, batch_first=True, bidirectional=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (word_emb): Embedding(304, 300, padding_idx=0)\n",
       "  (gnn): GraphSAGE(\n",
       "    (GraphSAGE_layers): ModuleList(\n",
       "      (0): GraphSAGELayer(\n",
       "        (model): BiFuseGraphSAGELayerConv(\n",
       "          (feat_drop): Dropout(p=0.3, inplace=False)\n",
       "          (lstm_fw): LSTM(300, 300, batch_first=True)\n",
       "          (lstm_bw): LSTM(300, 300, batch_first=True)\n",
       "          (fc_self_fw): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (fc_self_bw): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (fc_neigh): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (fuse_linear): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (clf): FeedForwardNN(\n",
       "    (graph_pool): AvgPooling()\n",
       "    (classifier): FeedForwardNNLayer(\n",
       "      (classifier): Sequential(\n",
       "        (fc0): Linear(in_features=300, out_features=300, bias=True)\n",
       "        (activate0): ReLU()\n",
       "        (fc_end): Linear(in_features=300, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (loss): GeneralLoss(\n",
       "    (loss_function): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50282e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHandler:\n",
    "    def __init__(self, config):\n",
    "        super(ModelHandler, self).__init__()\n",
    "        self.config = config\n",
    "        self.logger = Logger(self.config['out_dir'], config={k:v for k, v in self.config.items() if k != 'device'}, overwrite=True)\n",
    "        self.logger.write(self.config['out_dir'])\n",
    "        self._build_device()\n",
    "        self._build_dataloader()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "        self._build_evaluation()\n",
    "\n",
    "    def _build_device(self):\n",
    "        if not self.config['no_cuda'] and torch.cuda.is_available():\n",
    "            print('[ Using CUDA ]')\n",
    "            self.config['device'] = torch.device('cuda' if self.config['gpu'] < 0 else 'cuda:%d' % self.config['gpu'])\n",
    "            torch.cuda.manual_seed(self.config['seed'])\n",
    "            torch.cuda.manual_seed_all(self.config['seed'])\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            cudnn.benchmark = False\n",
    "        else:\n",
    "            self.config['device'] = torch.device('cpu')\n",
    "        \n",
    "    def _build_dataloader(self):\n",
    "        dynamic_init_topology_builder = None\n",
    "        if self.config['graph_type'] == 'dependency':\n",
    "            topology_builder = DependencyBasedGraphConstruction\n",
    "            graph_type = 'static'\n",
    "            merge_strategy = 'tailhead'\n",
    "        elif self.config['graph_type'] == 'constituency':\n",
    "            topology_builder = ConstituencyBasedGraphConstruction\n",
    "            graph_type = 'static'\n",
    "            merge_strategy = 'tailhead'\n",
    "        elif self.config['graph_type'] == 'ie':\n",
    "            topology_builder = IEBasedGraphConstruction\n",
    "            graph_type = 'static'\n",
    "            merge_strategy = 'global'\n",
    "        elif self.config['graph_type'] == 'node_emb':\n",
    "            topology_builder = NodeEmbeddingBasedGraphConstruction\n",
    "            graph_type = 'dynamic'\n",
    "            merge_strategy = None\n",
    "        elif self.config['graph_type'] == 'node_emb_refined':\n",
    "            topology_builder = NodeEmbeddingBasedRefinedGraphConstruction\n",
    "            graph_type = 'dynamic'\n",
    "            merge_strategy = 'tailhead'\n",
    "\n",
    "            if self.config['init_graph_type'] == 'line':\n",
    "                dynamic_init_topology_builder = None\n",
    "            elif self.config['init_graph_type'] == 'dependency':\n",
    "                dynamic_init_topology_builder = DependencyBasedGraphConstruction\n",
    "            elif self.config['init_graph_type'] == 'constituency':\n",
    "                dynamic_init_topology_builder = ConstituencyBasedGraphConstruction\n",
    "            elif self.config['init_graph_type'] == 'ie':\n",
    "                merge_strategy = 'global'\n",
    "                dynamic_init_topology_builder = IEBasedGraphConstruction\n",
    "            else:\n",
    "                raise RuntimeError('Define your own dynamic_init_topology_builder')\n",
    "        else:\n",
    "            raise RuntimeError('Unknown graph_type: {}'.format(self.config['graph_type']))\n",
    "\n",
    "        topology_subdir = '{}_graph'.format(self.config['graph_type'])\n",
    "        if self.config['graph_type'] == 'node_emb_refined':\n",
    "            topology_subdir += '_{}'.format(self.config['init_graph_type'])\n",
    "\n",
    "        dataset = TrecDataset(root_dir=self.config.get('root_dir', self.config['root_data_dir']),\n",
    "                              pretrained_word_emb_name=self.config.get('pretrained_word_emb_name', \"840B\"),\n",
    "                              merge_strategy=merge_strategy,\n",
    "                              seed=self.config['seed'],\n",
    "                              thread_number=4,\n",
    "                              port=9000,\n",
    "                              timeout=15000,\n",
    "                              word_emb_size=300,\n",
    "                              graph_type=graph_type,\n",
    "                              topology_builder=topology_builder,\n",
    "                              topology_subdir=topology_subdir,\n",
    "                              dynamic_graph_type=self.config['graph_type'] if \\\n",
    "                                  self.config['graph_type'] in ('node_emb', 'node_emb_refined') else None,\n",
    "                              dynamic_init_topology_builder=dynamic_init_topology_builder,\n",
    "                              dynamic_init_topology_aux_args={'dummy_param': 0})\n",
    "\n",
    "        self.train_dataloader = DataLoader(dataset.train, batch_size=self.config['batch_size'], shuffle=True,\n",
    "                                           num_workers=self.config['num_workers'],\n",
    "                                           collate_fn=dataset.collate_fn)\n",
    "        if hasattr(dataset, 'val')==False:\n",
    "            dataset.val = dataset.test\n",
    "        self.val_dataloader = DataLoader(dataset.val, batch_size=self.config['batch_size'], shuffle=False,\n",
    "                                          num_workers=self.config['num_workers'],\n",
    "                                          collate_fn=dataset.collate_fn)\n",
    "        self.test_dataloader = DataLoader(dataset.test, batch_size=self.config['batch_size'], shuffle=False,\n",
    "                                          num_workers=self.config['num_workers'],\n",
    "                                          collate_fn=dataset.collate_fn)\n",
    "        self.vocab = dataset.vocab_model\n",
    "        self.config['num_classes'] = dataset.num_classes\n",
    "        self.num_train = len(dataset.train)\n",
    "        self.num_val = len(dataset.val)\n",
    "        self.num_test = len(dataset.test)\n",
    "        print('Train size: {}, Val size: {}, Test size: {}'\n",
    "            .format(self.num_train, self.num_val, self.num_test))\n",
    "        self.logger.write('Train size: {}, Val size: {}, Test size: {}'\n",
    "            .format(self.num_train, self.num_val, self.num_test))\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.model = TextClassifier(self.vocab, self.config).to(self.config['device'])\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        self.optimizer = optim.Adam(parameters, lr=self.config['lr'])\n",
    "        #self.stopper = EarlyStopping(os.path.join(self.config['out_dir'], Constants._SAVED_WEIGHTS_FILE), patience=self.config['patience'])\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='max', factor=self.config['lr_reduce_factor'], \\\n",
    "            patience=self.config['lr_patience'], verbose=True)\n",
    "\n",
    "    def _build_evaluation(self):\n",
    "        self.metric = Accuracy(['accuracy'])\n",
    "\n",
    "    def train(self):\n",
    "        dur = []\n",
    "        for epoch in range(self.config['epochs']):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            train_acc = []\n",
    "            t0 = time.time()\n",
    "            for i, data in enumerate(self.train_dataloader):\n",
    "                tgt = data['tgt_tensor'].to(self.config['device'])\n",
    "                data['graph_data'] = data['graph_data'].to(self.config['device'])\n",
    "                logits, loss = self.model(data['graph_data'], tgt, require_loss=True)\n",
    "\n",
    "                # add graph regularization loss if available\n",
    "                if data['graph_data'].graph_attributes.get('graph_reg', None) is not None:\n",
    "                    loss = loss + data['graph_data'].graph_attributes['graph_reg']\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                pred = torch.max(logits, dim=-1)[1].cpu()\n",
    "                train_acc.append(self.metric.calculate_scores(ground_truth=tgt.cpu(), predict=pred.cpu(), zero_division=0)[0])\n",
    "                dur.append(time.time() - t0)\n",
    "\n",
    "            val_acc = self.evaluate(self.val_dataloader)\n",
    "            self.scheduler.step(val_acc)\n",
    "            print('Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} | Train Acc: {:.4f} | Val Acc: {:.4f}'.\n",
    "              format(epoch + 1, self.config['epochs'], np.mean(dur), np.mean(train_loss), np.mean(train_acc), val_acc))\n",
    "            self.logger.write('Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} | Train Acc: {:.4f} | Val Acc: {:.4f}'.\n",
    "                        format(epoch + 1, self.config['epochs'], np.mean(dur), np.mean(train_loss), np.mean(train_acc), val_acc))\n",
    "\n",
    "            #if self.stopper.step(val_acc, self.model):\n",
    "             #   break\n",
    "\n",
    "        return self.stopper.best_score\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_collect = []\n",
    "            gt_collect = []\n",
    "            for i, data in enumerate(dataloader):\n",
    "                tgt = data['tgt_tensor'].to(self.config['device'])\n",
    "                data['graph_data'] = data['graph_data'].to(self.config[\"device\"])\n",
    "                logits = self.model(data['graph_data'], require_loss=False)\n",
    "                pred_collect.append(logits)\n",
    "                gt_collect.append(tgt)\n",
    "\n",
    "            pred_collect = torch.max(torch.cat(pred_collect, 0), dim=-1)[1].cpu()\n",
    "            gt_collect = torch.cat(gt_collect, 0).cpu()\n",
    "            score = self.metric.calculate_scores(ground_truth=gt_collect, predict=pred_collect, zero_division=0)[0]\n",
    "\n",
    "            return score\n",
    "\n",
    "    def test(self):\n",
    "        # restored best saved model\n",
    "        self.stopper.load_checkpoint(self.model)\n",
    "\n",
    "        t0 = time.time()\n",
    "        acc = self.evaluate(self.test_dataloader)\n",
    "        dur = time.time() - t0\n",
    "        print('Test examples: {} | Time: {:.2f}s |  Test Acc: {:.4f}'.\n",
    "          format(self.num_test, dur, acc))\n",
    "        self.logger.write('Test examples: {} | Time: {:.2f}s |  Test Acc: {:.4f}'.\n",
    "          format(self.num_test, dur, acc))\n",
    "\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97aafc04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[43mconfig\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m ts \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mtimestamp()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(config['seed'])\n",
    "torch.manual_seed(config['seed'])\n",
    "\n",
    "ts = datetime.datetime.now().timestamp()\n",
    "config['out_dir'] += '_{}'.format(ts)\n",
    "print('\\n' + config['out_dir'])\n",
    "\n",
    "runner = ModelHandler(config)\n",
    "t0 = time.time()\n",
    "\n",
    "val_acc = runner.train()\n",
    "test_acc = runner.test()\n",
    "\n",
    "runtime = time.time() - t0\n",
    "print('Total runtime: {:.2f}s'.format(runtime))\n",
    "runner.logger.write('Total runtime: {:.2f}s\\n'.format(runtime))\n",
    "runner.logger.close()\n",
    "\n",
    "print('val acc: {}, test acc: {}'.format(val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1aaf513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.multiprocessing\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from graph4nlp.pytorch.datasets.trec import TrecDataset\n",
    "from graph4nlp.pytorch.modules.evaluation.accuracy import Accuracy\n",
    "from graph4nlp.pytorch.modules.graph_construction import (\n",
    "    NodeEmbeddingBasedGraphConstruction,\n",
    "    NodeEmbeddingBasedRefinedGraphConstruction,\n",
    ")\n",
    "from graph4nlp.pytorch.modules.graph_embedding_initialization.embedding_construction import (\n",
    "    WordEmbedding,\n",
    ")\n",
    "from graph4nlp.pytorch.modules.graph_embedding_initialization.graph_embedding_initialization import (  # noqa\n",
    "    GraphEmbeddingInitialization,\n",
    ")\n",
    "from graph4nlp.pytorch.modules.graph_embedding_learning import GAT, GGNN, GraphSAGE\n",
    "from graph4nlp.pytorch.modules.loss.general_loss import GeneralLoss\n",
    "from graph4nlp.pytorch.modules.prediction.classification.graph_classification import FeedForwardNN\n",
    "from graph4nlp.pytorch.modules.utils import constants as Constants\n",
    "#from graph4nlp.pytorch.modules.utils.generic_utils import EarlyStopping, grid, to_cuda\n",
    "from graph4nlp.pytorch.modules.utils.logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4970a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
