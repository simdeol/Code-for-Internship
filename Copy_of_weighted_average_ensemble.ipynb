{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-10_84BVYUp",
        "outputId": "2f117e4b-aceb-4150-d50e-cd99d5cb0cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.2.5\n",
            "  Downloading Keras-2.2.5-py2.py3-none-any.whl (336 kB)\n",
            "\u001b[K     |████████████████████████████████| 336 kB 28.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (6.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.7.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.5) (1.5.2)\n",
            "Installing collected packages: keras-applications, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires keras<2.9,>=2.8.0rc0, but you have keras 2.2.5 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.2.5 keras-applications-1.0.8\n"
          ]
        }
      ],
      "source": [
        "pip install keras==2.2.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==1.14.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew0J1N78VZYs",
        "outputId": "04822a36-a735-4907-a494-1fd84da5d082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.14.0\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3 MB 50 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.21.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.37.1)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 54.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.17.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.2.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.5.3)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 67.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.47.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.8.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14.0) (1.5.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "TOIVJLRhVbOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('Final_DL_Data_1.xlsx')"
      ],
      "metadata": {
        "id": "apkCkx5AVe7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['metaphor_label'] = data['metaphor_label'].fillna(0)"
      ],
      "metadata": {
        "id": "Y1wsObhyVffo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDIUFmrTVzWl",
        "outputId": "cdb9cf89-fe51-4b21-a13e-457e3b2a2981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0               0\n",
              "Unnamed: 0.1             0\n",
              "Unnamed: 0.1.1           0\n",
              "final_tweets             1\n",
              "label                    0\n",
              "metaphor_label           0\n",
              "simile_label             0\n",
              "clause_polarity_label    0\n",
              "punctuation_count        0\n",
              "positive_words           0\n",
              "negative_words           0\n",
              "opposite_polarity        0\n",
              "sequence                 0\n",
              "is_intensifier           0\n",
              "interjection_count       0\n",
              "mixed_case               0\n",
              "repeated_letters         0\n",
              "repeated_words           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "bH-N8TXkVWN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABNET**"
      ],
      "metadata": {
        "id": "biHYAAA6V7sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDmOFanaV6lM",
        "outputId": "b117ba24-d556-4225-b08b-32fd8533f583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-3.1.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.21.6)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (4.64.0)\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.12.1+cu113)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->pytorch-tabnet) (4.1.1)\n",
            "Installing collected packages: pytorch-tabnet\n",
            "Successfully installed pytorch-tabnet-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(['Unnamed: 0','Unnamed: 0.1.1','Unnamed: 0.1','final_tweets','label'],axis=1)\n",
        "y = data['label']"
      ],
      "metadata": {
        "id": "a8hW6EC6V1db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=14)"
      ],
      "metadata": {
        "id": "sGO_7Z3qWGhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= X.to_numpy()\n",
        "y= y.to_numpy() \n",
        "y= y.flatten()"
      ],
      "metadata": {
        "id": "of95GhmyWMZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5, random_state=14, shuffle=True)\n",
        "CV_score_array  =[]\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_valid = X[train_index], X[test_index]\n",
        "    y_train, y_valid = y[train_index], y[test_index]\n",
        "    tb_cls = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
        "                       optimizer_params=dict(lr=0.0025),\n",
        "                       scheduler_params={\"step_size\":10, \"gamma\":0.4},\n",
        "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "                       mask_type=\"entmax\"\n",
        "                       )\n",
        "    tb_cls.fit(X_train,y_train,\n",
        "               eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "               eval_name=['train', 'valid'],\n",
        "               eval_metric=['accuracy'],\n",
        "               max_epochs=100 , patience=0,\n",
        "               batch_size=25, drop_last=False)            "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNGa4lzmWPw4",
        "outputId": "49448dc4-41a2-4b0b-d22c-918f80fd8a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used : cuda\n",
            "No early stopping will be performed, last training weights will be used.\n",
            "epoch 0  | loss: 0.8476  | train_accuracy: 0.52185 | valid_accuracy: 0.53287 |  0:00:03s\n",
            "epoch 1  | loss: 0.66011 | train_accuracy: 0.60407 | valid_accuracy: 0.6263  |  0:00:06s\n",
            "epoch 2  | loss: 0.60411 | train_accuracy: 0.71051 | valid_accuracy: 0.72145 |  0:00:09s\n",
            "epoch 3  | loss: 0.54262 | train_accuracy: 0.74513 | valid_accuracy: 0.75606 |  0:00:12s\n",
            "epoch 4  | loss: 0.51851 | train_accuracy: 0.75552 | valid_accuracy: 0.76125 |  0:00:15s\n",
            "epoch 5  | loss: 0.50337 | train_accuracy: 0.76158 | valid_accuracy: 0.77336 |  0:00:18s\n",
            "epoch 6  | loss: 0.49676 | train_accuracy: 0.76677 | valid_accuracy: 0.76298 |  0:00:21s\n",
            "epoch 7  | loss: 0.4967  | train_accuracy: 0.76417 | valid_accuracy: 0.75606 |  0:00:24s\n",
            "epoch 8  | loss: 0.48825 | train_accuracy: 0.76417 | valid_accuracy: 0.76125 |  0:00:27s\n",
            "epoch 9  | loss: 0.47759 | train_accuracy: 0.77109 | valid_accuracy: 0.77855 |  0:00:30s\n",
            "epoch 10 | loss: 0.47711 | train_accuracy: 0.77456 | valid_accuracy: 0.77163 |  0:00:33s\n",
            "epoch 11 | loss: 0.47851 | train_accuracy: 0.77499 | valid_accuracy: 0.76644 |  0:00:36s\n",
            "epoch 12 | loss: 0.46947 | train_accuracy: 0.77585 | valid_accuracy: 0.7699  |  0:00:39s\n",
            "epoch 13 | loss: 0.4681  | train_accuracy: 0.77715 | valid_accuracy: 0.77336 |  0:00:42s\n",
            "epoch 14 | loss: 0.46699 | train_accuracy: 0.78235 | valid_accuracy: 0.77682 |  0:00:45s\n",
            "epoch 15 | loss: 0.46207 | train_accuracy: 0.78061 | valid_accuracy: 0.77336 |  0:00:48s\n",
            "epoch 16 | loss: 0.45795 | train_accuracy: 0.78061 | valid_accuracy: 0.77855 |  0:00:51s\n",
            "epoch 17 | loss: 0.46035 | train_accuracy: 0.77975 | valid_accuracy: 0.7699  |  0:00:54s\n",
            "epoch 18 | loss: 0.45419 | train_accuracy: 0.78105 | valid_accuracy: 0.77163 |  0:00:57s\n",
            "epoch 19 | loss: 0.46276 | train_accuracy: 0.77975 | valid_accuracy: 0.77509 |  0:01:01s\n",
            "epoch 20 | loss: 0.45128 | train_accuracy: 0.77932 | valid_accuracy: 0.77336 |  0:01:04s\n",
            "epoch 21 | loss: 0.44997 | train_accuracy: 0.78018 | valid_accuracy: 0.77336 |  0:01:07s\n",
            "epoch 22 | loss: 0.4571  | train_accuracy: 0.78278 | valid_accuracy: 0.78374 |  0:01:10s\n",
            "epoch 23 | loss: 0.44684 | train_accuracy: 0.78105 | valid_accuracy: 0.77509 |  0:01:13s\n",
            "epoch 24 | loss: 0.44777 | train_accuracy: 0.78321 | valid_accuracy: 0.77855 |  0:01:16s\n",
            "epoch 25 | loss: 0.45003 | train_accuracy: 0.78148 | valid_accuracy: 0.77682 |  0:01:19s\n",
            "epoch 26 | loss: 0.46033 | train_accuracy: 0.78278 | valid_accuracy: 0.77855 |  0:01:22s\n",
            "epoch 27 | loss: 0.45689 | train_accuracy: 0.78321 | valid_accuracy: 0.77855 |  0:01:25s\n",
            "epoch 28 | loss: 0.44458 | train_accuracy: 0.78148 | valid_accuracy: 0.78201 |  0:01:28s\n",
            "epoch 29 | loss: 0.44218 | train_accuracy: 0.78321 | valid_accuracy: 0.77682 |  0:01:31s\n",
            "epoch 30 | loss: 0.4458  | train_accuracy: 0.78235 | valid_accuracy: 0.77336 |  0:01:34s\n",
            "epoch 31 | loss: 0.44606 | train_accuracy: 0.78191 | valid_accuracy: 0.77509 |  0:01:37s\n",
            "epoch 32 | loss: 0.4554  | train_accuracy: 0.78451 | valid_accuracy: 0.77509 |  0:01:40s\n",
            "epoch 33 | loss: 0.44477 | train_accuracy: 0.78451 | valid_accuracy: 0.78547 |  0:01:43s\n",
            "epoch 34 | loss: 0.44852 | train_accuracy: 0.78581 | valid_accuracy: 0.77682 |  0:01:46s\n",
            "epoch 35 | loss: 0.45394 | train_accuracy: 0.78494 | valid_accuracy: 0.77855 |  0:01:49s\n",
            "epoch 36 | loss: 0.4499  | train_accuracy: 0.78537 | valid_accuracy: 0.77682 |  0:01:52s\n",
            "epoch 37 | loss: 0.45297 | train_accuracy: 0.78667 | valid_accuracy: 0.77855 |  0:01:55s\n",
            "epoch 38 | loss: 0.44717 | train_accuracy: 0.78321 | valid_accuracy: 0.77509 |  0:01:58s\n",
            "epoch 39 | loss: 0.44486 | train_accuracy: 0.78537 | valid_accuracy: 0.77509 |  0:02:01s\n",
            "epoch 40 | loss: 0.45102 | train_accuracy: 0.78451 | valid_accuracy: 0.77855 |  0:02:04s\n",
            "epoch 41 | loss: 0.44604 | train_accuracy: 0.78797 | valid_accuracy: 0.77682 |  0:02:07s\n",
            "epoch 42 | loss: 0.44092 | train_accuracy: 0.78667 | valid_accuracy: 0.77682 |  0:02:10s\n",
            "epoch 43 | loss: 0.45039 | train_accuracy: 0.78537 | valid_accuracy: 0.77336 |  0:02:13s\n",
            "epoch 44 | loss: 0.43644 | train_accuracy: 0.7884  | valid_accuracy: 0.78374 |  0:02:16s\n",
            "epoch 45 | loss: 0.44231 | train_accuracy: 0.78754 | valid_accuracy: 0.78201 |  0:02:19s\n",
            "epoch 46 | loss: 0.44557 | train_accuracy: 0.78711 | valid_accuracy: 0.78028 |  0:02:22s\n",
            "epoch 47 | loss: 0.447   | train_accuracy: 0.78624 | valid_accuracy: 0.78201 |  0:02:25s\n",
            "epoch 48 | loss: 0.44088 | train_accuracy: 0.78797 | valid_accuracy: 0.77855 |  0:02:28s\n",
            "epoch 49 | loss: 0.43841 | train_accuracy: 0.78537 | valid_accuracy: 0.77855 |  0:02:31s\n",
            "epoch 50 | loss: 0.44464 | train_accuracy: 0.78581 | valid_accuracy: 0.78201 |  0:02:34s\n",
            "epoch 51 | loss: 0.44444 | train_accuracy: 0.78581 | valid_accuracy: 0.77855 |  0:02:38s\n",
            "epoch 52 | loss: 0.44281 | train_accuracy: 0.78667 | valid_accuracy: 0.78028 |  0:02:41s\n",
            "epoch 53 | loss: 0.44376 | train_accuracy: 0.78581 | valid_accuracy: 0.78201 |  0:02:44s\n",
            "epoch 54 | loss: 0.44224 | train_accuracy: 0.78494 | valid_accuracy: 0.77855 |  0:02:47s\n",
            "epoch 55 | loss: 0.44577 | train_accuracy: 0.78754 | valid_accuracy: 0.77855 |  0:02:50s\n",
            "epoch 56 | loss: 0.44632 | train_accuracy: 0.78537 | valid_accuracy: 0.77336 |  0:02:53s\n",
            "epoch 57 | loss: 0.44386 | train_accuracy: 0.78667 | valid_accuracy: 0.77855 |  0:02:56s\n",
            "epoch 58 | loss: 0.44722 | train_accuracy: 0.7884  | valid_accuracy: 0.77682 |  0:02:59s\n",
            "epoch 59 | loss: 0.44398 | train_accuracy: 0.78754 | valid_accuracy: 0.77682 |  0:03:02s\n",
            "epoch 60 | loss: 0.44105 | train_accuracy: 0.78711 | valid_accuracy: 0.78028 |  0:03:05s\n",
            "epoch 61 | loss: 0.446   | train_accuracy: 0.78537 | valid_accuracy: 0.77682 |  0:03:08s\n",
            "epoch 62 | loss: 0.44572 | train_accuracy: 0.78537 | valid_accuracy: 0.78028 |  0:03:11s\n",
            "epoch 63 | loss: 0.44175 | train_accuracy: 0.78754 | valid_accuracy: 0.77855 |  0:03:14s\n",
            "epoch 64 | loss: 0.44054 | train_accuracy: 0.78797 | valid_accuracy: 0.77509 |  0:03:17s\n",
            "epoch 65 | loss: 0.44991 | train_accuracy: 0.7884  | valid_accuracy: 0.78201 |  0:03:20s\n",
            "epoch 66 | loss: 0.44504 | train_accuracy: 0.78884 | valid_accuracy: 0.77509 |  0:03:22s\n",
            "epoch 67 | loss: 0.44656 | train_accuracy: 0.78711 | valid_accuracy: 0.78374 |  0:03:25s\n",
            "epoch 68 | loss: 0.44245 | train_accuracy: 0.78797 | valid_accuracy: 0.78201 |  0:03:28s\n",
            "epoch 69 | loss: 0.44583 | train_accuracy: 0.78884 | valid_accuracy: 0.77855 |  0:03:31s\n",
            "epoch 70 | loss: 0.44756 | train_accuracy: 0.78581 | valid_accuracy: 0.77855 |  0:03:34s\n",
            "epoch 71 | loss: 0.44481 | train_accuracy: 0.7884  | valid_accuracy: 0.78201 |  0:03:38s\n",
            "epoch 72 | loss: 0.44528 | train_accuracy: 0.78711 | valid_accuracy: 0.77682 |  0:03:41s\n",
            "epoch 73 | loss: 0.44282 | train_accuracy: 0.78667 | valid_accuracy: 0.77682 |  0:03:44s\n",
            "epoch 74 | loss: 0.44211 | train_accuracy: 0.78624 | valid_accuracy: 0.77336 |  0:03:47s\n",
            "epoch 75 | loss: 0.4491  | train_accuracy: 0.78754 | valid_accuracy: 0.77682 |  0:03:50s\n",
            "epoch 76 | loss: 0.43756 | train_accuracy: 0.78581 | valid_accuracy: 0.78028 |  0:03:53s\n",
            "epoch 77 | loss: 0.44297 | train_accuracy: 0.78797 | valid_accuracy: 0.77682 |  0:03:56s\n",
            "epoch 78 | loss: 0.44721 | train_accuracy: 0.78754 | valid_accuracy: 0.78028 |  0:03:59s\n",
            "epoch 79 | loss: 0.44868 | train_accuracy: 0.78711 | valid_accuracy: 0.77682 |  0:04:02s\n",
            "epoch 80 | loss: 0.44469 | train_accuracy: 0.7884  | valid_accuracy: 0.78201 |  0:04:05s\n",
            "epoch 81 | loss: 0.44387 | train_accuracy: 0.78667 | valid_accuracy: 0.77855 |  0:04:08s\n",
            "epoch 82 | loss: 0.44325 | train_accuracy: 0.78754 | valid_accuracy: 0.77509 |  0:04:11s\n",
            "epoch 83 | loss: 0.43418 | train_accuracy: 0.78797 | valid_accuracy: 0.78028 |  0:04:14s\n",
            "epoch 84 | loss: 0.44551 | train_accuracy: 0.7897  | valid_accuracy: 0.78374 |  0:04:17s\n",
            "epoch 85 | loss: 0.44847 | train_accuracy: 0.7884  | valid_accuracy: 0.78201 |  0:04:20s\n",
            "epoch 86 | loss: 0.44025 | train_accuracy: 0.78754 | valid_accuracy: 0.78028 |  0:04:23s\n",
            "epoch 87 | loss: 0.44401 | train_accuracy: 0.78754 | valid_accuracy: 0.77855 |  0:04:26s\n",
            "epoch 88 | loss: 0.44703 | train_accuracy: 0.78754 | valid_accuracy: 0.77855 |  0:04:29s\n",
            "epoch 89 | loss: 0.44196 | train_accuracy: 0.78711 | valid_accuracy: 0.77682 |  0:04:32s\n",
            "epoch 90 | loss: 0.44799 | train_accuracy: 0.78884 | valid_accuracy: 0.78201 |  0:04:35s\n",
            "epoch 91 | loss: 0.43808 | train_accuracy: 0.78711 | valid_accuracy: 0.78201 |  0:04:38s\n",
            "epoch 92 | loss: 0.43926 | train_accuracy: 0.78711 | valid_accuracy: 0.78201 |  0:04:41s\n",
            "epoch 93 | loss: 0.44746 | train_accuracy: 0.78927 | valid_accuracy: 0.78028 |  0:04:44s\n",
            "epoch 94 | loss: 0.44184 | train_accuracy: 0.7884  | valid_accuracy: 0.77855 |  0:04:47s\n",
            "epoch 95 | loss: 0.44463 | train_accuracy: 0.78711 | valid_accuracy: 0.78201 |  0:04:50s\n",
            "epoch 96 | loss: 0.45738 | train_accuracy: 0.78754 | valid_accuracy: 0.77855 |  0:04:53s\n",
            "epoch 97 | loss: 0.44296 | train_accuracy: 0.78624 | valid_accuracy: 0.78028 |  0:04:56s\n",
            "epoch 98 | loss: 0.44561 | train_accuracy: 0.78797 | valid_accuracy: 0.77509 |  0:04:59s\n",
            "epoch 99 | loss: 0.44792 | train_accuracy: 0.78797 | valid_accuracy: 0.77855 |  0:05:02s\n",
            "Device used : cuda\n",
            "No early stopping will be performed, last training weights will be used.\n",
            "epoch 0  | loss: 0.87248 | train_accuracy: 0.5119  | valid_accuracy: 0.5173  |  0:00:02s\n",
            "epoch 1  | loss: 0.66254 | train_accuracy: 0.61099 | valid_accuracy: 0.61419 |  0:00:06s\n",
            "epoch 2  | loss: 0.61355 | train_accuracy: 0.70056 | valid_accuracy: 0.70069 |  0:00:09s\n",
            "epoch 3  | loss: 0.56664 | train_accuracy: 0.72653 | valid_accuracy: 0.74394 |  0:00:11s\n",
            "epoch 4  | loss: 0.53106 | train_accuracy: 0.75595 | valid_accuracy: 0.7526  |  0:00:14s\n",
            "epoch 5  | loss: 0.51456 | train_accuracy: 0.75595 | valid_accuracy: 0.75952 |  0:00:17s\n",
            "epoch 6  | loss: 0.49763 | train_accuracy: 0.76417 | valid_accuracy: 0.76644 |  0:00:20s\n",
            "epoch 7  | loss: 0.50513 | train_accuracy: 0.75941 | valid_accuracy: 0.75779 |  0:00:23s\n",
            "epoch 8  | loss: 0.50059 | train_accuracy: 0.75682 | valid_accuracy: 0.77509 |  0:00:26s\n",
            "epoch 9  | loss: 0.49026 | train_accuracy: 0.7646  | valid_accuracy: 0.7699  |  0:00:29s\n",
            "epoch 10 | loss: 0.47393 | train_accuracy: 0.76633 | valid_accuracy: 0.77163 |  0:00:32s\n",
            "epoch 11 | loss: 0.4882  | train_accuracy: 0.7685  | valid_accuracy: 0.77336 |  0:00:35s\n",
            "epoch 12 | loss: 0.47206 | train_accuracy: 0.76677 | valid_accuracy: 0.77163 |  0:00:38s\n",
            "epoch 13 | loss: 0.47345 | train_accuracy: 0.77153 | valid_accuracy: 0.77855 |  0:00:41s\n",
            "epoch 14 | loss: 0.47218 | train_accuracy: 0.77239 | valid_accuracy: 0.78028 |  0:00:44s\n",
            "epoch 15 | loss: 0.47681 | train_accuracy: 0.77066 | valid_accuracy: 0.78201 |  0:00:47s\n",
            "epoch 16 | loss: 0.48073 | train_accuracy: 0.77283 | valid_accuracy: 0.78374 |  0:00:50s\n",
            "epoch 17 | loss: 0.47575 | train_accuracy: 0.77326 | valid_accuracy: 0.78547 |  0:00:54s\n",
            "epoch 18 | loss: 0.46186 | train_accuracy: 0.77932 | valid_accuracy: 0.78374 |  0:00:57s\n",
            "epoch 19 | loss: 0.47099 | train_accuracy: 0.77629 | valid_accuracy: 0.78201 |  0:01:00s\n",
            "epoch 20 | loss: 0.46145 | train_accuracy: 0.77629 | valid_accuracy: 0.77855 |  0:01:03s\n",
            "epoch 21 | loss: 0.46929 | train_accuracy: 0.78061 | valid_accuracy: 0.78547 |  0:01:06s\n",
            "epoch 22 | loss: 0.47094 | train_accuracy: 0.78105 | valid_accuracy: 0.78547 |  0:01:10s\n",
            "epoch 23 | loss: 0.46348 | train_accuracy: 0.77932 | valid_accuracy: 0.7872  |  0:01:13s\n",
            "epoch 24 | loss: 0.46615 | train_accuracy: 0.77759 | valid_accuracy: 0.7872  |  0:01:16s\n",
            "epoch 25 | loss: 0.46058 | train_accuracy: 0.77932 | valid_accuracy: 0.78893 |  0:01:19s\n",
            "epoch 26 | loss: 0.47233 | train_accuracy: 0.77888 | valid_accuracy: 0.78547 |  0:01:22s\n",
            "epoch 27 | loss: 0.46826 | train_accuracy: 0.78018 | valid_accuracy: 0.78374 |  0:01:25s\n",
            "epoch 28 | loss: 0.4658  | train_accuracy: 0.77975 | valid_accuracy: 0.78028 |  0:01:28s\n",
            "epoch 29 | loss: 0.45667 | train_accuracy: 0.78321 | valid_accuracy: 0.78547 |  0:01:31s\n",
            "epoch 30 | loss: 0.47041 | train_accuracy: 0.78191 | valid_accuracy: 0.78547 |  0:01:34s\n",
            "epoch 31 | loss: 0.46489 | train_accuracy: 0.77975 | valid_accuracy: 0.78201 |  0:01:37s\n",
            "epoch 32 | loss: 0.46585 | train_accuracy: 0.78018 | valid_accuracy: 0.78374 |  0:01:41s\n",
            "epoch 33 | loss: 0.46263 | train_accuracy: 0.78451 | valid_accuracy: 0.78374 |  0:01:44s\n",
            "epoch 34 | loss: 0.46064 | train_accuracy: 0.78278 | valid_accuracy: 0.78374 |  0:01:47s\n",
            "epoch 35 | loss: 0.45577 | train_accuracy: 0.78191 | valid_accuracy: 0.78201 |  0:01:50s\n",
            "epoch 36 | loss: 0.46021 | train_accuracy: 0.78191 | valid_accuracy: 0.78374 |  0:01:53s\n",
            "epoch 37 | loss: 0.46481 | train_accuracy: 0.78191 | valid_accuracy: 0.78374 |  0:01:56s\n",
            "epoch 38 | loss: 0.45757 | train_accuracy: 0.78364 | valid_accuracy: 0.78547 |  0:01:59s\n",
            "epoch 39 | loss: 0.4617  | train_accuracy: 0.78235 | valid_accuracy: 0.78374 |  0:02:02s\n",
            "epoch 40 | loss: 0.46697 | train_accuracy: 0.78278 | valid_accuracy: 0.78201 |  0:02:05s\n",
            "epoch 41 | loss: 0.45746 | train_accuracy: 0.78321 | valid_accuracy: 0.78374 |  0:02:08s\n",
            "epoch 42 | loss: 0.45808 | train_accuracy: 0.78148 | valid_accuracy: 0.78374 |  0:02:11s\n",
            "epoch 43 | loss: 0.46226 | train_accuracy: 0.78364 | valid_accuracy: 0.7872  |  0:02:14s\n",
            "epoch 44 | loss: 0.46754 | train_accuracy: 0.78321 | valid_accuracy: 0.78374 |  0:02:17s\n",
            "epoch 45 | loss: 0.46225 | train_accuracy: 0.78494 | valid_accuracy: 0.78374 |  0:02:20s\n",
            "epoch 46 | loss: 0.46423 | train_accuracy: 0.78408 | valid_accuracy: 0.78547 |  0:02:23s\n",
            "epoch 47 | loss: 0.46451 | train_accuracy: 0.78191 | valid_accuracy: 0.78374 |  0:02:26s\n",
            "epoch 48 | loss: 0.45648 | train_accuracy: 0.78191 | valid_accuracy: 0.78374 |  0:02:29s\n",
            "epoch 49 | loss: 0.46309 | train_accuracy: 0.78278 | valid_accuracy: 0.78547 |  0:02:32s\n",
            "epoch 50 | loss: 0.45976 | train_accuracy: 0.78321 | valid_accuracy: 0.78547 |  0:02:35s\n",
            "epoch 51 | loss: 0.4553  | train_accuracy: 0.78278 | valid_accuracy: 0.78547 |  0:02:38s\n",
            "epoch 52 | loss: 0.45415 | train_accuracy: 0.78321 | valid_accuracy: 0.78547 |  0:02:41s\n",
            "epoch 53 | loss: 0.45859 | train_accuracy: 0.78581 | valid_accuracy: 0.78547 |  0:02:44s\n",
            "epoch 54 | loss: 0.4618  | train_accuracy: 0.78537 | valid_accuracy: 0.7872  |  0:02:47s\n",
            "epoch 55 | loss: 0.46069 | train_accuracy: 0.78754 | valid_accuracy: 0.78547 |  0:02:50s\n",
            "epoch 56 | loss: 0.46816 | train_accuracy: 0.78494 | valid_accuracy: 0.78547 |  0:02:53s\n",
            "epoch 57 | loss: 0.45917 | train_accuracy: 0.78581 | valid_accuracy: 0.78547 |  0:02:56s\n",
            "epoch 58 | loss: 0.46359 | train_accuracy: 0.78364 | valid_accuracy: 0.7872  |  0:02:59s\n",
            "epoch 59 | loss: 0.45489 | train_accuracy: 0.78321 | valid_accuracy: 0.78893 |  0:03:02s\n",
            "epoch 60 | loss: 0.4675  | train_accuracy: 0.78278 | valid_accuracy: 0.78374 |  0:03:05s\n",
            "epoch 61 | loss: 0.45698 | train_accuracy: 0.78537 | valid_accuracy: 0.78374 |  0:03:08s\n",
            "epoch 62 | loss: 0.46229 | train_accuracy: 0.78278 | valid_accuracy: 0.78547 |  0:03:11s\n",
            "epoch 63 | loss: 0.45677 | train_accuracy: 0.78278 | valid_accuracy: 0.78547 |  0:03:14s\n",
            "epoch 64 | loss: 0.45924 | train_accuracy: 0.78494 | valid_accuracy: 0.78374 |  0:03:17s\n",
            "epoch 65 | loss: 0.46096 | train_accuracy: 0.78494 | valid_accuracy: 0.78547 |  0:03:20s\n",
            "epoch 66 | loss: 0.46014 | train_accuracy: 0.78278 | valid_accuracy: 0.78547 |  0:03:23s\n",
            "epoch 67 | loss: 0.46452 | train_accuracy: 0.78321 | valid_accuracy: 0.7872  |  0:03:26s\n",
            "epoch 68 | loss: 0.46058 | train_accuracy: 0.78494 | valid_accuracy: 0.78893 |  0:03:29s\n",
            "epoch 69 | loss: 0.46791 | train_accuracy: 0.78321 | valid_accuracy: 0.7872  |  0:03:32s\n",
            "epoch 70 | loss: 0.45304 | train_accuracy: 0.78451 | valid_accuracy: 0.7872  |  0:03:35s\n",
            "epoch 71 | loss: 0.45707 | train_accuracy: 0.78451 | valid_accuracy: 0.78547 |  0:03:38s\n",
            "epoch 72 | loss: 0.46159 | train_accuracy: 0.78408 | valid_accuracy: 0.7872  |  0:03:41s\n",
            "epoch 73 | loss: 0.45751 | train_accuracy: 0.78235 | valid_accuracy: 0.78374 |  0:03:44s\n",
            "epoch 74 | loss: 0.46225 | train_accuracy: 0.78191 | valid_accuracy: 0.7872  |  0:03:47s\n",
            "epoch 75 | loss: 0.4536  | train_accuracy: 0.78364 | valid_accuracy: 0.79066 |  0:03:50s\n",
            "epoch 76 | loss: 0.45864 | train_accuracy: 0.78235 | valid_accuracy: 0.7872  |  0:03:53s\n",
            "epoch 77 | loss: 0.47396 | train_accuracy: 0.78321 | valid_accuracy: 0.78547 |  0:03:56s\n",
            "epoch 78 | loss: 0.4673  | train_accuracy: 0.78581 | valid_accuracy: 0.78547 |  0:03:59s\n",
            "epoch 79 | loss: 0.45897 | train_accuracy: 0.78235 | valid_accuracy: 0.7872  |  0:04:02s\n",
            "epoch 80 | loss: 0.46656 | train_accuracy: 0.78494 | valid_accuracy: 0.78201 |  0:04:05s\n",
            "epoch 81 | loss: 0.45366 | train_accuracy: 0.78494 | valid_accuracy: 0.78547 |  0:04:08s\n",
            "epoch 82 | loss: 0.45865 | train_accuracy: 0.78408 | valid_accuracy: 0.79066 |  0:04:11s\n",
            "epoch 83 | loss: 0.45693 | train_accuracy: 0.78191 | valid_accuracy: 0.7872  |  0:04:14s\n",
            "epoch 84 | loss: 0.45195 | train_accuracy: 0.78364 | valid_accuracy: 0.7872  |  0:04:17s\n",
            "epoch 85 | loss: 0.46347 | train_accuracy: 0.78494 | valid_accuracy: 0.7872  |  0:04:20s\n",
            "epoch 86 | loss: 0.45139 | train_accuracy: 0.78364 | valid_accuracy: 0.79066 |  0:04:23s\n",
            "epoch 87 | loss: 0.45843 | train_accuracy: 0.78408 | valid_accuracy: 0.7872  |  0:04:26s\n",
            "epoch 88 | loss: 0.45839 | train_accuracy: 0.78711 | valid_accuracy: 0.79066 |  0:04:29s\n",
            "epoch 89 | loss: 0.46047 | train_accuracy: 0.78364 | valid_accuracy: 0.7872  |  0:04:32s\n",
            "epoch 90 | loss: 0.4626  | train_accuracy: 0.78494 | valid_accuracy: 0.7872  |  0:04:35s\n",
            "epoch 91 | loss: 0.45338 | train_accuracy: 0.78364 | valid_accuracy: 0.78893 |  0:04:38s\n",
            "epoch 92 | loss: 0.4606  | train_accuracy: 0.78364 | valid_accuracy: 0.78893 |  0:04:41s\n",
            "epoch 93 | loss: 0.45799 | train_accuracy: 0.78408 | valid_accuracy: 0.7872  |  0:04:44s\n",
            "epoch 94 | loss: 0.46303 | train_accuracy: 0.78321 | valid_accuracy: 0.78374 |  0:04:47s\n",
            "epoch 95 | loss: 0.46266 | train_accuracy: 0.78494 | valid_accuracy: 0.78893 |  0:04:50s\n",
            "epoch 96 | loss: 0.4626  | train_accuracy: 0.78408 | valid_accuracy: 0.78374 |  0:04:53s\n",
            "epoch 97 | loss: 0.45405 | train_accuracy: 0.78537 | valid_accuracy: 0.7872  |  0:04:56s\n",
            "epoch 98 | loss: 0.45786 | train_accuracy: 0.78278 | valid_accuracy: 0.78893 |  0:04:59s\n",
            "epoch 99 | loss: 0.45794 | train_accuracy: 0.78191 | valid_accuracy: 0.78893 |  0:05:02s\n",
            "Device used : cuda\n",
            "No early stopping will be performed, last training weights will be used.\n",
            "epoch 0  | loss: 0.8746  | train_accuracy: 0.50974 | valid_accuracy: 0.47059 |  0:00:02s\n",
            "epoch 1  | loss: 0.67793 | train_accuracy: 0.57464 | valid_accuracy: 0.54152 |  0:00:05s\n",
            "epoch 2  | loss: 0.6315  | train_accuracy: 0.66681 | valid_accuracy: 0.67128 |  0:00:09s\n",
            "epoch 3  | loss: 0.58857 | train_accuracy: 0.71874 | valid_accuracy: 0.72145 |  0:00:11s\n",
            "epoch 4  | loss: 0.56695 | train_accuracy: 0.73042 | valid_accuracy: 0.74048 |  0:00:14s\n",
            "epoch 5  | loss: 0.53391 | train_accuracy: 0.73864 | valid_accuracy: 0.75952 |  0:00:17s\n",
            "epoch 6  | loss: 0.51941 | train_accuracy: 0.746   | valid_accuracy: 0.75606 |  0:00:20s\n",
            "epoch 7  | loss: 0.50964 | train_accuracy: 0.75206 | valid_accuracy: 0.76644 |  0:00:23s\n",
            "epoch 8  | loss: 0.50398 | train_accuracy: 0.76028 | valid_accuracy: 0.76817 |  0:00:26s\n",
            "epoch 9  | loss: 0.51477 | train_accuracy: 0.76287 | valid_accuracy: 0.75952 |  0:00:29s\n",
            "epoch 10 | loss: 0.4912  | train_accuracy: 0.7685  | valid_accuracy: 0.76471 |  0:00:32s\n",
            "epoch 11 | loss: 0.4973  | train_accuracy: 0.76504 | valid_accuracy: 0.77336 |  0:00:36s\n",
            "epoch 12 | loss: 0.48241 | train_accuracy: 0.77153 | valid_accuracy: 0.78201 |  0:00:39s\n",
            "epoch 13 | loss: 0.48966 | train_accuracy: 0.77109 | valid_accuracy: 0.76817 |  0:00:42s\n",
            "epoch 14 | loss: 0.47881 | train_accuracy: 0.77629 | valid_accuracy: 0.76817 |  0:00:45s\n",
            "epoch 15 | loss: 0.48395 | train_accuracy: 0.78018 | valid_accuracy: 0.76817 |  0:00:48s\n",
            "epoch 16 | loss: 0.48153 | train_accuracy: 0.77932 | valid_accuracy: 0.76817 |  0:00:51s\n",
            "epoch 17 | loss: 0.47816 | train_accuracy: 0.77369 | valid_accuracy: 0.77682 |  0:00:54s\n",
            "epoch 18 | loss: 0.46355 | train_accuracy: 0.77196 | valid_accuracy: 0.77509 |  0:00:57s\n",
            "epoch 19 | loss: 0.467   | train_accuracy: 0.77715 | valid_accuracy: 0.77509 |  0:01:00s\n",
            "epoch 20 | loss: 0.47085 | train_accuracy: 0.77585 | valid_accuracy: 0.77682 |  0:01:03s\n",
            "epoch 21 | loss: 0.47729 | train_accuracy: 0.77456 | valid_accuracy: 0.78374 |  0:01:06s\n",
            "epoch 22 | loss: 0.46589 | train_accuracy: 0.77542 | valid_accuracy: 0.78028 |  0:01:09s\n",
            "epoch 23 | loss: 0.46885 | train_accuracy: 0.77369 | valid_accuracy: 0.78547 |  0:01:12s\n",
            "epoch 24 | loss: 0.46072 | train_accuracy: 0.77629 | valid_accuracy: 0.77509 |  0:01:15s\n",
            "epoch 25 | loss: 0.46779 | train_accuracy: 0.77715 | valid_accuracy: 0.77682 |  0:01:18s\n",
            "epoch 26 | loss: 0.47222 | train_accuracy: 0.78235 | valid_accuracy: 0.76817 |  0:01:21s\n",
            "epoch 27 | loss: 0.45637 | train_accuracy: 0.77888 | valid_accuracy: 0.77682 |  0:01:24s\n",
            "epoch 28 | loss: 0.47202 | train_accuracy: 0.78018 | valid_accuracy: 0.77163 |  0:01:27s\n",
            "epoch 29 | loss: 0.46413 | train_accuracy: 0.78105 | valid_accuracy: 0.77509 |  0:01:30s\n",
            "epoch 30 | loss: 0.46346 | train_accuracy: 0.78061 | valid_accuracy: 0.77336 |  0:01:33s\n",
            "epoch 31 | loss: 0.45566 | train_accuracy: 0.78018 | valid_accuracy: 0.77682 |  0:01:36s\n",
            "epoch 32 | loss: 0.46902 | train_accuracy: 0.77672 | valid_accuracy: 0.77509 |  0:01:39s\n",
            "epoch 33 | loss: 0.46393 | train_accuracy: 0.77932 | valid_accuracy: 0.77509 |  0:01:42s\n",
            "epoch 34 | loss: 0.47072 | train_accuracy: 0.77802 | valid_accuracy: 0.77855 |  0:01:45s\n",
            "epoch 35 | loss: 0.46112 | train_accuracy: 0.78148 | valid_accuracy: 0.77855 |  0:01:48s\n",
            "epoch 36 | loss: 0.45726 | train_accuracy: 0.77672 | valid_accuracy: 0.77509 |  0:01:51s\n",
            "epoch 37 | loss: 0.46005 | train_accuracy: 0.78061 | valid_accuracy: 0.77163 |  0:01:54s\n",
            "epoch 38 | loss: 0.45962 | train_accuracy: 0.78321 | valid_accuracy: 0.77336 |  0:01:57s\n",
            "epoch 39 | loss: 0.45912 | train_accuracy: 0.77802 | valid_accuracy: 0.77509 |  0:02:00s\n",
            "epoch 40 | loss: 0.45853 | train_accuracy: 0.77975 | valid_accuracy: 0.78028 |  0:02:03s\n",
            "epoch 41 | loss: 0.46474 | train_accuracy: 0.78061 | valid_accuracy: 0.78028 |  0:02:06s\n",
            "epoch 42 | loss: 0.46377 | train_accuracy: 0.78061 | valid_accuracy: 0.77509 |  0:02:09s\n",
            "epoch 43 | loss: 0.45363 | train_accuracy: 0.78191 | valid_accuracy: 0.77682 |  0:02:12s\n",
            "epoch 44 | loss: 0.46037 | train_accuracy: 0.78278 | valid_accuracy: 0.78374 |  0:02:15s\n",
            "epoch 45 | loss: 0.45448 | train_accuracy: 0.77759 | valid_accuracy: 0.78028 |  0:02:18s\n",
            "epoch 46 | loss: 0.44968 | train_accuracy: 0.77932 | valid_accuracy: 0.77509 |  0:02:21s\n",
            "epoch 47 | loss: 0.46912 | train_accuracy: 0.78061 | valid_accuracy: 0.78028 |  0:02:24s\n",
            "epoch 48 | loss: 0.46047 | train_accuracy: 0.77975 | valid_accuracy: 0.78028 |  0:02:27s\n",
            "epoch 49 | loss: 0.46062 | train_accuracy: 0.78105 | valid_accuracy: 0.77855 |  0:02:30s\n",
            "epoch 50 | loss: 0.45732 | train_accuracy: 0.78191 | valid_accuracy: 0.77855 |  0:02:33s\n",
            "epoch 51 | loss: 0.46085 | train_accuracy: 0.78408 | valid_accuracy: 0.78028 |  0:02:36s\n",
            "epoch 52 | loss: 0.4591  | train_accuracy: 0.78105 | valid_accuracy: 0.77682 |  0:02:39s\n",
            "epoch 53 | loss: 0.46363 | train_accuracy: 0.78235 | valid_accuracy: 0.77682 |  0:02:42s\n",
            "epoch 54 | loss: 0.45812 | train_accuracy: 0.78408 | valid_accuracy: 0.77855 |  0:02:45s\n",
            "epoch 55 | loss: 0.46496 | train_accuracy: 0.78451 | valid_accuracy: 0.78028 |  0:02:48s\n",
            "epoch 56 | loss: 0.45616 | train_accuracy: 0.78191 | valid_accuracy: 0.78028 |  0:02:51s\n",
            "epoch 57 | loss: 0.45212 | train_accuracy: 0.78018 | valid_accuracy: 0.77855 |  0:02:54s\n",
            "epoch 58 | loss: 0.46833 | train_accuracy: 0.77932 | valid_accuracy: 0.77509 |  0:02:57s\n",
            "epoch 59 | loss: 0.45765 | train_accuracy: 0.78364 | valid_accuracy: 0.77682 |  0:03:00s\n",
            "epoch 60 | loss: 0.46995 | train_accuracy: 0.78408 | valid_accuracy: 0.78028 |  0:03:03s\n",
            "epoch 61 | loss: 0.46438 | train_accuracy: 0.78364 | valid_accuracy: 0.77855 |  0:03:06s\n",
            "epoch 62 | loss: 0.45    | train_accuracy: 0.78235 | valid_accuracy: 0.78374 |  0:03:09s\n",
            "epoch 63 | loss: 0.44988 | train_accuracy: 0.78191 | valid_accuracy: 0.78028 |  0:03:12s\n",
            "epoch 64 | loss: 0.44869 | train_accuracy: 0.78321 | valid_accuracy: 0.78028 |  0:03:15s\n",
            "epoch 65 | loss: 0.45335 | train_accuracy: 0.78105 | valid_accuracy: 0.77682 |  0:03:18s\n",
            "epoch 66 | loss: 0.45459 | train_accuracy: 0.78321 | valid_accuracy: 0.77855 |  0:03:21s\n",
            "epoch 67 | loss: 0.46766 | train_accuracy: 0.78235 | valid_accuracy: 0.77509 |  0:03:24s\n",
            "epoch 68 | loss: 0.45315 | train_accuracy: 0.78451 | valid_accuracy: 0.77682 |  0:03:27s\n",
            "epoch 69 | loss: 0.44794 | train_accuracy: 0.78278 | valid_accuracy: 0.77855 |  0:03:30s\n",
            "epoch 70 | loss: 0.45881 | train_accuracy: 0.78364 | valid_accuracy: 0.78201 |  0:03:33s\n",
            "epoch 71 | loss: 0.46177 | train_accuracy: 0.77888 | valid_accuracy: 0.77682 |  0:03:36s\n",
            "epoch 72 | loss: 0.45205 | train_accuracy: 0.78061 | valid_accuracy: 0.77855 |  0:03:39s\n",
            "epoch 73 | loss: 0.45199 | train_accuracy: 0.78321 | valid_accuracy: 0.78201 |  0:03:42s\n",
            "epoch 74 | loss: 0.45375 | train_accuracy: 0.78235 | valid_accuracy: 0.77855 |  0:03:45s\n",
            "epoch 75 | loss: 0.45811 | train_accuracy: 0.78278 | valid_accuracy: 0.77855 |  0:03:48s\n",
            "epoch 76 | loss: 0.45918 | train_accuracy: 0.78148 | valid_accuracy: 0.77855 |  0:03:51s\n",
            "epoch 77 | loss: 0.47284 | train_accuracy: 0.78278 | valid_accuracy: 0.77509 |  0:03:54s\n",
            "epoch 78 | loss: 0.45934 | train_accuracy: 0.78321 | valid_accuracy: 0.77509 |  0:03:57s\n",
            "epoch 79 | loss: 0.46423 | train_accuracy: 0.78278 | valid_accuracy: 0.77509 |  0:04:00s\n",
            "epoch 80 | loss: 0.45669 | train_accuracy: 0.78061 | valid_accuracy: 0.77509 |  0:04:03s\n",
            "epoch 81 | loss: 0.45556 | train_accuracy: 0.78364 | valid_accuracy: 0.77682 |  0:04:06s\n",
            "epoch 82 | loss: 0.45796 | train_accuracy: 0.78494 | valid_accuracy: 0.77682 |  0:04:09s\n",
            "epoch 83 | loss: 0.45915 | train_accuracy: 0.78408 | valid_accuracy: 0.77682 |  0:04:12s\n",
            "epoch 84 | loss: 0.45382 | train_accuracy: 0.78451 | valid_accuracy: 0.77682 |  0:04:15s\n",
            "epoch 85 | loss: 0.46161 | train_accuracy: 0.78364 | valid_accuracy: 0.77682 |  0:04:18s\n",
            "epoch 86 | loss: 0.46279 | train_accuracy: 0.78278 | valid_accuracy: 0.77509 |  0:04:21s\n",
            "epoch 87 | loss: 0.45855 | train_accuracy: 0.77975 | valid_accuracy: 0.77682 |  0:04:24s\n",
            "epoch 88 | loss: 0.45239 | train_accuracy: 0.78191 | valid_accuracy: 0.77682 |  0:04:27s\n",
            "epoch 89 | loss: 0.46805 | train_accuracy: 0.78408 | valid_accuracy: 0.77855 |  0:04:30s\n",
            "epoch 90 | loss: 0.46134 | train_accuracy: 0.78364 | valid_accuracy: 0.77855 |  0:04:33s\n",
            "epoch 91 | loss: 0.45896 | train_accuracy: 0.77975 | valid_accuracy: 0.77682 |  0:04:36s\n",
            "epoch 92 | loss: 0.47236 | train_accuracy: 0.78191 | valid_accuracy: 0.77682 |  0:04:39s\n",
            "epoch 93 | loss: 0.45159 | train_accuracy: 0.78451 | valid_accuracy: 0.77855 |  0:04:42s\n",
            "epoch 94 | loss: 0.45613 | train_accuracy: 0.78191 | valid_accuracy: 0.78028 |  0:04:45s\n",
            "epoch 95 | loss: 0.45704 | train_accuracy: 0.77932 | valid_accuracy: 0.77509 |  0:04:48s\n",
            "epoch 96 | loss: 0.45754 | train_accuracy: 0.78494 | valid_accuracy: 0.77682 |  0:04:51s\n",
            "epoch 97 | loss: 0.44896 | train_accuracy: 0.78278 | valid_accuracy: 0.77855 |  0:04:54s\n",
            "epoch 98 | loss: 0.45898 | train_accuracy: 0.78364 | valid_accuracy: 0.78374 |  0:04:57s\n",
            "epoch 99 | loss: 0.45509 | train_accuracy: 0.78408 | valid_accuracy: 0.77855 |  0:05:00s\n",
            "Device used : cuda\n",
            "No early stopping will be performed, last training weights will be used.\n",
            "epoch 0  | loss: 0.8646  | train_accuracy: 0.51233 | valid_accuracy: 0.50519 |  0:00:02s\n",
            "epoch 1  | loss: 0.67841 | train_accuracy: 0.56642 | valid_accuracy: 0.53979 |  0:00:05s\n",
            "epoch 2  | loss: 0.62559 | train_accuracy: 0.67979 | valid_accuracy: 0.64187 |  0:00:08s\n",
            "epoch 3  | loss: 0.57643 | train_accuracy: 0.73518 | valid_accuracy: 0.71107 |  0:00:11s\n",
            "epoch 4  | loss: 0.53712 | train_accuracy: 0.75032 | valid_accuracy: 0.75606 |  0:00:14s\n",
            "epoch 5  | loss: 0.52033 | train_accuracy: 0.75552 | valid_accuracy: 0.75606 |  0:00:17s\n",
            "epoch 6  | loss: 0.50534 | train_accuracy: 0.76071 | valid_accuracy: 0.74567 |  0:00:20s\n",
            "epoch 7  | loss: 0.49419 | train_accuracy: 0.75984 | valid_accuracy: 0.75087 |  0:00:23s\n",
            "epoch 8  | loss: 0.48784 | train_accuracy: 0.7672  | valid_accuracy: 0.7526  |  0:00:26s\n",
            "epoch 9  | loss: 0.47709 | train_accuracy: 0.76936 | valid_accuracy: 0.7526  |  0:00:29s\n",
            "epoch 10 | loss: 0.46277 | train_accuracy: 0.77066 | valid_accuracy: 0.74567 |  0:00:32s\n",
            "epoch 11 | loss: 0.47044 | train_accuracy: 0.77542 | valid_accuracy: 0.75087 |  0:00:35s\n",
            "epoch 12 | loss: 0.45358 | train_accuracy: 0.77932 | valid_accuracy: 0.7526  |  0:00:38s\n",
            "epoch 13 | loss: 0.45936 | train_accuracy: 0.78061 | valid_accuracy: 0.75779 |  0:00:41s\n",
            "epoch 14 | loss: 0.45847 | train_accuracy: 0.78061 | valid_accuracy: 0.75952 |  0:00:45s\n",
            "epoch 15 | loss: 0.45368 | train_accuracy: 0.77715 | valid_accuracy: 0.75779 |  0:00:48s\n",
            "epoch 16 | loss: 0.45626 | train_accuracy: 0.78148 | valid_accuracy: 0.75952 |  0:00:51s\n",
            "epoch 17 | loss: 0.46112 | train_accuracy: 0.78581 | valid_accuracy: 0.76471 |  0:00:54s\n",
            "epoch 18 | loss: 0.45003 | train_accuracy: 0.78884 | valid_accuracy: 0.76471 |  0:00:57s\n",
            "epoch 19 | loss: 0.45829 | train_accuracy: 0.78884 | valid_accuracy: 0.76471 |  0:01:00s\n",
            "epoch 20 | loss: 0.4542  | train_accuracy: 0.78711 | valid_accuracy: 0.76644 |  0:01:03s\n",
            "epoch 21 | loss: 0.45878 | train_accuracy: 0.78364 | valid_accuracy: 0.76644 |  0:01:06s\n",
            "epoch 22 | loss: 0.44957 | train_accuracy: 0.78581 | valid_accuracy: 0.76644 |  0:01:09s\n",
            "epoch 23 | loss: 0.45478 | train_accuracy: 0.78797 | valid_accuracy: 0.76644 |  0:01:12s\n",
            "epoch 24 | loss: 0.46099 | train_accuracy: 0.78927 | valid_accuracy: 0.76817 |  0:01:15s\n",
            "epoch 25 | loss: 0.45492 | train_accuracy: 0.7884  | valid_accuracy: 0.76644 |  0:01:18s\n",
            "epoch 26 | loss: 0.45557 | train_accuracy: 0.78754 | valid_accuracy: 0.76644 |  0:01:21s\n",
            "epoch 27 | loss: 0.44998 | train_accuracy: 0.78927 | valid_accuracy: 0.77163 |  0:01:24s\n",
            "epoch 28 | loss: 0.44648 | train_accuracy: 0.78754 | valid_accuracy: 0.77163 |  0:01:27s\n",
            "epoch 29 | loss: 0.44135 | train_accuracy: 0.78927 | valid_accuracy: 0.7699  |  0:01:30s\n",
            "epoch 30 | loss: 0.45369 | train_accuracy: 0.7897  | valid_accuracy: 0.7699  |  0:01:33s\n",
            "epoch 31 | loss: 0.44867 | train_accuracy: 0.79013 | valid_accuracy: 0.77336 |  0:01:36s\n",
            "epoch 32 | loss: 0.44598 | train_accuracy: 0.78754 | valid_accuracy: 0.77336 |  0:01:39s\n",
            "epoch 33 | loss: 0.45257 | train_accuracy: 0.791   | valid_accuracy: 0.77509 |  0:01:42s\n",
            "epoch 34 | loss: 0.44622 | train_accuracy: 0.79057 | valid_accuracy: 0.77682 |  0:01:45s\n",
            "epoch 35 | loss: 0.44008 | train_accuracy: 0.78927 | valid_accuracy: 0.77163 |  0:01:48s\n",
            "epoch 36 | loss: 0.44877 | train_accuracy: 0.79143 | valid_accuracy: 0.77509 |  0:01:51s\n",
            "epoch 37 | loss: 0.45681 | train_accuracy: 0.79057 | valid_accuracy: 0.77163 |  0:01:54s\n",
            "epoch 38 | loss: 0.44434 | train_accuracy: 0.79013 | valid_accuracy: 0.77682 |  0:01:57s\n",
            "epoch 39 | loss: 0.44568 | train_accuracy: 0.79057 | valid_accuracy: 0.77336 |  0:02:00s\n",
            "epoch 40 | loss: 0.45183 | train_accuracy: 0.79013 | valid_accuracy: 0.77336 |  0:02:03s\n",
            "epoch 41 | loss: 0.4494  | train_accuracy: 0.79143 | valid_accuracy: 0.77855 |  0:02:06s\n",
            "epoch 42 | loss: 0.4413  | train_accuracy: 0.79143 | valid_accuracy: 0.77509 |  0:02:09s\n",
            "epoch 43 | loss: 0.44676 | train_accuracy: 0.791   | valid_accuracy: 0.77509 |  0:02:12s\n",
            "epoch 44 | loss: 0.44622 | train_accuracy: 0.7897  | valid_accuracy: 0.77336 |  0:02:15s\n",
            "epoch 45 | loss: 0.44134 | train_accuracy: 0.79013 | valid_accuracy: 0.77509 |  0:02:18s\n",
            "epoch 46 | loss: 0.43521 | train_accuracy: 0.791   | valid_accuracy: 0.77855 |  0:02:21s\n",
            "epoch 47 | loss: 0.44069 | train_accuracy: 0.791   | valid_accuracy: 0.77336 |  0:02:24s\n",
            "epoch 48 | loss: 0.45077 | train_accuracy: 0.791   | valid_accuracy: 0.77336 |  0:02:27s\n",
            "epoch 49 | loss: 0.44362 | train_accuracy: 0.78927 | valid_accuracy: 0.77336 |  0:02:30s\n",
            "epoch 50 | loss: 0.45063 | train_accuracy: 0.7897  | valid_accuracy: 0.77509 |  0:02:33s\n",
            "epoch 51 | loss: 0.44416 | train_accuracy: 0.79143 | valid_accuracy: 0.77336 |  0:02:36s\n",
            "epoch 52 | loss: 0.43639 | train_accuracy: 0.79186 | valid_accuracy: 0.77163 |  0:02:39s\n",
            "epoch 53 | loss: 0.43799 | train_accuracy: 0.79143 | valid_accuracy: 0.76644 |  0:02:42s\n",
            "epoch 54 | loss: 0.4524  | train_accuracy: 0.79143 | valid_accuracy: 0.77336 |  0:02:45s\n",
            "epoch 55 | loss: 0.45448 | train_accuracy: 0.79057 | valid_accuracy: 0.7699  |  0:02:48s\n",
            "epoch 56 | loss: 0.43968 | train_accuracy: 0.7923  | valid_accuracy: 0.77336 |  0:02:51s\n",
            "epoch 57 | loss: 0.43976 | train_accuracy: 0.791   | valid_accuracy: 0.7699  |  0:02:54s\n",
            "epoch 58 | loss: 0.44077 | train_accuracy: 0.7923  | valid_accuracy: 0.77509 |  0:02:57s\n",
            "epoch 59 | loss: 0.43979 | train_accuracy: 0.791   | valid_accuracy: 0.77682 |  0:03:00s\n",
            "epoch 60 | loss: 0.44718 | train_accuracy: 0.79316 | valid_accuracy: 0.77509 |  0:03:03s\n",
            "epoch 61 | loss: 0.44397 | train_accuracy: 0.7936  | valid_accuracy: 0.77682 |  0:03:06s\n",
            "epoch 62 | loss: 0.44473 | train_accuracy: 0.79316 | valid_accuracy: 0.77509 |  0:03:09s\n",
            "epoch 63 | loss: 0.44172 | train_accuracy: 0.791   | valid_accuracy: 0.77163 |  0:03:12s\n",
            "epoch 64 | loss: 0.44171 | train_accuracy: 0.7897  | valid_accuracy: 0.77163 |  0:03:15s\n",
            "epoch 65 | loss: 0.4369  | train_accuracy: 0.79186 | valid_accuracy: 0.77336 |  0:03:18s\n",
            "epoch 66 | loss: 0.45836 | train_accuracy: 0.7923  | valid_accuracy: 0.77682 |  0:03:21s\n",
            "epoch 67 | loss: 0.4487  | train_accuracy: 0.7923  | valid_accuracy: 0.77336 |  0:03:24s\n",
            "epoch 68 | loss: 0.44036 | train_accuracy: 0.79013 | valid_accuracy: 0.77509 |  0:03:27s\n",
            "epoch 69 | loss: 0.44978 | train_accuracy: 0.79186 | valid_accuracy: 0.77336 |  0:03:30s\n",
            "epoch 70 | loss: 0.43806 | train_accuracy: 0.79143 | valid_accuracy: 0.77163 |  0:03:33s\n",
            "epoch 71 | loss: 0.4463  | train_accuracy: 0.79186 | valid_accuracy: 0.77509 |  0:03:36s\n",
            "epoch 72 | loss: 0.45048 | train_accuracy: 0.79143 | valid_accuracy: 0.77509 |  0:03:39s\n",
            "epoch 73 | loss: 0.43625 | train_accuracy: 0.79143 | valid_accuracy: 0.77509 |  0:03:41s\n",
            "epoch 74 | loss: 0.44195 | train_accuracy: 0.7923  | valid_accuracy: 0.77509 |  0:03:44s\n",
            "epoch 75 | loss: 0.44568 | train_accuracy: 0.79143 | valid_accuracy: 0.77509 |  0:03:47s\n",
            "epoch 76 | loss: 0.44532 | train_accuracy: 0.79057 | valid_accuracy: 0.77336 |  0:03:50s\n",
            "epoch 77 | loss: 0.44679 | train_accuracy: 0.79316 | valid_accuracy: 0.77336 |  0:03:54s\n",
            "epoch 78 | loss: 0.44011 | train_accuracy: 0.7936  | valid_accuracy: 0.77509 |  0:03:56s\n",
            "epoch 79 | loss: 0.45199 | train_accuracy: 0.79273 | valid_accuracy: 0.77336 |  0:04:00s\n",
            "epoch 80 | loss: 0.44433 | train_accuracy: 0.79186 | valid_accuracy: 0.77336 |  0:04:03s\n",
            "epoch 81 | loss: 0.44535 | train_accuracy: 0.79057 | valid_accuracy: 0.77509 |  0:04:06s\n",
            "epoch 82 | loss: 0.44281 | train_accuracy: 0.7936  | valid_accuracy: 0.77336 |  0:04:09s\n",
            "epoch 83 | loss: 0.44903 | train_accuracy: 0.79273 | valid_accuracy: 0.77509 |  0:04:12s\n",
            "epoch 84 | loss: 0.44652 | train_accuracy: 0.791   | valid_accuracy: 0.77509 |  0:04:15s\n",
            "epoch 85 | loss: 0.43938 | train_accuracy: 0.791   | valid_accuracy: 0.77509 |  0:04:18s\n",
            "epoch 86 | loss: 0.43361 | train_accuracy: 0.7923  | valid_accuracy: 0.77336 |  0:04:21s\n",
            "epoch 87 | loss: 0.44954 | train_accuracy: 0.79057 | valid_accuracy: 0.77509 |  0:04:24s\n",
            "epoch 88 | loss: 0.44592 | train_accuracy: 0.79186 | valid_accuracy: 0.77509 |  0:04:27s\n",
            "epoch 89 | loss: 0.44313 | train_accuracy: 0.79186 | valid_accuracy: 0.77509 |  0:04:30s\n",
            "epoch 90 | loss: 0.44006 | train_accuracy: 0.79143 | valid_accuracy: 0.77163 |  0:04:33s\n",
            "epoch 91 | loss: 0.43772 | train_accuracy: 0.79057 | valid_accuracy: 0.77163 |  0:04:36s\n",
            "epoch 92 | loss: 0.4458  | train_accuracy: 0.79143 | valid_accuracy: 0.77336 |  0:04:39s\n",
            "epoch 93 | loss: 0.44243 | train_accuracy: 0.7923  | valid_accuracy: 0.77682 |  0:04:42s\n",
            "epoch 94 | loss: 0.44357 | train_accuracy: 0.79057 | valid_accuracy: 0.77509 |  0:04:45s\n",
            "epoch 95 | loss: 0.44061 | train_accuracy: 0.7923  | valid_accuracy: 0.77682 |  0:04:48s\n",
            "epoch 96 | loss: 0.44121 | train_accuracy: 0.7923  | valid_accuracy: 0.77336 |  0:04:51s\n",
            "epoch 97 | loss: 0.44012 | train_accuracy: 0.79186 | valid_accuracy: 0.77509 |  0:04:54s\n",
            "epoch 98 | loss: 0.44671 | train_accuracy: 0.79057 | valid_accuracy: 0.7699  |  0:04:57s\n",
            "epoch 99 | loss: 0.44313 | train_accuracy: 0.7923  | valid_accuracy: 0.77336 |  0:05:00s\n",
            "Device used : cuda\n",
            "No early stopping will be performed, last training weights will be used.\n",
            "epoch 0  | loss: 0.85874 | train_accuracy: 0.49784 | valid_accuracy: 0.52686 |  0:00:03s\n",
            "epoch 1  | loss: 0.65393 | train_accuracy: 0.58478 | valid_accuracy: 0.58752 |  0:00:06s\n",
            "epoch 2  | loss: 0.60522 | train_accuracy: 0.70156 | valid_accuracy: 0.65511 |  0:00:09s\n",
            "epoch 3  | loss: 0.55517 | train_accuracy: 0.75303 | valid_accuracy: 0.71577 |  0:00:12s\n",
            "epoch 4  | loss: 0.53532 | train_accuracy: 0.76211 | valid_accuracy: 0.7331  |  0:00:15s\n",
            "epoch 5  | loss: 0.51209 | train_accuracy: 0.75822 | valid_accuracy: 0.7435  |  0:00:18s\n",
            "epoch 6  | loss: 0.49195 | train_accuracy: 0.76125 | valid_accuracy: 0.7435  |  0:00:21s\n",
            "epoch 7  | loss: 0.48539 | train_accuracy: 0.76903 | valid_accuracy: 0.7487  |  0:00:24s\n",
            "epoch 8  | loss: 0.47878 | train_accuracy: 0.77292 | valid_accuracy: 0.74523 |  0:00:27s\n",
            "epoch 9  | loss: 0.4778  | train_accuracy: 0.77336 | valid_accuracy: 0.7487  |  0:00:30s\n",
            "epoch 10 | loss: 0.47157 | train_accuracy: 0.77595 | valid_accuracy: 0.7487  |  0:00:33s\n",
            "epoch 11 | loss: 0.47572 | train_accuracy: 0.77552 | valid_accuracy: 0.74697 |  0:00:36s\n",
            "epoch 12 | loss: 0.46555 | train_accuracy: 0.77595 | valid_accuracy: 0.74697 |  0:00:39s\n",
            "epoch 13 | loss: 0.46604 | train_accuracy: 0.77292 | valid_accuracy: 0.73484 |  0:00:42s\n",
            "epoch 14 | loss: 0.46301 | train_accuracy: 0.77638 | valid_accuracy: 0.75043 |  0:00:45s\n",
            "epoch 15 | loss: 0.46305 | train_accuracy: 0.77941 | valid_accuracy: 0.7383  |  0:00:48s\n",
            "epoch 16 | loss: 0.46299 | train_accuracy: 0.78028 | valid_accuracy: 0.74177 |  0:00:51s\n",
            "epoch 17 | loss: 0.46691 | train_accuracy: 0.78157 | valid_accuracy: 0.74523 |  0:00:54s\n",
            "epoch 18 | loss: 0.46383 | train_accuracy: 0.78071 | valid_accuracy: 0.7487  |  0:00:57s\n",
            "epoch 19 | loss: 0.46561 | train_accuracy: 0.77941 | valid_accuracy: 0.74697 |  0:01:00s\n",
            "epoch 20 | loss: 0.45479 | train_accuracy: 0.77984 | valid_accuracy: 0.7539  |  0:01:03s\n",
            "epoch 21 | loss: 0.45703 | train_accuracy: 0.77984 | valid_accuracy: 0.7539  |  0:01:06s\n",
            "epoch 22 | loss: 0.45423 | train_accuracy: 0.78114 | valid_accuracy: 0.75737 |  0:01:09s\n",
            "epoch 23 | loss: 0.45256 | train_accuracy: 0.78114 | valid_accuracy: 0.7539  |  0:01:12s\n",
            "epoch 24 | loss: 0.45553 | train_accuracy: 0.78157 | valid_accuracy: 0.7539  |  0:01:15s\n",
            "epoch 25 | loss: 0.45462 | train_accuracy: 0.78201 | valid_accuracy: 0.7539  |  0:01:18s\n",
            "epoch 26 | loss: 0.45386 | train_accuracy: 0.78417 | valid_accuracy: 0.75737 |  0:01:21s\n",
            "epoch 27 | loss: 0.44887 | train_accuracy: 0.78417 | valid_accuracy: 0.75217 |  0:01:24s\n",
            "epoch 28 | loss: 0.44827 | train_accuracy: 0.78633 | valid_accuracy: 0.75563 |  0:01:27s\n",
            "epoch 29 | loss: 0.44667 | train_accuracy: 0.78676 | valid_accuracy: 0.7591  |  0:01:30s\n",
            "epoch 30 | loss: 0.45171 | train_accuracy: 0.7859  | valid_accuracy: 0.7591  |  0:01:33s\n",
            "epoch 31 | loss: 0.45233 | train_accuracy: 0.78547 | valid_accuracy: 0.75737 |  0:01:36s\n",
            "epoch 32 | loss: 0.44694 | train_accuracy: 0.78503 | valid_accuracy: 0.76083 |  0:01:39s\n",
            "epoch 33 | loss: 0.44804 | train_accuracy: 0.7872  | valid_accuracy: 0.7591  |  0:01:42s\n",
            "epoch 34 | loss: 0.45228 | train_accuracy: 0.78763 | valid_accuracy: 0.7591  |  0:01:45s\n",
            "epoch 35 | loss: 0.45443 | train_accuracy: 0.78936 | valid_accuracy: 0.75563 |  0:01:48s\n",
            "epoch 36 | loss: 0.44188 | train_accuracy: 0.78763 | valid_accuracy: 0.7591  |  0:01:51s\n",
            "epoch 37 | loss: 0.45574 | train_accuracy: 0.7872  | valid_accuracy: 0.7539  |  0:01:54s\n",
            "epoch 38 | loss: 0.45244 | train_accuracy: 0.78676 | valid_accuracy: 0.75563 |  0:01:57s\n",
            "epoch 39 | loss: 0.44558 | train_accuracy: 0.78676 | valid_accuracy: 0.7591  |  0:02:00s\n",
            "epoch 40 | loss: 0.45094 | train_accuracy: 0.7859  | valid_accuracy: 0.75737 |  0:02:03s\n",
            "epoch 41 | loss: 0.4432  | train_accuracy: 0.78503 | valid_accuracy: 0.7591  |  0:02:06s\n",
            "epoch 42 | loss: 0.44359 | train_accuracy: 0.78763 | valid_accuracy: 0.7591  |  0:02:09s\n",
            "epoch 43 | loss: 0.4487  | train_accuracy: 0.78633 | valid_accuracy: 0.76256 |  0:02:12s\n",
            "epoch 44 | loss: 0.45063 | train_accuracy: 0.78806 | valid_accuracy: 0.76083 |  0:02:15s\n",
            "epoch 45 | loss: 0.44665 | train_accuracy: 0.7872  | valid_accuracy: 0.76256 |  0:02:18s\n",
            "epoch 46 | loss: 0.44909 | train_accuracy: 0.78547 | valid_accuracy: 0.76256 |  0:02:21s\n",
            "epoch 47 | loss: 0.44661 | train_accuracy: 0.78676 | valid_accuracy: 0.76083 |  0:02:24s\n",
            "epoch 48 | loss: 0.45638 | train_accuracy: 0.78547 | valid_accuracy: 0.75737 |  0:02:27s\n",
            "epoch 49 | loss: 0.44125 | train_accuracy: 0.78763 | valid_accuracy: 0.76083 |  0:02:30s\n",
            "epoch 50 | loss: 0.44707 | train_accuracy: 0.78806 | valid_accuracy: 0.76256 |  0:02:33s\n",
            "epoch 51 | loss: 0.44212 | train_accuracy: 0.78806 | valid_accuracy: 0.76256 |  0:02:36s\n",
            "epoch 52 | loss: 0.4442  | train_accuracy: 0.78893 | valid_accuracy: 0.76256 |  0:02:39s\n",
            "epoch 53 | loss: 0.4548  | train_accuracy: 0.78763 | valid_accuracy: 0.76256 |  0:02:42s\n",
            "epoch 54 | loss: 0.44777 | train_accuracy: 0.78893 | valid_accuracy: 0.7643  |  0:02:45s\n",
            "epoch 55 | loss: 0.44259 | train_accuracy: 0.78979 | valid_accuracy: 0.7643  |  0:02:48s\n",
            "epoch 56 | loss: 0.45205 | train_accuracy: 0.7859  | valid_accuracy: 0.7591  |  0:02:51s\n",
            "epoch 57 | loss: 0.44761 | train_accuracy: 0.7872  | valid_accuracy: 0.76083 |  0:02:54s\n",
            "epoch 58 | loss: 0.44976 | train_accuracy: 0.78676 | valid_accuracy: 0.76083 |  0:02:57s\n",
            "epoch 59 | loss: 0.44426 | train_accuracy: 0.78893 | valid_accuracy: 0.76256 |  0:03:00s\n",
            "epoch 60 | loss: 0.44841 | train_accuracy: 0.78676 | valid_accuracy: 0.7643  |  0:03:03s\n",
            "epoch 61 | loss: 0.44241 | train_accuracy: 0.78936 | valid_accuracy: 0.76256 |  0:03:06s\n",
            "epoch 62 | loss: 0.44676 | train_accuracy: 0.78849 | valid_accuracy: 0.7643  |  0:03:09s\n",
            "epoch 63 | loss: 0.43855 | train_accuracy: 0.78936 | valid_accuracy: 0.76083 |  0:03:12s\n",
            "epoch 64 | loss: 0.44425 | train_accuracy: 0.78763 | valid_accuracy: 0.76083 |  0:03:15s\n",
            "epoch 65 | loss: 0.44189 | train_accuracy: 0.7872  | valid_accuracy: 0.76083 |  0:03:18s\n",
            "epoch 66 | loss: 0.44226 | train_accuracy: 0.78763 | valid_accuracy: 0.7591  |  0:03:21s\n",
            "epoch 67 | loss: 0.44462 | train_accuracy: 0.78806 | valid_accuracy: 0.7643  |  0:03:24s\n",
            "epoch 68 | loss: 0.45183 | train_accuracy: 0.78849 | valid_accuracy: 0.76083 |  0:03:27s\n",
            "epoch 69 | loss: 0.45102 | train_accuracy: 0.78676 | valid_accuracy: 0.7643  |  0:03:30s\n",
            "epoch 70 | loss: 0.44037 | train_accuracy: 0.78763 | valid_accuracy: 0.76256 |  0:03:33s\n",
            "epoch 71 | loss: 0.43727 | train_accuracy: 0.7859  | valid_accuracy: 0.76083 |  0:03:36s\n",
            "epoch 72 | loss: 0.44092 | train_accuracy: 0.7872  | valid_accuracy: 0.76256 |  0:03:39s\n",
            "epoch 73 | loss: 0.44428 | train_accuracy: 0.78806 | valid_accuracy: 0.76256 |  0:03:42s\n",
            "epoch 74 | loss: 0.44997 | train_accuracy: 0.79022 | valid_accuracy: 0.76083 |  0:03:45s\n",
            "epoch 75 | loss: 0.44073 | train_accuracy: 0.78806 | valid_accuracy: 0.76256 |  0:03:48s\n",
            "epoch 76 | loss: 0.43599 | train_accuracy: 0.78763 | valid_accuracy: 0.76083 |  0:03:51s\n",
            "epoch 77 | loss: 0.44749 | train_accuracy: 0.78676 | valid_accuracy: 0.75737 |  0:03:54s\n",
            "epoch 78 | loss: 0.44765 | train_accuracy: 0.7872  | valid_accuracy: 0.7643  |  0:03:57s\n",
            "epoch 79 | loss: 0.44052 | train_accuracy: 0.7872  | valid_accuracy: 0.7643  |  0:04:00s\n",
            "epoch 80 | loss: 0.45354 | train_accuracy: 0.78849 | valid_accuracy: 0.76256 |  0:04:03s\n",
            "epoch 81 | loss: 0.44233 | train_accuracy: 0.78763 | valid_accuracy: 0.7591  |  0:04:06s\n",
            "epoch 82 | loss: 0.45053 | train_accuracy: 0.78633 | valid_accuracy: 0.7643  |  0:04:09s\n",
            "epoch 83 | loss: 0.44752 | train_accuracy: 0.7859  | valid_accuracy: 0.76083 |  0:04:12s\n",
            "epoch 84 | loss: 0.43797 | train_accuracy: 0.7872  | valid_accuracy: 0.7643  |  0:04:15s\n",
            "epoch 85 | loss: 0.44728 | train_accuracy: 0.7872  | valid_accuracy: 0.7591  |  0:04:18s\n",
            "epoch 86 | loss: 0.44732 | train_accuracy: 0.78849 | valid_accuracy: 0.7643  |  0:04:21s\n",
            "epoch 87 | loss: 0.43892 | train_accuracy: 0.78676 | valid_accuracy: 0.75563 |  0:04:24s\n",
            "epoch 88 | loss: 0.44668 | train_accuracy: 0.7872  | valid_accuracy: 0.7591  |  0:04:27s\n",
            "epoch 89 | loss: 0.44506 | train_accuracy: 0.78633 | valid_accuracy: 0.76256 |  0:04:30s\n",
            "epoch 90 | loss: 0.44561 | train_accuracy: 0.78633 | valid_accuracy: 0.76083 |  0:04:33s\n",
            "epoch 91 | loss: 0.44877 | train_accuracy: 0.78676 | valid_accuracy: 0.76083 |  0:04:36s\n",
            "epoch 92 | loss: 0.44886 | train_accuracy: 0.7859  | valid_accuracy: 0.76083 |  0:04:39s\n",
            "epoch 93 | loss: 0.44246 | train_accuracy: 0.78806 | valid_accuracy: 0.76083 |  0:04:42s\n",
            "epoch 94 | loss: 0.44519 | train_accuracy: 0.7872  | valid_accuracy: 0.76083 |  0:04:45s\n",
            "epoch 95 | loss: 0.44458 | train_accuracy: 0.78893 | valid_accuracy: 0.76256 |  0:04:48s\n",
            "epoch 96 | loss: 0.44338 | train_accuracy: 0.78893 | valid_accuracy: 0.75737 |  0:04:51s\n",
            "epoch 97 | loss: 0.4472  | train_accuracy: 0.78806 | valid_accuracy: 0.7643  |  0:04:54s\n",
            "epoch 98 | loss: 0.45026 | train_accuracy: 0.78806 | valid_accuracy: 0.7643  |  0:04:57s\n",
            "epoch 99 | loss: 0.445   | train_accuracy: 0.78763 | valid_accuracy: 0.76083 |  0:05:00s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(CV_score_array)"
      ],
      "metadata": {
        "id": "icKc--kNWpG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions =[ 0 if i < 0.5 else 1 for i in tb_cls.predict(X_test.to_numpy())]"
      ],
      "metadata": {
        "id": "zbQ5ar7FWqPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XpPhhMy6ii47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(y_test==predictions).mean() #test accuracy"
      ],
      "metadata": {
        "id": "3jo9IW1Qiw_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_probas=tb_cls.predict_proba(X_valid) #probability of validation classes"
      ],
      "metadata": {
        "id": "yzg5qmOrjObt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_probas=tb_cls.predict_proba(X_train) #probability of training classes"
      ],
      "metadata": {
        "id": "eMe6boPpjcF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_probas=tb_cls.predict_proba(X_test.to_numpy()) #probability of test classes"
      ],
      "metadata": {
        "id": "897FGZo0kJ5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_probas.shape"
      ],
      "metadata": {
        "id": "651lpJLt26KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid.shape"
      ],
      "metadata": {
        "id": "fehX_xWLBtmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gopVsCBF36un",
        "outputId": "0e9f864c-aefe-457b-8b66-a3acc5c46d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEEP-STACK"
      ],
      "metadata": {
        "id": "-yoXZVtOmtYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deepstack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEPT4TaukRN5",
        "outputId": "0061fa9d-be6b-488b-8353-21c65174f4f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepstack\n",
            "  Downloading deepstack-0.0.9-py3-none-any.whl (8.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.7/dist-packages (from deepstack) (1.0.2)\n",
            "Requirement already satisfied: tensorflow>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from deepstack) (1.14.0)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from deepstack) (1.21.6)\n",
            "Requirement already satisfied: keras>=2.2.5 in /usr/local/lib/python3.7/dist-packages (from deepstack) (2.2.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.5->deepstack) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.5->deepstack) (1.7.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.5->deepstack) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.5->deepstack) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.5->deepstack) (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.5->deepstack) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->deepstack) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->deepstack) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14.0->deepstack) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14.0->deepstack) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14.0->deepstack) (1.14.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14.0->deepstack) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14.0->deepstack) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14.0->deepstack) (0.37.1)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14.0->deepstack) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14.0->deepstack) (1.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14.0->deepstack) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14.0->deepstack) (0.5.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14.0->deepstack) (1.47.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow>=1.14.0->deepstack) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow>=1.14.0->deepstack) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow>=1.14.0->deepstack) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow>=1.14.0->deepstack) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow>=1.14.0->deepstack) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow>=1.14.0->deepstack) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.2.5->deepstack) (1.5.2)\n",
            "Installing collected packages: deepstack\n",
            "Successfully installed deepstack-0.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import deepstack\n",
        "from deepstack.base import KerasMember\n",
        "from deepstack.base import Member\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from deepstack.base import KerasMember\n",
        "from deepstack.ensemble import StackEnsemble\n",
        "from deepstack.ensemble import DirichletEnsemble"
      ],
      "metadata": {
        "id": "Vc_P6uZUmxbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import precision_score"
      ],
      "metadata": {
        "id": "ACWbdmxinerC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dirichletEnsemble"
      ],
      "metadata": {
        "id": "C8EeBXZEqd0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dirichletEnsemble = DirichletEnsemble(N=2000*4,metric=accuracy_score)"
      ],
      "metadata": {
        "id": "n8_pKBzhnfhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = tb_cls# A Keras pre-trained Model (Base-Learner)\n",
        "# A numpy tuple (X_val, y_val) or Keras Data Iterator - Validation Data for Meta-Learner\n",
        "member1 = Member(name=\"TABNET\", train_probs=X_train_probas,train_classes=y_train,val_classes=y_valid,val_probs=X_val_probas,submission_probs=X_test_probas)"
      ],
      "metadata": {
        "id": "SqSgNhggniag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = m #(Base-Learner)\n",
        "member2 = Member(name=\"1-D CNN\", train_probs=predict_prob_train1,train_classes=Y1_train,val_classes=Y1_val,val_probs=predict_prob_val1,submission_probs=predict_prob_test1)"
      ],
      "metadata": {
        "id": "pUl6Cstonkzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = mlp #(Base-Learner)\n",
        "member3 = Member(name=\"MLP\", train_probs=X2_train_prob,train_classes=y2_train,val_classes=y2_valid,val_probs=X2_valid_prob,submission_probs=X2_test_prob)"
      ],
      "metadata": {
        "id": "3WD92rHdBHmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qc4DmD5nNsjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirichletEnsemble.add_member(member1)\n",
        "dirichletEnsemble.add_member(member2)\n",
        "dirichletEnsemble.add_member(member3)"
      ],
      "metadata": {
        "id": "XHfwQI2snlge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirichletEnsemble.fit()"
      ],
      "metadata": {
        "id": "U7LfH30AnpAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirichletEnsemble.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQJ1gjapnq1m",
        "outputId": "2a35d9db-e892-4182-b77e-bbf10fda686e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TABNET - Weight: 0.6383 - accuracy_score: 0.8580\n",
            "1-D CNN - Weight: 0.0006 - accuracy_score: 0.8007\n",
            "MLP - Weight: 0.3610 - accuracy_score: 0.8603\n",
            "DirichletEnsemble accuracy_score: 0.8779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-FNJP8nxOnEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Kq17yoOpQfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNOKh-d7PC0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "wlViEKmiPTf7",
        "outputId": "e98f24c4-9964-442f-9d3f-56019efbdda3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-405-e67cf0a6d445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepstack/ensemble.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Assumption: all members have same train_batches.classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepstack/ensemble.py\u001b[0m in \u001b[0;36m_fit_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepstack/ensemble.py\u001b[0m in \u001b[0;36m_get_train_X\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_train_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_probs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_val_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepstack/ensemble.py\u001b[0m in \u001b[0;36m_get_X\u001b[0;34m(self, attrname)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmember\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1358 is out of bounds for axis 0 with size 1358"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacked Ensemble"
      ],
      "metadata": {
        "id": "zRCleYg4qhE9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_X3nVVkFo4B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1D CNN"
      ],
      "metadata": {
        "id": "3NRDKu5Uqqyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " X1= data.drop(['Unnamed: 0','Unnamed: 0.1.1','Unnamed: 0.1','final_tweets','label'],axis=1)\n",
        " Y1 = data['label']"
      ],
      "metadata": {
        "id": "FawQgEBnqr0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1,Y1, test_size=0.33, random_state=14)"
      ],
      "metadata": {
        "id": "rf9rrXd2qw56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1_train, X1_val, Y1_train, Y1_val = train_test_split(X1_train, Y1_train, test_size=0.29855, random_state=1) # 0.25 x 0.67= 0.16"
      ],
      "metadata": {
        "id": "Mbh-9jqMrHMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1_train = np.array(X1_train).reshape(-1,13,1)\n",
        "X1_test = np.array(X1_test).reshape(-1,13,1)"
      ],
      "metadata": {
        "id": "eqQJSrtXuof7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X1_train.shape,X1_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XvyOLzau3Eg",
        "outputId": "9b3dcdc8-8ece-4f62-9b77-c88d44117f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1358, 13, 1) (954, 13, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1_val.to_numpy().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdnrnBiqCzeu",
        "outputId": "ea9f3c39-63a1-49c0-d915-746035fd5eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(578, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1_val.to_numpy().reshape(-1,13,1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPpzgoGOJIED",
        "outputId": "27d3bf78-6a19-4797-d0c6-6353f5759823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(578, 13, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Convolution1D, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import adam\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "from keras.initializers import random_uniform\n",
        "\n",
        "#hyperparameters\n",
        "SEED = 1234\n",
        "np.random.seed(SEED)\n",
        "\n",
        "input_dimension = 226\n",
        "learning_rate = 0.0025\n",
        "momentum = 0.\n",
        "hidden_initializer = random_uniform(seed=SEED)\n",
        "dropout_rate = 0.8"
      ],
      "metadata": {
        "id": "CiLNCH_1vCOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Convolution1D(32, 3, input_shape= (13, 1), activation='relu'))\n",
        "model.add(Convolution1D(16, 1, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(Dense(128, input_dim=input_dimension, kernel_initializer=hidden_initializer, activation='relu'))\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(Dense(64, kernel_initializer=hidden_initializer, activation='relu'))\n",
        "model.add(Dense(2, kernel_initializer=hidden_initializer, activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=learning_rate, momentum=momentum)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['acc'])"
      ],
      "metadata": {
        "id": "BpHluIO4vT34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2227b1de-ac3c-4332-bcee-45661e40fe8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1TDIWUmvXkb",
        "outputId": "a835e0fc-99c1-4a96-8318-1cc7a6d3bc29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_13 (Conv1D)           (None, 11, 32)            128       \n",
            "_________________________________________________________________\n",
            "conv1d_14 (Conv1D)           (None, 11, 16)            528       \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 176)               0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 176)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 128)               22656     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 31,698\n",
            "Trainable params: 31,698\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = model.fit(X1_train, Y1_train, epochs=2000, batch_size=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d28RKPHva8u",
        "outputId": "2e314502-4caf-4e70-9697-abfec476b76f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "1358/1358 [==============================] - 0s 335us/step - loss: 0.6961 - acc: 0.4713\n",
            "Epoch 2/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6918 - acc: 0.5272\n",
            "Epoch 3/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6887 - acc: 0.5383\n",
            "Epoch 4/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6871 - acc: 0.5405\n",
            "Epoch 5/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6856 - acc: 0.5405\n",
            "Epoch 6/2000\n",
            "1358/1358 [==============================] - 0s 59us/step - loss: 0.6879 - acc: 0.5405\n",
            "Epoch 7/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6876 - acc: 0.5405\n",
            "Epoch 8/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6837 - acc: 0.5405\n",
            "Epoch 9/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6855 - acc: 0.5405\n",
            "Epoch 10/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6867 - acc: 0.5405\n",
            "Epoch 11/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6850 - acc: 0.5405\n",
            "Epoch 12/2000\n",
            "1358/1358 [==============================] - 0s 58us/step - loss: 0.6857 - acc: 0.5405\n",
            "Epoch 13/2000\n",
            "1358/1358 [==============================] - 0s 59us/step - loss: 0.6851 - acc: 0.5405\n",
            "Epoch 14/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6833 - acc: 0.5405\n",
            "Epoch 15/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6865 - acc: 0.5405\n",
            "Epoch 16/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6835 - acc: 0.5405\n",
            "Epoch 17/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6854 - acc: 0.5390\n",
            "Epoch 18/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6831 - acc: 0.5412\n",
            "Epoch 19/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6858 - acc: 0.5405\n",
            "Epoch 20/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6826 - acc: 0.5405\n",
            "Epoch 21/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6833 - acc: 0.5353\n",
            "Epoch 22/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6837 - acc: 0.5434\n",
            "Epoch 23/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6845 - acc: 0.5508\n",
            "Epoch 24/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6815 - acc: 0.5508\n",
            "Epoch 25/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6889 - acc: 0.5353\n",
            "Epoch 26/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6837 - acc: 0.5434\n",
            "Epoch 27/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6844 - acc: 0.5368\n",
            "Epoch 28/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6837 - acc: 0.5538\n",
            "Epoch 29/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6865 - acc: 0.5449\n",
            "Epoch 30/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6846 - acc: 0.5523\n",
            "Epoch 31/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6821 - acc: 0.5464\n",
            "Epoch 32/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6812 - acc: 0.5479\n",
            "Epoch 33/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6829 - acc: 0.5582\n",
            "Epoch 34/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6809 - acc: 0.5552\n",
            "Epoch 35/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.6834 - acc: 0.5398\n",
            "Epoch 36/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6858 - acc: 0.5405\n",
            "Epoch 37/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6820 - acc: 0.5538\n",
            "Epoch 38/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6834 - acc: 0.5390\n",
            "Epoch 39/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6821 - acc: 0.5434\n",
            "Epoch 40/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6825 - acc: 0.5457\n",
            "Epoch 41/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6841 - acc: 0.5420\n",
            "Epoch 42/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6834 - acc: 0.5479\n",
            "Epoch 43/2000\n",
            "1358/1358 [==============================] - 0s 59us/step - loss: 0.6848 - acc: 0.5434\n",
            "Epoch 44/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6805 - acc: 0.5434\n",
            "Epoch 45/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6821 - acc: 0.5486\n",
            "Epoch 46/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6824 - acc: 0.5434\n",
            "Epoch 47/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6837 - acc: 0.5361\n",
            "Epoch 48/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6855 - acc: 0.5383\n",
            "Epoch 49/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6837 - acc: 0.5567\n",
            "Epoch 50/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6846 - acc: 0.5471\n",
            "Epoch 51/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6851 - acc: 0.5361\n",
            "Epoch 52/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6826 - acc: 0.5376\n",
            "Epoch 53/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6830 - acc: 0.5574\n",
            "Epoch 54/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6860 - acc: 0.5331\n",
            "Epoch 55/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6813 - acc: 0.5405\n",
            "Epoch 56/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6832 - acc: 0.5420\n",
            "Epoch 57/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6857 - acc: 0.5295\n",
            "Epoch 58/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6823 - acc: 0.5457\n",
            "Epoch 59/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6817 - acc: 0.5449\n",
            "Epoch 60/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6827 - acc: 0.5383\n",
            "Epoch 61/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6822 - acc: 0.5405\n",
            "Epoch 62/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6826 - acc: 0.5479\n",
            "Epoch 63/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6817 - acc: 0.5538\n",
            "Epoch 64/2000\n",
            "1358/1358 [==============================] - 0s 58us/step - loss: 0.6814 - acc: 0.5361\n",
            "Epoch 65/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6816 - acc: 0.5626\n",
            "Epoch 66/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6830 - acc: 0.5545\n",
            "Epoch 67/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6818 - acc: 0.5376\n",
            "Epoch 68/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6814 - acc: 0.5552\n",
            "Epoch 69/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6833 - acc: 0.5633\n",
            "Epoch 70/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6780 - acc: 0.5427\n",
            "Epoch 71/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6820 - acc: 0.5493\n",
            "Epoch 72/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6807 - acc: 0.5641\n",
            "Epoch 73/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6817 - acc: 0.5383\n",
            "Epoch 74/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6795 - acc: 0.5398\n",
            "Epoch 75/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6805 - acc: 0.5596\n",
            "Epoch 76/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6796 - acc: 0.5677\n",
            "Epoch 77/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6822 - acc: 0.5427\n",
            "Epoch 78/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6829 - acc: 0.5619\n",
            "Epoch 79/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6817 - acc: 0.5442\n",
            "Epoch 80/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6854 - acc: 0.5464\n",
            "Epoch 81/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6838 - acc: 0.5530\n",
            "Epoch 82/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6797 - acc: 0.5560\n",
            "Epoch 83/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6815 - acc: 0.5398\n",
            "Epoch 84/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6822 - acc: 0.5574\n",
            "Epoch 85/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6838 - acc: 0.5552\n",
            "Epoch 86/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6826 - acc: 0.5412\n",
            "Epoch 87/2000\n",
            "1358/1358 [==============================] - 0s 58us/step - loss: 0.6812 - acc: 0.5501\n",
            "Epoch 88/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.6796 - acc: 0.5626\n",
            "Epoch 89/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6817 - acc: 0.5493\n",
            "Epoch 90/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6848 - acc: 0.5567\n",
            "Epoch 91/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6822 - acc: 0.5471\n",
            "Epoch 92/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6839 - acc: 0.5434\n",
            "Epoch 93/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6839 - acc: 0.5508\n",
            "Epoch 94/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6789 - acc: 0.5530\n",
            "Epoch 95/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6836 - acc: 0.5390\n",
            "Epoch 96/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6737 - acc: 0.5626\n",
            "Epoch 97/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6783 - acc: 0.5604\n",
            "Epoch 98/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6815 - acc: 0.5449\n",
            "Epoch 99/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6830 - acc: 0.5486\n",
            "Epoch 100/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6822 - acc: 0.5582\n",
            "Epoch 101/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6781 - acc: 0.5626\n",
            "Epoch 102/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6836 - acc: 0.5412\n",
            "Epoch 103/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6816 - acc: 0.5530\n",
            "Epoch 104/2000\n",
            "1358/1358 [==============================] - 0s 58us/step - loss: 0.6805 - acc: 0.5493\n",
            "Epoch 105/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6841 - acc: 0.5412\n",
            "Epoch 106/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6769 - acc: 0.5641\n",
            "Epoch 107/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6803 - acc: 0.5538\n",
            "Epoch 108/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6820 - acc: 0.5538\n",
            "Epoch 109/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6787 - acc: 0.5670\n",
            "Epoch 110/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6794 - acc: 0.5420\n",
            "Epoch 111/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6794 - acc: 0.5552\n",
            "Epoch 112/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6802 - acc: 0.5633\n",
            "Epoch 113/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6797 - acc: 0.5567\n",
            "Epoch 114/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6780 - acc: 0.5663\n",
            "Epoch 115/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6783 - acc: 0.5412\n",
            "Epoch 116/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6819 - acc: 0.5515\n",
            "Epoch 117/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6786 - acc: 0.5670\n",
            "Epoch 118/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6797 - acc: 0.5589\n",
            "Epoch 119/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6800 - acc: 0.5331\n",
            "Epoch 120/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6879 - acc: 0.5265\n",
            "Epoch 121/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6818 - acc: 0.5596\n",
            "Epoch 122/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6822 - acc: 0.5670\n",
            "Epoch 123/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6825 - acc: 0.5346\n",
            "Epoch 124/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6831 - acc: 0.5501\n",
            "Epoch 125/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6833 - acc: 0.5420\n",
            "Epoch 126/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6829 - acc: 0.5508\n",
            "Epoch 127/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6813 - acc: 0.5457\n",
            "Epoch 128/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6840 - acc: 0.5530\n",
            "Epoch 129/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6807 - acc: 0.5655\n",
            "Epoch 130/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6799 - acc: 0.5508\n",
            "Epoch 131/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6842 - acc: 0.5339\n",
            "Epoch 132/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6813 - acc: 0.5589\n",
            "Epoch 133/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6814 - acc: 0.5471\n",
            "Epoch 134/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.6835 - acc: 0.5567\n",
            "Epoch 135/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6810 - acc: 0.5376\n",
            "Epoch 136/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6816 - acc: 0.5486\n",
            "Epoch 137/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6797 - acc: 0.5596\n",
            "Epoch 138/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6805 - acc: 0.5574\n",
            "Epoch 139/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6771 - acc: 0.5589\n",
            "Epoch 140/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6783 - acc: 0.5331\n",
            "Epoch 141/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6787 - acc: 0.5582\n",
            "Epoch 142/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6802 - acc: 0.5567\n",
            "Epoch 143/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6812 - acc: 0.5560\n",
            "Epoch 144/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6781 - acc: 0.5714\n",
            "Epoch 145/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6808 - acc: 0.5434\n",
            "Epoch 146/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6778 - acc: 0.5567\n",
            "Epoch 147/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6788 - acc: 0.5457\n",
            "Epoch 148/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6814 - acc: 0.5515\n",
            "Epoch 149/2000\n",
            "1358/1358 [==============================] - 0s 59us/step - loss: 0.6794 - acc: 0.5457\n",
            "Epoch 150/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6793 - acc: 0.5530\n",
            "Epoch 151/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6806 - acc: 0.5604\n",
            "Epoch 152/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6795 - acc: 0.5442\n",
            "Epoch 153/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6816 - acc: 0.5515\n",
            "Epoch 154/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6766 - acc: 0.5567\n",
            "Epoch 155/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6763 - acc: 0.5560\n",
            "Epoch 156/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6806 - acc: 0.5530\n",
            "Epoch 157/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6747 - acc: 0.5523\n",
            "Epoch 158/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6759 - acc: 0.5582\n",
            "Epoch 159/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6778 - acc: 0.5648\n",
            "Epoch 160/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6756 - acc: 0.5574\n",
            "Epoch 161/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6793 - acc: 0.5663\n",
            "Epoch 162/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6768 - acc: 0.5582\n",
            "Epoch 163/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6808 - acc: 0.5538\n",
            "Epoch 164/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6795 - acc: 0.5611\n",
            "Epoch 165/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6827 - acc: 0.5317\n",
            "Epoch 166/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6788 - acc: 0.5545\n",
            "Epoch 167/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6776 - acc: 0.5596\n",
            "Epoch 168/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6819 - acc: 0.5464\n",
            "Epoch 169/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6817 - acc: 0.5493\n",
            "Epoch 170/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6804 - acc: 0.5545\n",
            "Epoch 171/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6798 - acc: 0.5677\n",
            "Epoch 172/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6792 - acc: 0.5641\n",
            "Epoch 173/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6761 - acc: 0.5700\n",
            "Epoch 174/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6796 - acc: 0.5722\n",
            "Epoch 175/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6752 - acc: 0.5648\n",
            "Epoch 176/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6793 - acc: 0.5619\n",
            "Epoch 177/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6806 - acc: 0.5538\n",
            "Epoch 178/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6791 - acc: 0.5324\n",
            "Epoch 179/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6765 - acc: 0.5700\n",
            "Epoch 180/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6814 - acc: 0.5361\n",
            "Epoch 181/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6791 - acc: 0.5501\n",
            "Epoch 182/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6803 - acc: 0.5449\n",
            "Epoch 183/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6778 - acc: 0.5714\n",
            "Epoch 184/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6790 - acc: 0.5633\n",
            "Epoch 185/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6785 - acc: 0.5361\n",
            "Epoch 186/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6766 - acc: 0.5663\n",
            "Epoch 187/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6798 - acc: 0.5449\n",
            "Epoch 188/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6772 - acc: 0.5538\n",
            "Epoch 189/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6760 - acc: 0.5530\n",
            "Epoch 190/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.6787 - acc: 0.5626\n",
            "Epoch 191/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6785 - acc: 0.5692\n",
            "Epoch 192/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6805 - acc: 0.5567\n",
            "Epoch 193/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.6842 - acc: 0.5427\n",
            "Epoch 194/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6804 - acc: 0.5596\n",
            "Epoch 195/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6787 - acc: 0.5560\n",
            "Epoch 196/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6822 - acc: 0.5523\n",
            "Epoch 197/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6789 - acc: 0.5596\n",
            "Epoch 198/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6815 - acc: 0.5486\n",
            "Epoch 199/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6801 - acc: 0.5427\n",
            "Epoch 200/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6810 - acc: 0.5596\n",
            "Epoch 201/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6800 - acc: 0.5560\n",
            "Epoch 202/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6768 - acc: 0.5685\n",
            "Epoch 203/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6767 - acc: 0.5486\n",
            "Epoch 204/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6792 - acc: 0.5552\n",
            "Epoch 205/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6808 - acc: 0.5552\n",
            "Epoch 206/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6780 - acc: 0.5501\n",
            "Epoch 207/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.6812 - acc: 0.5471\n",
            "Epoch 208/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.6803 - acc: 0.5552\n",
            "Epoch 209/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6745 - acc: 0.5633\n",
            "Epoch 210/2000\n",
            "1358/1358 [==============================] - 0s 61us/step - loss: 0.6750 - acc: 0.5663\n",
            "Epoch 211/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6785 - acc: 0.5707\n",
            "Epoch 212/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6799 - acc: 0.5567\n",
            "Epoch 213/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6797 - acc: 0.5560\n",
            "Epoch 214/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6761 - acc: 0.5633\n",
            "Epoch 215/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6745 - acc: 0.5692\n",
            "Epoch 216/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6828 - acc: 0.5552\n",
            "Epoch 217/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6761 - acc: 0.5825\n",
            "Epoch 218/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6782 - acc: 0.5619\n",
            "Epoch 219/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6777 - acc: 0.5560\n",
            "Epoch 220/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6800 - acc: 0.5626\n",
            "Epoch 221/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6721 - acc: 0.5825\n",
            "Epoch 222/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6781 - acc: 0.5663\n",
            "Epoch 223/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6748 - acc: 0.5722\n",
            "Epoch 224/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6768 - acc: 0.5736\n",
            "Epoch 225/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.6751 - acc: 0.5685\n",
            "Epoch 226/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6781 - acc: 0.5545\n",
            "Epoch 227/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6822 - acc: 0.5552\n",
            "Epoch 228/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6750 - acc: 0.5722\n",
            "Epoch 229/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6760 - acc: 0.5641\n",
            "Epoch 230/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6798 - acc: 0.5611\n",
            "Epoch 231/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6756 - acc: 0.5729\n",
            "Epoch 232/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6750 - acc: 0.5795\n",
            "Epoch 233/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6764 - acc: 0.5515\n",
            "Epoch 234/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6749 - acc: 0.5663\n",
            "Epoch 235/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6773 - acc: 0.5471\n",
            "Epoch 236/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6733 - acc: 0.5685\n",
            "Epoch 237/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6736 - acc: 0.5692\n",
            "Epoch 238/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6754 - acc: 0.5523\n",
            "Epoch 239/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6770 - acc: 0.5471\n",
            "Epoch 240/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6755 - acc: 0.5700\n",
            "Epoch 241/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6774 - acc: 0.5486\n",
            "Epoch 242/2000\n",
            "1358/1358 [==============================] - 0s 91us/step - loss: 0.6760 - acc: 0.5766\n",
            "Epoch 243/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6789 - acc: 0.5479\n",
            "Epoch 244/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6739 - acc: 0.5825\n",
            "Epoch 245/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6786 - acc: 0.5707\n",
            "Epoch 246/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.6778 - acc: 0.5722\n",
            "Epoch 247/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.6791 - acc: 0.5589\n",
            "Epoch 248/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6782 - acc: 0.5567\n",
            "Epoch 249/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6749 - acc: 0.5670\n",
            "Epoch 250/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6780 - acc: 0.5803\n",
            "Epoch 251/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.6763 - acc: 0.5641\n",
            "Epoch 252/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6781 - acc: 0.5648\n",
            "Epoch 253/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.6785 - acc: 0.5552\n",
            "Epoch 254/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6766 - acc: 0.5611\n",
            "Epoch 255/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6768 - acc: 0.5714\n",
            "Epoch 256/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6713 - acc: 0.5781\n",
            "Epoch 257/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6731 - acc: 0.5906\n",
            "Epoch 258/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6718 - acc: 0.5876\n",
            "Epoch 259/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6770 - acc: 0.5604\n",
            "Epoch 260/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6748 - acc: 0.5751\n",
            "Epoch 261/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6724 - acc: 0.5670\n",
            "Epoch 262/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6770 - acc: 0.5515\n",
            "Epoch 263/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6732 - acc: 0.5810\n",
            "Epoch 264/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.6754 - acc: 0.5832\n",
            "Epoch 265/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6741 - acc: 0.5817\n",
            "Epoch 266/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.6703 - acc: 0.5825\n",
            "Epoch 267/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6719 - acc: 0.5795\n",
            "Epoch 268/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6700 - acc: 0.5869\n",
            "Epoch 269/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6674 - acc: 0.5817\n",
            "Epoch 270/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6721 - acc: 0.5677\n",
            "Epoch 271/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6725 - acc: 0.5751\n",
            "Epoch 272/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6730 - acc: 0.5670\n",
            "Epoch 273/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6700 - acc: 0.5589\n",
            "Epoch 274/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6754 - acc: 0.5722\n",
            "Epoch 275/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6683 - acc: 0.5795\n",
            "Epoch 276/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6766 - acc: 0.5677\n",
            "Epoch 277/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6749 - acc: 0.5641\n",
            "Epoch 278/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.6724 - acc: 0.5781\n",
            "Epoch 279/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6725 - acc: 0.5714\n",
            "Epoch 280/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.6696 - acc: 0.5839\n",
            "Epoch 281/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6694 - acc: 0.5891\n",
            "Epoch 282/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6755 - acc: 0.5714\n",
            "Epoch 283/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6745 - acc: 0.5766\n",
            "Epoch 284/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6689 - acc: 0.5987\n",
            "Epoch 285/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6768 - acc: 0.5736\n",
            "Epoch 286/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.6665 - acc: 0.6001\n",
            "Epoch 287/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6628 - acc: 0.5957\n",
            "Epoch 288/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6667 - acc: 0.5898\n",
            "Epoch 289/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6712 - acc: 0.5920\n",
            "Epoch 290/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6754 - acc: 0.5670\n",
            "Epoch 291/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6652 - acc: 0.5972\n",
            "Epoch 292/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6755 - acc: 0.5648\n",
            "Epoch 293/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6681 - acc: 0.5847\n",
            "Epoch 294/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6653 - acc: 0.5869\n",
            "Epoch 295/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6726 - acc: 0.5913\n",
            "Epoch 296/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6597 - acc: 0.5950\n",
            "Epoch 297/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.6642 - acc: 0.5817\n",
            "Epoch 298/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6683 - acc: 0.5869\n",
            "Epoch 299/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.6646 - acc: 0.6105\n",
            "Epoch 300/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6722 - acc: 0.6024\n",
            "Epoch 301/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6709 - acc: 0.5847\n",
            "Epoch 302/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6643 - acc: 0.5906\n",
            "Epoch 303/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6683 - acc: 0.5884\n",
            "Epoch 304/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6665 - acc: 0.5736\n",
            "Epoch 305/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6675 - acc: 0.6053\n",
            "Epoch 306/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6651 - acc: 0.6016\n",
            "Epoch 307/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6614 - acc: 0.5943\n",
            "Epoch 308/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.6623 - acc: 0.5935\n",
            "Epoch 309/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.6654 - acc: 0.6046\n",
            "Epoch 310/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.6635 - acc: 0.6038\n",
            "Epoch 311/2000\n",
            "1358/1358 [==============================] - 0s 105us/step - loss: 0.6618 - acc: 0.6068\n",
            "Epoch 312/2000\n",
            "1358/1358 [==============================] - 0s 105us/step - loss: 0.6641 - acc: 0.5839\n",
            "Epoch 313/2000\n",
            "1358/1358 [==============================] - 0s 91us/step - loss: 0.6598 - acc: 0.6009\n",
            "Epoch 314/2000\n",
            "1358/1358 [==============================] - 0s 93us/step - loss: 0.6583 - acc: 0.6016\n",
            "Epoch 315/2000\n",
            "1358/1358 [==============================] - 0s 108us/step - loss: 0.6573 - acc: 0.6016\n",
            "Epoch 316/2000\n",
            "1358/1358 [==============================] - 0s 114us/step - loss: 0.6588 - acc: 0.5920\n",
            "Epoch 317/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.6573 - acc: 0.6134\n",
            "Epoch 318/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.6667 - acc: 0.5913\n",
            "Epoch 319/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.6582 - acc: 0.5994\n",
            "Epoch 320/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.6545 - acc: 0.5979\n",
            "Epoch 321/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.6515 - acc: 0.6237\n",
            "Epoch 322/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.6493 - acc: 0.6141\n",
            "Epoch 323/2000\n",
            "1358/1358 [==============================] - 0s 94us/step - loss: 0.6556 - acc: 0.6222\n",
            "Epoch 324/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.6456 - acc: 0.6333\n",
            "Epoch 325/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6495 - acc: 0.6038\n",
            "Epoch 326/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6425 - acc: 0.6318\n",
            "Epoch 327/2000\n",
            "1358/1358 [==============================] - 0s 60us/step - loss: 0.6500 - acc: 0.6112\n",
            "Epoch 328/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6439 - acc: 0.6222\n",
            "Epoch 329/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.6410 - acc: 0.6119\n",
            "Epoch 330/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.6500 - acc: 0.6105\n",
            "Epoch 331/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6441 - acc: 0.6303\n",
            "Epoch 332/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6496 - acc: 0.6016\n",
            "Epoch 333/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.6329 - acc: 0.6274\n",
            "Epoch 334/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6455 - acc: 0.6112\n",
            "Epoch 335/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.6446 - acc: 0.6075\n",
            "Epoch 336/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6376 - acc: 0.6392\n",
            "Epoch 337/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6242 - acc: 0.6340\n",
            "Epoch 338/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6371 - acc: 0.6333\n",
            "Epoch 339/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6398 - acc: 0.6193\n",
            "Epoch 340/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6319 - acc: 0.6370\n",
            "Epoch 341/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6266 - acc: 0.6362\n",
            "Epoch 342/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6259 - acc: 0.6414\n",
            "Epoch 343/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6363 - acc: 0.6105\n",
            "Epoch 344/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6334 - acc: 0.6230\n",
            "Epoch 345/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6249 - acc: 0.6333\n",
            "Epoch 346/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.6297 - acc: 0.6451\n",
            "Epoch 347/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.6278 - acc: 0.6318\n",
            "Epoch 348/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6216 - acc: 0.6252\n",
            "Epoch 349/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6208 - acc: 0.6303\n",
            "Epoch 350/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6298 - acc: 0.6267\n",
            "Epoch 351/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6301 - acc: 0.6244\n",
            "Epoch 352/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.6204 - acc: 0.6362\n",
            "Epoch 353/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6113 - acc: 0.6524\n",
            "Epoch 354/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6270 - acc: 0.6193\n",
            "Epoch 355/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.6262 - acc: 0.6244\n",
            "Epoch 356/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6191 - acc: 0.6532\n",
            "Epoch 357/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6120 - acc: 0.6465\n",
            "Epoch 358/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6246 - acc: 0.6141\n",
            "Epoch 359/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6080 - acc: 0.6510\n",
            "Epoch 360/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.6312 - acc: 0.6186\n",
            "Epoch 361/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6150 - acc: 0.6465\n",
            "Epoch 362/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6086 - acc: 0.6421\n",
            "Epoch 363/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6204 - acc: 0.6274\n",
            "Epoch 364/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.6189 - acc: 0.6362\n",
            "Epoch 365/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.6120 - acc: 0.6436\n",
            "Epoch 366/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6142 - acc: 0.6451\n",
            "Epoch 367/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.6176 - acc: 0.6480\n",
            "Epoch 368/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.6093 - acc: 0.6517\n",
            "Epoch 369/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.6176 - acc: 0.6436\n",
            "Epoch 370/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5947 - acc: 0.6510\n",
            "Epoch 371/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6098 - acc: 0.6532\n",
            "Epoch 372/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.6067 - acc: 0.6362\n",
            "Epoch 373/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.6142 - acc: 0.6259\n",
            "Epoch 374/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6106 - acc: 0.6627\n",
            "Epoch 375/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.6012 - acc: 0.6495\n",
            "Epoch 376/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.6195 - acc: 0.6325\n",
            "Epoch 377/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.6075 - acc: 0.6399\n",
            "Epoch 378/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.6104 - acc: 0.6340\n",
            "Epoch 379/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.6085 - acc: 0.6480\n",
            "Epoch 380/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5951 - acc: 0.6627\n",
            "Epoch 381/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.6054 - acc: 0.6487\n",
            "Epoch 382/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.6063 - acc: 0.6465\n",
            "Epoch 383/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.6075 - acc: 0.6399\n",
            "Epoch 384/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.6009 - acc: 0.6568\n",
            "Epoch 385/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.6032 - acc: 0.6524\n",
            "Epoch 386/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.6124 - acc: 0.6443\n",
            "Epoch 387/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.5931 - acc: 0.6539\n",
            "Epoch 388/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5980 - acc: 0.6620\n",
            "Epoch 389/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5952 - acc: 0.6510\n",
            "Epoch 390/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5954 - acc: 0.6613\n",
            "Epoch 391/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5986 - acc: 0.6664\n",
            "Epoch 392/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5910 - acc: 0.6568\n",
            "Epoch 393/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5863 - acc: 0.6672\n",
            "Epoch 394/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5815 - acc: 0.6627\n",
            "Epoch 395/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5904 - acc: 0.6679\n",
            "Epoch 396/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5792 - acc: 0.6716\n",
            "Epoch 397/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.6007 - acc: 0.6532\n",
            "Epoch 398/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5888 - acc: 0.6672\n",
            "Epoch 399/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5979 - acc: 0.6495\n",
            "Epoch 400/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5920 - acc: 0.6627\n",
            "Epoch 401/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5808 - acc: 0.6620\n",
            "Epoch 402/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5839 - acc: 0.6716\n",
            "Epoch 403/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5913 - acc: 0.6591\n",
            "Epoch 404/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5908 - acc: 0.6723\n",
            "Epoch 405/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5843 - acc: 0.6613\n",
            "Epoch 406/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5822 - acc: 0.6583\n",
            "Epoch 407/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5869 - acc: 0.6657\n",
            "Epoch 408/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5793 - acc: 0.6804\n",
            "Epoch 409/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5848 - acc: 0.6532\n",
            "Epoch 410/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5766 - acc: 0.6672\n",
            "Epoch 411/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5881 - acc: 0.6546\n",
            "Epoch 412/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5777 - acc: 0.6672\n",
            "Epoch 413/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.5862 - acc: 0.6804\n",
            "Epoch 414/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5969 - acc: 0.6613\n",
            "Epoch 415/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5838 - acc: 0.6664\n",
            "Epoch 416/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.5716 - acc: 0.6753\n",
            "Epoch 417/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5876 - acc: 0.6657\n",
            "Epoch 418/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.5887 - acc: 0.6502\n",
            "Epoch 419/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5823 - acc: 0.6789\n",
            "Epoch 420/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5640 - acc: 0.6819\n",
            "Epoch 421/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5821 - acc: 0.6561\n",
            "Epoch 422/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5737 - acc: 0.6679\n",
            "Epoch 423/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5693 - acc: 0.6789\n",
            "Epoch 424/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5717 - acc: 0.6951\n",
            "Epoch 425/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5706 - acc: 0.6664\n",
            "Epoch 426/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5758 - acc: 0.6804\n",
            "Epoch 427/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5658 - acc: 0.6730\n",
            "Epoch 428/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5801 - acc: 0.6775\n",
            "Epoch 429/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5826 - acc: 0.6738\n",
            "Epoch 430/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5729 - acc: 0.6819\n",
            "Epoch 431/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5593 - acc: 0.6848\n",
            "Epoch 432/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5745 - acc: 0.6657\n",
            "Epoch 433/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.5719 - acc: 0.6937\n",
            "Epoch 434/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5689 - acc: 0.6797\n",
            "Epoch 435/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5816 - acc: 0.6679\n",
            "Epoch 436/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5709 - acc: 0.6716\n",
            "Epoch 437/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5772 - acc: 0.6723\n",
            "Epoch 438/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5646 - acc: 0.6745\n",
            "Epoch 439/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5599 - acc: 0.6878\n",
            "Epoch 440/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5626 - acc: 0.6819\n",
            "Epoch 441/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5595 - acc: 0.6981\n",
            "Epoch 442/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5664 - acc: 0.6870\n",
            "Epoch 443/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5739 - acc: 0.6679\n",
            "Epoch 444/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5788 - acc: 0.6627\n",
            "Epoch 445/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.5543 - acc: 0.7003\n",
            "Epoch 446/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5544 - acc: 0.6981\n",
            "Epoch 447/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.5741 - acc: 0.6782\n",
            "Epoch 448/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5572 - acc: 0.6900\n",
            "Epoch 449/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5560 - acc: 0.6775\n",
            "Epoch 450/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.5740 - acc: 0.6797\n",
            "Epoch 451/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5505 - acc: 0.7003\n",
            "Epoch 452/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5692 - acc: 0.6745\n",
            "Epoch 453/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5619 - acc: 0.6944\n",
            "Epoch 454/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5389 - acc: 0.7128\n",
            "Epoch 455/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5517 - acc: 0.6996\n",
            "Epoch 456/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5574 - acc: 0.6760\n",
            "Epoch 457/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5636 - acc: 0.6900\n",
            "Epoch 458/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5616 - acc: 0.6937\n",
            "Epoch 459/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5743 - acc: 0.6701\n",
            "Epoch 460/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5636 - acc: 0.6878\n",
            "Epoch 461/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5491 - acc: 0.6907\n",
            "Epoch 462/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.5631 - acc: 0.6892\n",
            "Epoch 463/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5532 - acc: 0.6973\n",
            "Epoch 464/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.5526 - acc: 0.6944\n",
            "Epoch 465/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5621 - acc: 0.6834\n",
            "Epoch 466/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5529 - acc: 0.6959\n",
            "Epoch 467/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5507 - acc: 0.6944\n",
            "Epoch 468/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5578 - acc: 0.6915\n",
            "Epoch 469/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5569 - acc: 0.7062\n",
            "Epoch 470/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5510 - acc: 0.6922\n",
            "Epoch 471/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5386 - acc: 0.6966\n",
            "Epoch 472/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5466 - acc: 0.7032\n",
            "Epoch 473/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5630 - acc: 0.6944\n",
            "Epoch 474/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5567 - acc: 0.6900\n",
            "Epoch 475/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5515 - acc: 0.6915\n",
            "Epoch 476/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5464 - acc: 0.6959\n",
            "Epoch 477/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5510 - acc: 0.7003\n",
            "Epoch 478/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5586 - acc: 0.6981\n",
            "Epoch 479/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5489 - acc: 0.7047\n",
            "Epoch 480/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5530 - acc: 0.6922\n",
            "Epoch 481/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5426 - acc: 0.6951\n",
            "Epoch 482/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5559 - acc: 0.6996\n",
            "Epoch 483/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5575 - acc: 0.6929\n",
            "Epoch 484/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5501 - acc: 0.6870\n",
            "Epoch 485/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5525 - acc: 0.7106\n",
            "Epoch 486/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5521 - acc: 0.6937\n",
            "Epoch 487/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5342 - acc: 0.7018\n",
            "Epoch 488/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5437 - acc: 0.7025\n",
            "Epoch 489/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5640 - acc: 0.6856\n",
            "Epoch 490/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5444 - acc: 0.7047\n",
            "Epoch 491/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5578 - acc: 0.6929\n",
            "Epoch 492/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5624 - acc: 0.6841\n",
            "Epoch 493/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5498 - acc: 0.6959\n",
            "Epoch 494/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5498 - acc: 0.6981\n",
            "Epoch 495/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5562 - acc: 0.6937\n",
            "Epoch 496/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5369 - acc: 0.7025\n",
            "Epoch 497/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5381 - acc: 0.7032\n",
            "Epoch 498/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5511 - acc: 0.6996\n",
            "Epoch 499/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5570 - acc: 0.6988\n",
            "Epoch 500/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5327 - acc: 0.6988\n",
            "Epoch 501/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5550 - acc: 0.6937\n",
            "Epoch 502/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5356 - acc: 0.6996\n",
            "Epoch 503/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5588 - acc: 0.6929\n",
            "Epoch 504/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5610 - acc: 0.6922\n",
            "Epoch 505/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5509 - acc: 0.6907\n",
            "Epoch 506/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5349 - acc: 0.7165\n",
            "Epoch 507/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5437 - acc: 0.7054\n",
            "Epoch 508/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5481 - acc: 0.7010\n",
            "Epoch 509/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5331 - acc: 0.7194\n",
            "Epoch 510/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5491 - acc: 0.6929\n",
            "Epoch 511/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5503 - acc: 0.6870\n",
            "Epoch 512/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5510 - acc: 0.6922\n",
            "Epoch 513/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5512 - acc: 0.7172\n",
            "Epoch 514/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5382 - acc: 0.7040\n",
            "Epoch 515/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5349 - acc: 0.7128\n",
            "Epoch 516/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5429 - acc: 0.7246\n",
            "Epoch 517/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5536 - acc: 0.6951\n",
            "Epoch 518/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5376 - acc: 0.7180\n",
            "Epoch 519/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5428 - acc: 0.7158\n",
            "Epoch 520/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5329 - acc: 0.7165\n",
            "Epoch 521/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5282 - acc: 0.7268\n",
            "Epoch 522/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.5427 - acc: 0.7062\n",
            "Epoch 523/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.5401 - acc: 0.7018\n",
            "Epoch 524/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5395 - acc: 0.7069\n",
            "Epoch 525/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5492 - acc: 0.7018\n",
            "Epoch 526/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5466 - acc: 0.7018\n",
            "Epoch 527/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5478 - acc: 0.6841\n",
            "Epoch 528/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5474 - acc: 0.6996\n",
            "Epoch 529/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5356 - acc: 0.7069\n",
            "Epoch 530/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5403 - acc: 0.7010\n",
            "Epoch 531/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5481 - acc: 0.6996\n",
            "Epoch 532/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5438 - acc: 0.7128\n",
            "Epoch 533/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5356 - acc: 0.7084\n",
            "Epoch 534/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5545 - acc: 0.6944\n",
            "Epoch 535/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5330 - acc: 0.7187\n",
            "Epoch 536/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5255 - acc: 0.7268\n",
            "Epoch 537/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5405 - acc: 0.7077\n",
            "Epoch 538/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5366 - acc: 0.7062\n",
            "Epoch 539/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5448 - acc: 0.6951\n",
            "Epoch 540/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5425 - acc: 0.6988\n",
            "Epoch 541/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5479 - acc: 0.7135\n",
            "Epoch 542/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5417 - acc: 0.7040\n",
            "Epoch 543/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5373 - acc: 0.7113\n",
            "Epoch 544/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5205 - acc: 0.7158\n",
            "Epoch 545/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5426 - acc: 0.6988\n",
            "Epoch 546/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5377 - acc: 0.7135\n",
            "Epoch 547/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5581 - acc: 0.7069\n",
            "Epoch 548/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5511 - acc: 0.6915\n",
            "Epoch 549/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5563 - acc: 0.6900\n",
            "Epoch 550/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5520 - acc: 0.6981\n",
            "Epoch 551/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5280 - acc: 0.7091\n",
            "Epoch 552/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5543 - acc: 0.7003\n",
            "Epoch 553/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5380 - acc: 0.6981\n",
            "Epoch 554/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5531 - acc: 0.6951\n",
            "Epoch 555/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5322 - acc: 0.7165\n",
            "Epoch 556/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5314 - acc: 0.7143\n",
            "Epoch 557/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5399 - acc: 0.7018\n",
            "Epoch 558/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5328 - acc: 0.7025\n",
            "Epoch 559/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5476 - acc: 0.6937\n",
            "Epoch 560/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5531 - acc: 0.6863\n",
            "Epoch 561/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5395 - acc: 0.7018\n",
            "Epoch 562/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5466 - acc: 0.6973\n",
            "Epoch 563/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.5371 - acc: 0.7202\n",
            "Epoch 564/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5459 - acc: 0.7077\n",
            "Epoch 565/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5360 - acc: 0.7165\n",
            "Epoch 566/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5641 - acc: 0.7069\n",
            "Epoch 567/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5422 - acc: 0.7150\n",
            "Epoch 568/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5424 - acc: 0.7158\n",
            "Epoch 569/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5298 - acc: 0.7202\n",
            "Epoch 570/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5274 - acc: 0.7062\n",
            "Epoch 571/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5363 - acc: 0.6996\n",
            "Epoch 572/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5476 - acc: 0.7040\n",
            "Epoch 573/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5272 - acc: 0.7216\n",
            "Epoch 574/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5319 - acc: 0.7099\n",
            "Epoch 575/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5366 - acc: 0.7202\n",
            "Epoch 576/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5371 - acc: 0.7128\n",
            "Epoch 577/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5415 - acc: 0.7069\n",
            "Epoch 578/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5547 - acc: 0.7113\n",
            "Epoch 579/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5305 - acc: 0.7025\n",
            "Epoch 580/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5422 - acc: 0.7069\n",
            "Epoch 581/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5242 - acc: 0.7231\n",
            "Epoch 582/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5384 - acc: 0.7143\n",
            "Epoch 583/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5351 - acc: 0.7209\n",
            "Epoch 584/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5482 - acc: 0.7106\n",
            "Epoch 585/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5350 - acc: 0.7135\n",
            "Epoch 586/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5405 - acc: 0.7010\n",
            "Epoch 587/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5323 - acc: 0.7128\n",
            "Epoch 588/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5510 - acc: 0.7113\n",
            "Epoch 589/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5494 - acc: 0.7054\n",
            "Epoch 590/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5352 - acc: 0.7121\n",
            "Epoch 591/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5512 - acc: 0.6973\n",
            "Epoch 592/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5578 - acc: 0.6973\n",
            "Epoch 593/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5234 - acc: 0.7121\n",
            "Epoch 594/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5445 - acc: 0.6981\n",
            "Epoch 595/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5404 - acc: 0.7047\n",
            "Epoch 596/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5341 - acc: 0.7158\n",
            "Epoch 597/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5207 - acc: 0.7165\n",
            "Epoch 598/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5406 - acc: 0.7143\n",
            "Epoch 599/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5331 - acc: 0.7202\n",
            "Epoch 600/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5175 - acc: 0.7297\n",
            "Epoch 601/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5301 - acc: 0.7025\n",
            "Epoch 602/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5268 - acc: 0.7158\n",
            "Epoch 603/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5575 - acc: 0.7121\n",
            "Epoch 604/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5307 - acc: 0.7165\n",
            "Epoch 605/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5347 - acc: 0.7180\n",
            "Epoch 606/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5402 - acc: 0.7069\n",
            "Epoch 607/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5282 - acc: 0.7224\n",
            "Epoch 608/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5362 - acc: 0.7099\n",
            "Epoch 609/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5316 - acc: 0.7084\n",
            "Epoch 610/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5293 - acc: 0.7165\n",
            "Epoch 611/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5346 - acc: 0.7077\n",
            "Epoch 612/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5435 - acc: 0.7268\n",
            "Epoch 613/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5371 - acc: 0.7091\n",
            "Epoch 614/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5279 - acc: 0.7099\n",
            "Epoch 615/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5400 - acc: 0.7099\n",
            "Epoch 616/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5376 - acc: 0.7069\n",
            "Epoch 617/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5304 - acc: 0.7106\n",
            "Epoch 618/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5356 - acc: 0.7128\n",
            "Epoch 619/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5334 - acc: 0.7106\n",
            "Epoch 620/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5420 - acc: 0.7121\n",
            "Epoch 621/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5458 - acc: 0.7172\n",
            "Epoch 622/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5432 - acc: 0.7040\n",
            "Epoch 623/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5411 - acc: 0.7062\n",
            "Epoch 624/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5246 - acc: 0.7187\n",
            "Epoch 625/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5373 - acc: 0.7084\n",
            "Epoch 626/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.5283 - acc: 0.7172\n",
            "Epoch 627/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5237 - acc: 0.7135\n",
            "Epoch 628/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5432 - acc: 0.7047\n",
            "Epoch 629/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5277 - acc: 0.7172\n",
            "Epoch 630/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5360 - acc: 0.7320\n",
            "Epoch 631/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.5331 - acc: 0.7084\n",
            "Epoch 632/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5241 - acc: 0.7158\n",
            "Epoch 633/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5219 - acc: 0.7202\n",
            "Epoch 634/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5309 - acc: 0.7172\n",
            "Epoch 635/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5310 - acc: 0.7180\n",
            "Epoch 636/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5372 - acc: 0.7128\n",
            "Epoch 637/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5341 - acc: 0.7143\n",
            "Epoch 638/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5293 - acc: 0.7054\n",
            "Epoch 639/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5360 - acc: 0.7106\n",
            "Epoch 640/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5259 - acc: 0.7099\n",
            "Epoch 641/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5355 - acc: 0.7106\n",
            "Epoch 642/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5308 - acc: 0.7099\n",
            "Epoch 643/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5363 - acc: 0.7069\n",
            "Epoch 644/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5227 - acc: 0.7187\n",
            "Epoch 645/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5369 - acc: 0.7187\n",
            "Epoch 646/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5375 - acc: 0.7032\n",
            "Epoch 647/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.5278 - acc: 0.7128\n",
            "Epoch 648/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5392 - acc: 0.7150\n",
            "Epoch 649/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5324 - acc: 0.7069\n",
            "Epoch 650/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5393 - acc: 0.7135\n",
            "Epoch 651/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5421 - acc: 0.7106\n",
            "Epoch 652/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5532 - acc: 0.7062\n",
            "Epoch 653/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5463 - acc: 0.7054\n",
            "Epoch 654/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5176 - acc: 0.7320\n",
            "Epoch 655/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5328 - acc: 0.7106\n",
            "Epoch 656/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5298 - acc: 0.7113\n",
            "Epoch 657/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5417 - acc: 0.7143\n",
            "Epoch 658/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5334 - acc: 0.7224\n",
            "Epoch 659/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5344 - acc: 0.7121\n",
            "Epoch 660/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5332 - acc: 0.7091\n",
            "Epoch 661/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5237 - acc: 0.7165\n",
            "Epoch 662/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5295 - acc: 0.7180\n",
            "Epoch 663/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5359 - acc: 0.7216\n",
            "Epoch 664/2000\n",
            "1358/1358 [==============================] - 0s 62us/step - loss: 0.5256 - acc: 0.7275\n",
            "Epoch 665/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5145 - acc: 0.7128\n",
            "Epoch 666/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5230 - acc: 0.7305\n",
            "Epoch 667/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5291 - acc: 0.7128\n",
            "Epoch 668/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5443 - acc: 0.7062\n",
            "Epoch 669/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5310 - acc: 0.7187\n",
            "Epoch 670/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5482 - acc: 0.7091\n",
            "Epoch 671/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5159 - acc: 0.7239\n",
            "Epoch 672/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5400 - acc: 0.7077\n",
            "Epoch 673/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5297 - acc: 0.7180\n",
            "Epoch 674/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5363 - acc: 0.7209\n",
            "Epoch 675/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5363 - acc: 0.7099\n",
            "Epoch 676/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5317 - acc: 0.7069\n",
            "Epoch 677/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5302 - acc: 0.7305\n",
            "Epoch 678/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5404 - acc: 0.7172\n",
            "Epoch 679/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5395 - acc: 0.6996\n",
            "Epoch 680/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.5229 - acc: 0.7253\n",
            "Epoch 681/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5367 - acc: 0.7261\n",
            "Epoch 682/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.5320 - acc: 0.6996\n",
            "Epoch 683/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5351 - acc: 0.7150\n",
            "Epoch 684/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5375 - acc: 0.7246\n",
            "Epoch 685/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5343 - acc: 0.7216\n",
            "Epoch 686/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5293 - acc: 0.7202\n",
            "Epoch 687/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5280 - acc: 0.7143\n",
            "Epoch 688/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5500 - acc: 0.7047\n",
            "Epoch 689/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5357 - acc: 0.7209\n",
            "Epoch 690/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5327 - acc: 0.7084\n",
            "Epoch 691/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5134 - acc: 0.7172\n",
            "Epoch 692/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5523 - acc: 0.7113\n",
            "Epoch 693/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5203 - acc: 0.7202\n",
            "Epoch 694/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5330 - acc: 0.7187\n",
            "Epoch 695/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5297 - acc: 0.7054\n",
            "Epoch 696/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5178 - acc: 0.7209\n",
            "Epoch 697/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5229 - acc: 0.7312\n",
            "Epoch 698/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5246 - acc: 0.7261\n",
            "Epoch 699/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5203 - acc: 0.7054\n",
            "Epoch 700/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5459 - acc: 0.7040\n",
            "Epoch 701/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5207 - acc: 0.7194\n",
            "Epoch 702/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5424 - acc: 0.7113\n",
            "Epoch 703/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5468 - acc: 0.7018\n",
            "Epoch 704/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5312 - acc: 0.7143\n",
            "Epoch 705/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5255 - acc: 0.7231\n",
            "Epoch 706/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5358 - acc: 0.7106\n",
            "Epoch 707/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5395 - acc: 0.7239\n",
            "Epoch 708/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5227 - acc: 0.7408\n",
            "Epoch 709/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5223 - acc: 0.7320\n",
            "Epoch 710/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5173 - acc: 0.7290\n",
            "Epoch 711/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5369 - acc: 0.7216\n",
            "Epoch 712/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5266 - acc: 0.7209\n",
            "Epoch 713/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5241 - acc: 0.7209\n",
            "Epoch 714/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5160 - acc: 0.7283\n",
            "Epoch 715/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5324 - acc: 0.7099\n",
            "Epoch 716/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5287 - acc: 0.7224\n",
            "Epoch 717/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5382 - acc: 0.7180\n",
            "Epoch 718/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5332 - acc: 0.7216\n",
            "Epoch 719/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5385 - acc: 0.7003\n",
            "Epoch 720/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5275 - acc: 0.7172\n",
            "Epoch 721/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5200 - acc: 0.7158\n",
            "Epoch 722/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5198 - acc: 0.7187\n",
            "Epoch 723/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5142 - acc: 0.7275\n",
            "Epoch 724/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5414 - acc: 0.7158\n",
            "Epoch 725/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5206 - acc: 0.7194\n",
            "Epoch 726/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5192 - acc: 0.7320\n",
            "Epoch 727/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5360 - acc: 0.7342\n",
            "Epoch 728/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5231 - acc: 0.7290\n",
            "Epoch 729/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5336 - acc: 0.7187\n",
            "Epoch 730/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5330 - acc: 0.7187\n",
            "Epoch 731/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5107 - acc: 0.7275\n",
            "Epoch 732/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5149 - acc: 0.7253\n",
            "Epoch 733/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5261 - acc: 0.7180\n",
            "Epoch 734/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5449 - acc: 0.7239\n",
            "Epoch 735/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5363 - acc: 0.7106\n",
            "Epoch 736/2000\n",
            "1358/1358 [==============================] - 0s 63us/step - loss: 0.5336 - acc: 0.7113\n",
            "Epoch 737/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5310 - acc: 0.7194\n",
            "Epoch 738/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5207 - acc: 0.7327\n",
            "Epoch 739/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5356 - acc: 0.7121\n",
            "Epoch 740/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5354 - acc: 0.7216\n",
            "Epoch 741/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5276 - acc: 0.7113\n",
            "Epoch 742/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5360 - acc: 0.7047\n",
            "Epoch 743/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5348 - acc: 0.7180\n",
            "Epoch 744/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5185 - acc: 0.7401\n",
            "Epoch 745/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5260 - acc: 0.7202\n",
            "Epoch 746/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.5283 - acc: 0.7135\n",
            "Epoch 747/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5470 - acc: 0.7216\n",
            "Epoch 748/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5344 - acc: 0.7099\n",
            "Epoch 749/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5391 - acc: 0.7297\n",
            "Epoch 750/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5217 - acc: 0.7047\n",
            "Epoch 751/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5375 - acc: 0.7187\n",
            "Epoch 752/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5218 - acc: 0.7202\n",
            "Epoch 753/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5115 - acc: 0.7349\n",
            "Epoch 754/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5394 - acc: 0.7158\n",
            "Epoch 755/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5324 - acc: 0.7297\n",
            "Epoch 756/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5153 - acc: 0.7334\n",
            "Epoch 757/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5203 - acc: 0.7172\n",
            "Epoch 758/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5216 - acc: 0.7297\n",
            "Epoch 759/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5269 - acc: 0.7290\n",
            "Epoch 760/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5401 - acc: 0.7113\n",
            "Epoch 761/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5374 - acc: 0.7158\n",
            "Epoch 762/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5263 - acc: 0.7216\n",
            "Epoch 763/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5317 - acc: 0.7040\n",
            "Epoch 764/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5394 - acc: 0.7158\n",
            "Epoch 765/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5221 - acc: 0.7283\n",
            "Epoch 766/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5201 - acc: 0.7158\n",
            "Epoch 767/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5093 - acc: 0.7437\n",
            "Epoch 768/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5249 - acc: 0.7209\n",
            "Epoch 769/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5234 - acc: 0.7312\n",
            "Epoch 770/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5277 - acc: 0.7216\n",
            "Epoch 771/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5196 - acc: 0.7275\n",
            "Epoch 772/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5318 - acc: 0.7209\n",
            "Epoch 773/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5263 - acc: 0.7246\n",
            "Epoch 774/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5163 - acc: 0.7216\n",
            "Epoch 775/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5178 - acc: 0.7172\n",
            "Epoch 776/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5130 - acc: 0.7187\n",
            "Epoch 777/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5250 - acc: 0.7246\n",
            "Epoch 778/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5249 - acc: 0.7297\n",
            "Epoch 779/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5264 - acc: 0.7165\n",
            "Epoch 780/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5163 - acc: 0.7401\n",
            "Epoch 781/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5317 - acc: 0.7187\n",
            "Epoch 782/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5274 - acc: 0.7187\n",
            "Epoch 783/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5266 - acc: 0.7231\n",
            "Epoch 784/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5251 - acc: 0.7224\n",
            "Epoch 785/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5172 - acc: 0.7342\n",
            "Epoch 786/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5202 - acc: 0.7261\n",
            "Epoch 787/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5190 - acc: 0.7187\n",
            "Epoch 788/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5203 - acc: 0.7275\n",
            "Epoch 789/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5371 - acc: 0.7216\n",
            "Epoch 790/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5222 - acc: 0.7180\n",
            "Epoch 791/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5191 - acc: 0.7496\n",
            "Epoch 792/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5087 - acc: 0.7356\n",
            "Epoch 793/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5063 - acc: 0.7275\n",
            "Epoch 794/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5234 - acc: 0.7150\n",
            "Epoch 795/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5295 - acc: 0.7231\n",
            "Epoch 796/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5258 - acc: 0.7150\n",
            "Epoch 797/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5277 - acc: 0.7187\n",
            "Epoch 798/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5266 - acc: 0.7290\n",
            "Epoch 799/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5221 - acc: 0.7334\n",
            "Epoch 800/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5170 - acc: 0.7283\n",
            "Epoch 801/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5208 - acc: 0.7290\n",
            "Epoch 802/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5355 - acc: 0.7172\n",
            "Epoch 803/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5076 - acc: 0.7268\n",
            "Epoch 804/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5321 - acc: 0.7158\n",
            "Epoch 805/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5056 - acc: 0.7401\n",
            "Epoch 806/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5305 - acc: 0.7275\n",
            "Epoch 807/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5299 - acc: 0.7180\n",
            "Epoch 808/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5052 - acc: 0.7312\n",
            "Epoch 809/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5233 - acc: 0.7261\n",
            "Epoch 810/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5106 - acc: 0.7268\n",
            "Epoch 811/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.4997 - acc: 0.7408\n",
            "Epoch 812/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5101 - acc: 0.7283\n",
            "Epoch 813/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5155 - acc: 0.7224\n",
            "Epoch 814/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5084 - acc: 0.7297\n",
            "Epoch 815/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.5271 - acc: 0.7150\n",
            "Epoch 816/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5260 - acc: 0.7268\n",
            "Epoch 817/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5031 - acc: 0.7393\n",
            "Epoch 818/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5228 - acc: 0.7364\n",
            "Epoch 819/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5155 - acc: 0.7334\n",
            "Epoch 820/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5204 - acc: 0.7180\n",
            "Epoch 821/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5098 - acc: 0.7231\n",
            "Epoch 822/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5291 - acc: 0.7216\n",
            "Epoch 823/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5239 - acc: 0.7113\n",
            "Epoch 824/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5229 - acc: 0.7327\n",
            "Epoch 825/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5069 - acc: 0.7342\n",
            "Epoch 826/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5273 - acc: 0.7246\n",
            "Epoch 827/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5130 - acc: 0.7474\n",
            "Epoch 828/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5259 - acc: 0.7364\n",
            "Epoch 829/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5092 - acc: 0.7283\n",
            "Epoch 830/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5394 - acc: 0.7202\n",
            "Epoch 831/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5065 - acc: 0.7356\n",
            "Epoch 832/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5059 - acc: 0.7297\n",
            "Epoch 833/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5373 - acc: 0.7150\n",
            "Epoch 834/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5111 - acc: 0.7327\n",
            "Epoch 835/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5157 - acc: 0.7320\n",
            "Epoch 836/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5114 - acc: 0.7393\n",
            "Epoch 837/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5154 - acc: 0.7290\n",
            "Epoch 838/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5198 - acc: 0.7261\n",
            "Epoch 839/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5155 - acc: 0.7356\n",
            "Epoch 840/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5082 - acc: 0.7283\n",
            "Epoch 841/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5307 - acc: 0.7180\n",
            "Epoch 842/2000\n",
            "1358/1358 [==============================] - 0s 64us/step - loss: 0.5342 - acc: 0.7158\n",
            "Epoch 843/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5212 - acc: 0.7297\n",
            "Epoch 844/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5291 - acc: 0.7165\n",
            "Epoch 845/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5243 - acc: 0.7150\n",
            "Epoch 846/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5274 - acc: 0.7143\n",
            "Epoch 847/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5176 - acc: 0.7290\n",
            "Epoch 848/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5181 - acc: 0.7246\n",
            "Epoch 849/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5126 - acc: 0.7364\n",
            "Epoch 850/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5216 - acc: 0.7239\n",
            "Epoch 851/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5207 - acc: 0.7231\n",
            "Epoch 852/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5192 - acc: 0.7246\n",
            "Epoch 853/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5145 - acc: 0.7334\n",
            "Epoch 854/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5314 - acc: 0.7165\n",
            "Epoch 855/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5260 - acc: 0.7135\n",
            "Epoch 856/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5160 - acc: 0.7253\n",
            "Epoch 857/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5180 - acc: 0.7158\n",
            "Epoch 858/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5251 - acc: 0.7202\n",
            "Epoch 859/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5394 - acc: 0.7150\n",
            "Epoch 860/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5188 - acc: 0.7246\n",
            "Epoch 861/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5282 - acc: 0.7224\n",
            "Epoch 862/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5300 - acc: 0.7180\n",
            "Epoch 863/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5195 - acc: 0.7423\n",
            "Epoch 864/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5365 - acc: 0.7290\n",
            "Epoch 865/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5277 - acc: 0.7275\n",
            "Epoch 866/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5166 - acc: 0.7371\n",
            "Epoch 867/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5240 - acc: 0.7401\n",
            "Epoch 868/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5259 - acc: 0.7334\n",
            "Epoch 869/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5124 - acc: 0.7261\n",
            "Epoch 870/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5104 - acc: 0.7320\n",
            "Epoch 871/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5224 - acc: 0.7290\n",
            "Epoch 872/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5138 - acc: 0.7349\n",
            "Epoch 873/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5220 - acc: 0.7356\n",
            "Epoch 874/2000\n",
            "1358/1358 [==============================] - 0s 98us/step - loss: 0.4958 - acc: 0.7290\n",
            "Epoch 875/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5083 - acc: 0.7408\n",
            "Epoch 876/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5241 - acc: 0.7253\n",
            "Epoch 877/2000\n",
            "1358/1358 [==============================] - 0s 93us/step - loss: 0.5212 - acc: 0.7371\n",
            "Epoch 878/2000\n",
            "1358/1358 [==============================] - 0s 93us/step - loss: 0.5219 - acc: 0.7150\n",
            "Epoch 879/2000\n",
            "1358/1358 [==============================] - 0s 92us/step - loss: 0.5301 - acc: 0.7150\n",
            "Epoch 880/2000\n",
            "1358/1358 [==============================] - 0s 96us/step - loss: 0.5144 - acc: 0.7327\n",
            "Epoch 881/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5332 - acc: 0.7209\n",
            "Epoch 882/2000\n",
            "1358/1358 [==============================] - 0s 110us/step - loss: 0.5289 - acc: 0.7224\n",
            "Epoch 883/2000\n",
            "1358/1358 [==============================] - 0s 94us/step - loss: 0.5274 - acc: 0.7194\n",
            "Epoch 884/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5105 - acc: 0.7334\n",
            "Epoch 885/2000\n",
            "1358/1358 [==============================] - 0s 94us/step - loss: 0.5291 - acc: 0.7224\n",
            "Epoch 886/2000\n",
            "1358/1358 [==============================] - 0s 109us/step - loss: 0.5231 - acc: 0.7187\n",
            "Epoch 887/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5124 - acc: 0.7312\n",
            "Epoch 888/2000\n",
            "1358/1358 [==============================] - 0s 99us/step - loss: 0.5117 - acc: 0.7297\n",
            "Epoch 889/2000\n",
            "1358/1358 [==============================] - 0s 99us/step - loss: 0.5189 - acc: 0.7158\n",
            "Epoch 890/2000\n",
            "1358/1358 [==============================] - 0s 101us/step - loss: 0.5245 - acc: 0.7297\n",
            "Epoch 891/2000\n",
            "1358/1358 [==============================] - 0s 99us/step - loss: 0.5138 - acc: 0.7290\n",
            "Epoch 892/2000\n",
            "1358/1358 [==============================] - 0s 97us/step - loss: 0.5245 - acc: 0.7312\n",
            "Epoch 893/2000\n",
            "1358/1358 [==============================] - 0s 97us/step - loss: 0.5053 - acc: 0.7320\n",
            "Epoch 894/2000\n",
            "1358/1358 [==============================] - 0s 93us/step - loss: 0.5247 - acc: 0.7305\n",
            "Epoch 895/2000\n",
            "1358/1358 [==============================] - 0s 113us/step - loss: 0.5232 - acc: 0.7312\n",
            "Epoch 896/2000\n",
            "1358/1358 [==============================] - 0s 97us/step - loss: 0.5272 - acc: 0.7305\n",
            "Epoch 897/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5129 - acc: 0.7349\n",
            "Epoch 898/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5208 - acc: 0.7334\n",
            "Epoch 899/2000\n",
            "1358/1358 [==============================] - 0s 92us/step - loss: 0.5240 - acc: 0.7158\n",
            "Epoch 900/2000\n",
            "1358/1358 [==============================] - 0s 97us/step - loss: 0.5183 - acc: 0.7253\n",
            "Epoch 901/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5059 - acc: 0.7349\n",
            "Epoch 902/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5218 - acc: 0.7283\n",
            "Epoch 903/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5274 - acc: 0.7349\n",
            "Epoch 904/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5270 - acc: 0.7349\n",
            "Epoch 905/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5169 - acc: 0.7482\n",
            "Epoch 906/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5201 - acc: 0.7305\n",
            "Epoch 907/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.4970 - acc: 0.7342\n",
            "Epoch 908/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5254 - acc: 0.7320\n",
            "Epoch 909/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5082 - acc: 0.7356\n",
            "Epoch 910/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5043 - acc: 0.7342\n",
            "Epoch 911/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5219 - acc: 0.7135\n",
            "Epoch 912/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5189 - acc: 0.7268\n",
            "Epoch 913/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5146 - acc: 0.7143\n",
            "Epoch 914/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5228 - acc: 0.7312\n",
            "Epoch 915/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5108 - acc: 0.7320\n",
            "Epoch 916/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5178 - acc: 0.7327\n",
            "Epoch 917/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5141 - acc: 0.7327\n",
            "Epoch 918/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5170 - acc: 0.7297\n",
            "Epoch 919/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5192 - acc: 0.7172\n",
            "Epoch 920/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5294 - acc: 0.7180\n",
            "Epoch 921/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5177 - acc: 0.7268\n",
            "Epoch 922/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.4997 - acc: 0.7327\n",
            "Epoch 923/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5260 - acc: 0.7305\n",
            "Epoch 924/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5211 - acc: 0.7423\n",
            "Epoch 925/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5096 - acc: 0.7261\n",
            "Epoch 926/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5278 - acc: 0.7135\n",
            "Epoch 927/2000\n",
            "1358/1358 [==============================] - 0s 96us/step - loss: 0.5204 - acc: 0.7106\n",
            "Epoch 928/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5068 - acc: 0.7334\n",
            "Epoch 929/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5222 - acc: 0.7261\n",
            "Epoch 930/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5025 - acc: 0.7305\n",
            "Epoch 931/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5272 - acc: 0.7327\n",
            "Epoch 932/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5092 - acc: 0.7320\n",
            "Epoch 933/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5185 - acc: 0.7224\n",
            "Epoch 934/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5086 - acc: 0.7401\n",
            "Epoch 935/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5142 - acc: 0.7194\n",
            "Epoch 936/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5033 - acc: 0.7415\n",
            "Epoch 937/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5177 - acc: 0.7305\n",
            "Epoch 938/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5271 - acc: 0.7261\n",
            "Epoch 939/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5245 - acc: 0.7239\n",
            "Epoch 940/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5254 - acc: 0.7224\n",
            "Epoch 941/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5272 - acc: 0.7135\n",
            "Epoch 942/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5206 - acc: 0.7194\n",
            "Epoch 943/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5180 - acc: 0.7275\n",
            "Epoch 944/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5133 - acc: 0.7327\n",
            "Epoch 945/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5197 - acc: 0.7290\n",
            "Epoch 946/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5152 - acc: 0.7253\n",
            "Epoch 947/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5141 - acc: 0.7327\n",
            "Epoch 948/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5421 - acc: 0.7099\n",
            "Epoch 949/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5298 - acc: 0.7187\n",
            "Epoch 950/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5175 - acc: 0.7393\n",
            "Epoch 951/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5205 - acc: 0.7268\n",
            "Epoch 952/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5132 - acc: 0.7216\n",
            "Epoch 953/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5266 - acc: 0.7202\n",
            "Epoch 954/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5175 - acc: 0.7327\n",
            "Epoch 955/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5167 - acc: 0.7327\n",
            "Epoch 956/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5025 - acc: 0.7430\n",
            "Epoch 957/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5147 - acc: 0.7246\n",
            "Epoch 958/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5227 - acc: 0.7187\n",
            "Epoch 959/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5212 - acc: 0.7290\n",
            "Epoch 960/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5075 - acc: 0.7386\n",
            "Epoch 961/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5150 - acc: 0.7209\n",
            "Epoch 962/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5135 - acc: 0.7305\n",
            "Epoch 963/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5294 - acc: 0.7290\n",
            "Epoch 964/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5200 - acc: 0.7297\n",
            "Epoch 965/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5180 - acc: 0.7180\n",
            "Epoch 966/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5224 - acc: 0.7305\n",
            "Epoch 967/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5117 - acc: 0.7239\n",
            "Epoch 968/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5090 - acc: 0.7364\n",
            "Epoch 969/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5244 - acc: 0.7275\n",
            "Epoch 970/2000\n",
            "1358/1358 [==============================] - 0s 65us/step - loss: 0.5217 - acc: 0.7216\n",
            "Epoch 971/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5115 - acc: 0.7327\n",
            "Epoch 972/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5025 - acc: 0.7312\n",
            "Epoch 973/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5280 - acc: 0.7216\n",
            "Epoch 974/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5288 - acc: 0.7349\n",
            "Epoch 975/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5197 - acc: 0.7305\n",
            "Epoch 976/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5190 - acc: 0.7305\n",
            "Epoch 977/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5222 - acc: 0.7261\n",
            "Epoch 978/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5138 - acc: 0.7518\n",
            "Epoch 979/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5233 - acc: 0.7290\n",
            "Epoch 980/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5139 - acc: 0.7268\n",
            "Epoch 981/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5114 - acc: 0.7356\n",
            "Epoch 982/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5337 - acc: 0.7261\n",
            "Epoch 983/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5227 - acc: 0.7297\n",
            "Epoch 984/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5214 - acc: 0.7209\n",
            "Epoch 985/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5202 - acc: 0.7261\n",
            "Epoch 986/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5120 - acc: 0.7297\n",
            "Epoch 987/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5166 - acc: 0.7297\n",
            "Epoch 988/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5124 - acc: 0.7253\n",
            "Epoch 989/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5131 - acc: 0.7187\n",
            "Epoch 990/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5195 - acc: 0.7327\n",
            "Epoch 991/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5131 - acc: 0.7091\n",
            "Epoch 992/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5411 - acc: 0.7231\n",
            "Epoch 993/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5312 - acc: 0.7106\n",
            "Epoch 994/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5380 - acc: 0.7194\n",
            "Epoch 995/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5207 - acc: 0.7312\n",
            "Epoch 996/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5092 - acc: 0.7349\n",
            "Epoch 997/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5203 - acc: 0.7364\n",
            "Epoch 998/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5203 - acc: 0.7224\n",
            "Epoch 999/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5077 - acc: 0.7283\n",
            "Epoch 1000/2000\n",
            "1358/1358 [==============================] - 0s 90us/step - loss: 0.5105 - acc: 0.7261\n",
            "Epoch 1001/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5219 - acc: 0.7312\n",
            "Epoch 1002/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5162 - acc: 0.7371\n",
            "Epoch 1003/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5137 - acc: 0.7239\n",
            "Epoch 1004/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5188 - acc: 0.7378\n",
            "Epoch 1005/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5156 - acc: 0.7297\n",
            "Epoch 1006/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5117 - acc: 0.7246\n",
            "Epoch 1007/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5188 - acc: 0.7224\n",
            "Epoch 1008/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5163 - acc: 0.7305\n",
            "Epoch 1009/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5170 - acc: 0.7342\n",
            "Epoch 1010/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5281 - acc: 0.7209\n",
            "Epoch 1011/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5098 - acc: 0.7320\n",
            "Epoch 1012/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5176 - acc: 0.7231\n",
            "Epoch 1013/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5229 - acc: 0.7356\n",
            "Epoch 1014/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5143 - acc: 0.7393\n",
            "Epoch 1015/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5091 - acc: 0.7349\n",
            "Epoch 1016/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5161 - acc: 0.7268\n",
            "Epoch 1017/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5144 - acc: 0.7253\n",
            "Epoch 1018/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5268 - acc: 0.7327\n",
            "Epoch 1019/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5058 - acc: 0.7253\n",
            "Epoch 1020/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5174 - acc: 0.7246\n",
            "Epoch 1021/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5120 - acc: 0.7261\n",
            "Epoch 1022/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5158 - acc: 0.7253\n",
            "Epoch 1023/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5076 - acc: 0.7467\n",
            "Epoch 1024/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5051 - acc: 0.7430\n",
            "Epoch 1025/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5253 - acc: 0.7342\n",
            "Epoch 1026/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5213 - acc: 0.7239\n",
            "Epoch 1027/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.4980 - acc: 0.7268\n",
            "Epoch 1028/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5195 - acc: 0.7320\n",
            "Epoch 1029/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5251 - acc: 0.7320\n",
            "Epoch 1030/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5104 - acc: 0.7275\n",
            "Epoch 1031/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5116 - acc: 0.7408\n",
            "Epoch 1032/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5432 - acc: 0.7172\n",
            "Epoch 1033/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5124 - acc: 0.7275\n",
            "Epoch 1034/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5252 - acc: 0.7209\n",
            "Epoch 1035/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.4999 - acc: 0.7408\n",
            "Epoch 1036/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5217 - acc: 0.7415\n",
            "Epoch 1037/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5199 - acc: 0.7158\n",
            "Epoch 1038/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5117 - acc: 0.7459\n",
            "Epoch 1039/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5192 - acc: 0.7364\n",
            "Epoch 1040/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5277 - acc: 0.7290\n",
            "Epoch 1041/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5213 - acc: 0.7261\n",
            "Epoch 1042/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5167 - acc: 0.7216\n",
            "Epoch 1043/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5088 - acc: 0.7275\n",
            "Epoch 1044/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5216 - acc: 0.7187\n",
            "Epoch 1045/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5305 - acc: 0.7231\n",
            "Epoch 1046/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5141 - acc: 0.7246\n",
            "Epoch 1047/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5099 - acc: 0.7415\n",
            "Epoch 1048/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5187 - acc: 0.7224\n",
            "Epoch 1049/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5220 - acc: 0.7334\n",
            "Epoch 1050/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5046 - acc: 0.7415\n",
            "Epoch 1051/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5038 - acc: 0.7415\n",
            "Epoch 1052/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5225 - acc: 0.7305\n",
            "Epoch 1053/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5191 - acc: 0.7283\n",
            "Epoch 1054/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5231 - acc: 0.7209\n",
            "Epoch 1055/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5064 - acc: 0.7386\n",
            "Epoch 1056/2000\n",
            "1358/1358 [==============================] - 0s 96us/step - loss: 0.4978 - acc: 0.7423\n",
            "Epoch 1057/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5142 - acc: 0.7356\n",
            "Epoch 1058/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5106 - acc: 0.7430\n",
            "Epoch 1059/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5025 - acc: 0.7423\n",
            "Epoch 1060/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5132 - acc: 0.7312\n",
            "Epoch 1061/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5302 - acc: 0.7231\n",
            "Epoch 1062/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5218 - acc: 0.7283\n",
            "Epoch 1063/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5331 - acc: 0.7320\n",
            "Epoch 1064/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5056 - acc: 0.7423\n",
            "Epoch 1065/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5096 - acc: 0.7268\n",
            "Epoch 1066/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5148 - acc: 0.7378\n",
            "Epoch 1067/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5257 - acc: 0.7261\n",
            "Epoch 1068/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5028 - acc: 0.7356\n",
            "Epoch 1069/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5198 - acc: 0.7202\n",
            "Epoch 1070/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5088 - acc: 0.7371\n",
            "Epoch 1071/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.4986 - acc: 0.7297\n",
            "Epoch 1072/2000\n",
            "1358/1358 [==============================] - 0s 92us/step - loss: 0.5129 - acc: 0.7393\n",
            "Epoch 1073/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5299 - acc: 0.7297\n",
            "Epoch 1074/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5078 - acc: 0.7452\n",
            "Epoch 1075/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5201 - acc: 0.7312\n",
            "Epoch 1076/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5185 - acc: 0.7268\n",
            "Epoch 1077/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5021 - acc: 0.7378\n",
            "Epoch 1078/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5155 - acc: 0.7334\n",
            "Epoch 1079/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5155 - acc: 0.7430\n",
            "Epoch 1080/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5068 - acc: 0.7378\n",
            "Epoch 1081/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5032 - acc: 0.7459\n",
            "Epoch 1082/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.4986 - acc: 0.7430\n",
            "Epoch 1083/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5290 - acc: 0.7297\n",
            "Epoch 1084/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5184 - acc: 0.7180\n",
            "Epoch 1085/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5283 - acc: 0.7283\n",
            "Epoch 1086/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5216 - acc: 0.7320\n",
            "Epoch 1087/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5168 - acc: 0.7261\n",
            "Epoch 1088/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5219 - acc: 0.7246\n",
            "Epoch 1089/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5141 - acc: 0.7356\n",
            "Epoch 1090/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5336 - acc: 0.7150\n",
            "Epoch 1091/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5077 - acc: 0.7386\n",
            "Epoch 1092/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5011 - acc: 0.7393\n",
            "Epoch 1093/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5297 - acc: 0.7239\n",
            "Epoch 1094/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5301 - acc: 0.7172\n",
            "Epoch 1095/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5183 - acc: 0.7356\n",
            "Epoch 1096/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5171 - acc: 0.7143\n",
            "Epoch 1097/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5220 - acc: 0.7283\n",
            "Epoch 1098/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5215 - acc: 0.7312\n",
            "Epoch 1099/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5308 - acc: 0.7158\n",
            "Epoch 1100/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5192 - acc: 0.7342\n",
            "Epoch 1101/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5061 - acc: 0.7356\n",
            "Epoch 1102/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5069 - acc: 0.7268\n",
            "Epoch 1103/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5111 - acc: 0.7224\n",
            "Epoch 1104/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5123 - acc: 0.7239\n",
            "Epoch 1105/2000\n",
            "1358/1358 [==============================] - 0s 90us/step - loss: 0.5179 - acc: 0.7246\n",
            "Epoch 1106/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5259 - acc: 0.7268\n",
            "Epoch 1107/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5079 - acc: 0.7430\n",
            "Epoch 1108/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5211 - acc: 0.7393\n",
            "Epoch 1109/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5047 - acc: 0.7320\n",
            "Epoch 1110/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5112 - acc: 0.7423\n",
            "Epoch 1111/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5227 - acc: 0.7312\n",
            "Epoch 1112/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5228 - acc: 0.7334\n",
            "Epoch 1113/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5222 - acc: 0.7158\n",
            "Epoch 1114/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5197 - acc: 0.7334\n",
            "Epoch 1115/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5074 - acc: 0.7378\n",
            "Epoch 1116/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5068 - acc: 0.7356\n",
            "Epoch 1117/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5261 - acc: 0.7378\n",
            "Epoch 1118/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5107 - acc: 0.7268\n",
            "Epoch 1119/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5073 - acc: 0.7290\n",
            "Epoch 1120/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5299 - acc: 0.7158\n",
            "Epoch 1121/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5120 - acc: 0.7378\n",
            "Epoch 1122/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5269 - acc: 0.7253\n",
            "Epoch 1123/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.4880 - acc: 0.7526\n",
            "Epoch 1124/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.4962 - acc: 0.7378\n",
            "Epoch 1125/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5065 - acc: 0.7437\n",
            "Epoch 1126/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.4939 - acc: 0.7378\n",
            "Epoch 1127/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5019 - acc: 0.7371\n",
            "Epoch 1128/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5229 - acc: 0.7283\n",
            "Epoch 1129/2000\n",
            "1358/1358 [==============================] - 0s 66us/step - loss: 0.5153 - acc: 0.7371\n",
            "Epoch 1130/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5337 - acc: 0.7224\n",
            "Epoch 1131/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5342 - acc: 0.7275\n",
            "Epoch 1132/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5155 - acc: 0.7253\n",
            "Epoch 1133/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5002 - acc: 0.7437\n",
            "Epoch 1134/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5061 - acc: 0.7364\n",
            "Epoch 1135/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5219 - acc: 0.7283\n",
            "Epoch 1136/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5041 - acc: 0.7467\n",
            "Epoch 1137/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5098 - acc: 0.7482\n",
            "Epoch 1138/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5233 - acc: 0.7320\n",
            "Epoch 1139/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.4977 - acc: 0.7320\n",
            "Epoch 1140/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5134 - acc: 0.7423\n",
            "Epoch 1141/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5176 - acc: 0.7253\n",
            "Epoch 1142/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5108 - acc: 0.7275\n",
            "Epoch 1143/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5144 - acc: 0.7437\n",
            "Epoch 1144/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5035 - acc: 0.7423\n",
            "Epoch 1145/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5202 - acc: 0.7386\n",
            "Epoch 1146/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5129 - acc: 0.7364\n",
            "Epoch 1147/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5012 - acc: 0.7482\n",
            "Epoch 1148/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5084 - acc: 0.7423\n",
            "Epoch 1149/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5107 - acc: 0.7371\n",
            "Epoch 1150/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5197 - acc: 0.7342\n",
            "Epoch 1151/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5075 - acc: 0.7312\n",
            "Epoch 1152/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5119 - acc: 0.7261\n",
            "Epoch 1153/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5103 - acc: 0.7349\n",
            "Epoch 1154/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5078 - acc: 0.7320\n",
            "Epoch 1155/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5164 - acc: 0.7393\n",
            "Epoch 1156/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5171 - acc: 0.7305\n",
            "Epoch 1157/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5161 - acc: 0.7239\n",
            "Epoch 1158/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5122 - acc: 0.7202\n",
            "Epoch 1159/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5152 - acc: 0.7371\n",
            "Epoch 1160/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5197 - acc: 0.7113\n",
            "Epoch 1161/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5200 - acc: 0.7320\n",
            "Epoch 1162/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5083 - acc: 0.7349\n",
            "Epoch 1163/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5130 - acc: 0.7320\n",
            "Epoch 1164/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5202 - acc: 0.7320\n",
            "Epoch 1165/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5070 - acc: 0.7364\n",
            "Epoch 1166/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5087 - acc: 0.7496\n",
            "Epoch 1167/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5106 - acc: 0.7342\n",
            "Epoch 1168/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5059 - acc: 0.7452\n",
            "Epoch 1169/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5160 - acc: 0.7334\n",
            "Epoch 1170/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5122 - acc: 0.7356\n",
            "Epoch 1171/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5118 - acc: 0.7246\n",
            "Epoch 1172/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5062 - acc: 0.7415\n",
            "Epoch 1173/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5258 - acc: 0.7305\n",
            "Epoch 1174/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5273 - acc: 0.7320\n",
            "Epoch 1175/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5198 - acc: 0.7246\n",
            "Epoch 1176/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5108 - acc: 0.7261\n",
            "Epoch 1177/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5139 - acc: 0.7290\n",
            "Epoch 1178/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5176 - acc: 0.7253\n",
            "Epoch 1179/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.4949 - acc: 0.7408\n",
            "Epoch 1180/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5170 - acc: 0.7349\n",
            "Epoch 1181/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5202 - acc: 0.7364\n",
            "Epoch 1182/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5121 - acc: 0.7246\n",
            "Epoch 1183/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5105 - acc: 0.7327\n",
            "Epoch 1184/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5193 - acc: 0.7371\n",
            "Epoch 1185/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5202 - acc: 0.7320\n",
            "Epoch 1186/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5212 - acc: 0.7224\n",
            "Epoch 1187/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5161 - acc: 0.7224\n",
            "Epoch 1188/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5177 - acc: 0.7378\n",
            "Epoch 1189/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5093 - acc: 0.7423\n",
            "Epoch 1190/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5127 - acc: 0.7268\n",
            "Epoch 1191/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5033 - acc: 0.7349\n",
            "Epoch 1192/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5147 - acc: 0.7364\n",
            "Epoch 1193/2000\n",
            "1358/1358 [==============================] - 0s 94us/step - loss: 0.5072 - acc: 0.7224\n",
            "Epoch 1194/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5161 - acc: 0.7320\n",
            "Epoch 1195/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5142 - acc: 0.7342\n",
            "Epoch 1196/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5243 - acc: 0.7305\n",
            "Epoch 1197/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5262 - acc: 0.7239\n",
            "Epoch 1198/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5182 - acc: 0.7275\n",
            "Epoch 1199/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.4959 - acc: 0.7371\n",
            "Epoch 1200/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5002 - acc: 0.7371\n",
            "Epoch 1201/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5193 - acc: 0.7459\n",
            "Epoch 1202/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5066 - acc: 0.7356\n",
            "Epoch 1203/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5134 - acc: 0.7327\n",
            "Epoch 1204/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5118 - acc: 0.7283\n",
            "Epoch 1205/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5096 - acc: 0.7268\n",
            "Epoch 1206/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5261 - acc: 0.7172\n",
            "Epoch 1207/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5119 - acc: 0.7312\n",
            "Epoch 1208/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5121 - acc: 0.7415\n",
            "Epoch 1209/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5176 - acc: 0.7143\n",
            "Epoch 1210/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5142 - acc: 0.7393\n",
            "Epoch 1211/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5078 - acc: 0.7474\n",
            "Epoch 1212/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5117 - acc: 0.7371\n",
            "Epoch 1213/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5194 - acc: 0.7327\n",
            "Epoch 1214/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5235 - acc: 0.7320\n",
            "Epoch 1215/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5060 - acc: 0.7334\n",
            "Epoch 1216/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5056 - acc: 0.7364\n",
            "Epoch 1217/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5060 - acc: 0.7386\n",
            "Epoch 1218/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5120 - acc: 0.7312\n",
            "Epoch 1219/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5087 - acc: 0.7297\n",
            "Epoch 1220/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5037 - acc: 0.7474\n",
            "Epoch 1221/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5086 - acc: 0.7401\n",
            "Epoch 1222/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.4932 - acc: 0.7445\n",
            "Epoch 1223/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5075 - acc: 0.7334\n",
            "Epoch 1224/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5193 - acc: 0.7312\n",
            "Epoch 1225/2000\n",
            "1358/1358 [==============================] - 0s 91us/step - loss: 0.5118 - acc: 0.7283\n",
            "Epoch 1226/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5153 - acc: 0.7216\n",
            "Epoch 1227/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5036 - acc: 0.7401\n",
            "Epoch 1228/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5174 - acc: 0.7334\n",
            "Epoch 1229/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5059 - acc: 0.7548\n",
            "Epoch 1230/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5259 - acc: 0.7334\n",
            "Epoch 1231/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5168 - acc: 0.7386\n",
            "Epoch 1232/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5132 - acc: 0.7415\n",
            "Epoch 1233/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5281 - acc: 0.7246\n",
            "Epoch 1234/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5074 - acc: 0.7467\n",
            "Epoch 1235/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5061 - acc: 0.7364\n",
            "Epoch 1236/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5050 - acc: 0.7408\n",
            "Epoch 1237/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5096 - acc: 0.7430\n",
            "Epoch 1238/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5087 - acc: 0.7393\n",
            "Epoch 1239/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5099 - acc: 0.7356\n",
            "Epoch 1240/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5118 - acc: 0.7320\n",
            "Epoch 1241/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5252 - acc: 0.7231\n",
            "Epoch 1242/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5053 - acc: 0.7349\n",
            "Epoch 1243/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5028 - acc: 0.7312\n",
            "Epoch 1244/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5249 - acc: 0.7290\n",
            "Epoch 1245/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5137 - acc: 0.7393\n",
            "Epoch 1246/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5131 - acc: 0.7305\n",
            "Epoch 1247/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5182 - acc: 0.7378\n",
            "Epoch 1248/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5268 - acc: 0.7283\n",
            "Epoch 1249/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5085 - acc: 0.7320\n",
            "Epoch 1250/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.4983 - acc: 0.7482\n",
            "Epoch 1251/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5088 - acc: 0.7312\n",
            "Epoch 1252/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5278 - acc: 0.7216\n",
            "Epoch 1253/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5081 - acc: 0.7393\n",
            "Epoch 1254/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5060 - acc: 0.7459\n",
            "Epoch 1255/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5044 - acc: 0.7401\n",
            "Epoch 1256/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5077 - acc: 0.7371\n",
            "Epoch 1257/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.4990 - acc: 0.7290\n",
            "Epoch 1258/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5014 - acc: 0.7474\n",
            "Epoch 1259/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5185 - acc: 0.7342\n",
            "Epoch 1260/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.4944 - acc: 0.7371\n",
            "Epoch 1261/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5143 - acc: 0.7401\n",
            "Epoch 1262/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5079 - acc: 0.7320\n",
            "Epoch 1263/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5118 - acc: 0.7349\n",
            "Epoch 1264/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5117 - acc: 0.7261\n",
            "Epoch 1265/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5145 - acc: 0.7430\n",
            "Epoch 1266/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5021 - acc: 0.7459\n",
            "Epoch 1267/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5327 - acc: 0.7172\n",
            "Epoch 1268/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5036 - acc: 0.7356\n",
            "Epoch 1269/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5252 - acc: 0.7275\n",
            "Epoch 1270/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5206 - acc: 0.7312\n",
            "Epoch 1271/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5289 - acc: 0.7239\n",
            "Epoch 1272/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5086 - acc: 0.7327\n",
            "Epoch 1273/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5098 - acc: 0.7371\n",
            "Epoch 1274/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5068 - acc: 0.7423\n",
            "Epoch 1275/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5177 - acc: 0.7378\n",
            "Epoch 1276/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5088 - acc: 0.7231\n",
            "Epoch 1277/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5214 - acc: 0.7091\n",
            "Epoch 1278/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5165 - acc: 0.7401\n",
            "Epoch 1279/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5080 - acc: 0.7504\n",
            "Epoch 1280/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5150 - acc: 0.7342\n",
            "Epoch 1281/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5237 - acc: 0.7275\n",
            "Epoch 1282/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5383 - acc: 0.7187\n",
            "Epoch 1283/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5113 - acc: 0.7364\n",
            "Epoch 1284/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5137 - acc: 0.7327\n",
            "Epoch 1285/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5014 - acc: 0.7356\n",
            "Epoch 1286/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5104 - acc: 0.7239\n",
            "Epoch 1287/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5024 - acc: 0.7320\n",
            "Epoch 1288/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5003 - acc: 0.7364\n",
            "Epoch 1289/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5097 - acc: 0.7327\n",
            "Epoch 1290/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5106 - acc: 0.7401\n",
            "Epoch 1291/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5097 - acc: 0.7393\n",
            "Epoch 1292/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5034 - acc: 0.7342\n",
            "Epoch 1293/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5332 - acc: 0.7253\n",
            "Epoch 1294/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5090 - acc: 0.7342\n",
            "Epoch 1295/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5154 - acc: 0.7401\n",
            "Epoch 1296/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5052 - acc: 0.7445\n",
            "Epoch 1297/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5211 - acc: 0.7371\n",
            "Epoch 1298/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5002 - acc: 0.7327\n",
            "Epoch 1299/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5101 - acc: 0.7342\n",
            "Epoch 1300/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5092 - acc: 0.7356\n",
            "Epoch 1301/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5043 - acc: 0.7378\n",
            "Epoch 1302/2000\n",
            "1358/1358 [==============================] - 0s 67us/step - loss: 0.5236 - acc: 0.7275\n",
            "Epoch 1303/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5098 - acc: 0.7305\n",
            "Epoch 1304/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5090 - acc: 0.7364\n",
            "Epoch 1305/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5009 - acc: 0.7364\n",
            "Epoch 1306/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5101 - acc: 0.7283\n",
            "Epoch 1307/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5167 - acc: 0.7334\n",
            "Epoch 1308/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5129 - acc: 0.7327\n",
            "Epoch 1309/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5094 - acc: 0.7423\n",
            "Epoch 1310/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5098 - acc: 0.7216\n",
            "Epoch 1311/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5158 - acc: 0.7342\n",
            "Epoch 1312/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5091 - acc: 0.7415\n",
            "Epoch 1313/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5183 - acc: 0.7305\n",
            "Epoch 1314/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5186 - acc: 0.7386\n",
            "Epoch 1315/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5109 - acc: 0.7371\n",
            "Epoch 1316/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5159 - acc: 0.7297\n",
            "Epoch 1317/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5276 - acc: 0.7239\n",
            "Epoch 1318/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5351 - acc: 0.7172\n",
            "Epoch 1319/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5123 - acc: 0.7386\n",
            "Epoch 1320/2000\n",
            "1358/1358 [==============================] - 0s 91us/step - loss: 0.5285 - acc: 0.7187\n",
            "Epoch 1321/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5040 - acc: 0.7445\n",
            "Epoch 1322/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.4988 - acc: 0.7401\n",
            "Epoch 1323/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5124 - acc: 0.7342\n",
            "Epoch 1324/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5141 - acc: 0.7224\n",
            "Epoch 1325/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5064 - acc: 0.7334\n",
            "Epoch 1326/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5083 - acc: 0.7393\n",
            "Epoch 1327/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5071 - acc: 0.7327\n",
            "Epoch 1328/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5202 - acc: 0.7378\n",
            "Epoch 1329/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5064 - acc: 0.7408\n",
            "Epoch 1330/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5051 - acc: 0.7320\n",
            "Epoch 1331/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5228 - acc: 0.7327\n",
            "Epoch 1332/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5071 - acc: 0.7334\n",
            "Epoch 1333/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5356 - acc: 0.7371\n",
            "Epoch 1334/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5085 - acc: 0.7305\n",
            "Epoch 1335/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5219 - acc: 0.7253\n",
            "Epoch 1336/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5167 - acc: 0.7334\n",
            "Epoch 1337/2000\n",
            "1358/1358 [==============================] - 0s 92us/step - loss: 0.5067 - acc: 0.7371\n",
            "Epoch 1338/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5012 - acc: 0.7504\n",
            "Epoch 1339/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5144 - acc: 0.7327\n",
            "Epoch 1340/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5034 - acc: 0.7430\n",
            "Epoch 1341/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5134 - acc: 0.7364\n",
            "Epoch 1342/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5027 - acc: 0.7423\n",
            "Epoch 1343/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5075 - acc: 0.7364\n",
            "Epoch 1344/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5074 - acc: 0.7408\n",
            "Epoch 1345/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5060 - acc: 0.7467\n",
            "Epoch 1346/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.4998 - acc: 0.7437\n",
            "Epoch 1347/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5117 - acc: 0.7356\n",
            "Epoch 1348/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.4871 - acc: 0.7445\n",
            "Epoch 1349/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5193 - acc: 0.7297\n",
            "Epoch 1350/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5294 - acc: 0.7239\n",
            "Epoch 1351/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5105 - acc: 0.7305\n",
            "Epoch 1352/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5059 - acc: 0.7378\n",
            "Epoch 1353/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5127 - acc: 0.7401\n",
            "Epoch 1354/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5235 - acc: 0.7275\n",
            "Epoch 1355/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5095 - acc: 0.7401\n",
            "Epoch 1356/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5020 - acc: 0.7408\n",
            "Epoch 1357/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5157 - acc: 0.7216\n",
            "Epoch 1358/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5159 - acc: 0.7231\n",
            "Epoch 1359/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5016 - acc: 0.7452\n",
            "Epoch 1360/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5131 - acc: 0.7312\n",
            "Epoch 1361/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5193 - acc: 0.7386\n",
            "Epoch 1362/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5174 - acc: 0.7202\n",
            "Epoch 1363/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5205 - acc: 0.7320\n",
            "Epoch 1364/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5147 - acc: 0.7091\n",
            "Epoch 1365/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5053 - acc: 0.7320\n",
            "Epoch 1366/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5265 - acc: 0.7334\n",
            "Epoch 1367/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5096 - acc: 0.7364\n",
            "Epoch 1368/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5078 - acc: 0.7489\n",
            "Epoch 1369/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5114 - acc: 0.7305\n",
            "Epoch 1370/2000\n",
            "1358/1358 [==============================] - 0s 90us/step - loss: 0.5099 - acc: 0.7261\n",
            "Epoch 1371/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5195 - acc: 0.7401\n",
            "Epoch 1372/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5056 - acc: 0.7305\n",
            "Epoch 1373/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5099 - acc: 0.7283\n",
            "Epoch 1374/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5089 - acc: 0.7320\n",
            "Epoch 1375/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5087 - acc: 0.7312\n",
            "Epoch 1376/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5184 - acc: 0.7231\n",
            "Epoch 1377/2000\n",
            "1358/1358 [==============================] - 0s 92us/step - loss: 0.5044 - acc: 0.7430\n",
            "Epoch 1378/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5061 - acc: 0.7239\n",
            "Epoch 1379/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5036 - acc: 0.7231\n",
            "Epoch 1380/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5257 - acc: 0.7202\n",
            "Epoch 1381/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5197 - acc: 0.7356\n",
            "Epoch 1382/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5026 - acc: 0.7474\n",
            "Epoch 1383/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5068 - acc: 0.7305\n",
            "Epoch 1384/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5068 - acc: 0.7349\n",
            "Epoch 1385/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5142 - acc: 0.7371\n",
            "Epoch 1386/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5229 - acc: 0.7393\n",
            "Epoch 1387/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5176 - acc: 0.7327\n",
            "Epoch 1388/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5099 - acc: 0.7305\n",
            "Epoch 1389/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5053 - acc: 0.7268\n",
            "Epoch 1390/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5060 - acc: 0.7305\n",
            "Epoch 1391/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5225 - acc: 0.7246\n",
            "Epoch 1392/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5136 - acc: 0.7356\n",
            "Epoch 1393/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5164 - acc: 0.7356\n",
            "Epoch 1394/2000\n",
            "1358/1358 [==============================] - 0s 91us/step - loss: 0.5219 - acc: 0.7261\n",
            "Epoch 1395/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5164 - acc: 0.7297\n",
            "Epoch 1396/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5189 - acc: 0.7496\n",
            "Epoch 1397/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5034 - acc: 0.7445\n",
            "Epoch 1398/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5171 - acc: 0.7297\n",
            "Epoch 1399/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5084 - acc: 0.7297\n",
            "Epoch 1400/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.4978 - acc: 0.7423\n",
            "Epoch 1401/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5223 - acc: 0.7268\n",
            "Epoch 1402/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5235 - acc: 0.7297\n",
            "Epoch 1403/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5141 - acc: 0.7275\n",
            "Epoch 1404/2000\n",
            "1358/1358 [==============================] - 0s 92us/step - loss: 0.5040 - acc: 0.7334\n",
            "Epoch 1405/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5194 - acc: 0.7253\n",
            "Epoch 1406/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5121 - acc: 0.7305\n",
            "Epoch 1407/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5173 - acc: 0.7401\n",
            "Epoch 1408/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.4969 - acc: 0.7342\n",
            "Epoch 1409/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5206 - acc: 0.7202\n",
            "Epoch 1410/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5194 - acc: 0.7253\n",
            "Epoch 1411/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5280 - acc: 0.7356\n",
            "Epoch 1412/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5171 - acc: 0.7371\n",
            "Epoch 1413/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5119 - acc: 0.7386\n",
            "Epoch 1414/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5056 - acc: 0.7342\n",
            "Epoch 1415/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5252 - acc: 0.7239\n",
            "Epoch 1416/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5058 - acc: 0.7437\n",
            "Epoch 1417/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5051 - acc: 0.7334\n",
            "Epoch 1418/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5181 - acc: 0.7246\n",
            "Epoch 1419/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5054 - acc: 0.7356\n",
            "Epoch 1420/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5159 - acc: 0.7275\n",
            "Epoch 1421/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.4970 - acc: 0.7452\n",
            "Epoch 1422/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5026 - acc: 0.7408\n",
            "Epoch 1423/2000\n",
            "1358/1358 [==============================] - 0s 95us/step - loss: 0.5098 - acc: 0.7364\n",
            "Epoch 1424/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5045 - acc: 0.7371\n",
            "Epoch 1425/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5197 - acc: 0.7401\n",
            "Epoch 1426/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5236 - acc: 0.7275\n",
            "Epoch 1427/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5057 - acc: 0.7371\n",
            "Epoch 1428/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5103 - acc: 0.7452\n",
            "Epoch 1429/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5061 - acc: 0.7334\n",
            "Epoch 1430/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5079 - acc: 0.7327\n",
            "Epoch 1431/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5162 - acc: 0.7342\n",
            "Epoch 1432/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.4952 - acc: 0.7371\n",
            "Epoch 1433/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5044 - acc: 0.7290\n",
            "Epoch 1434/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5117 - acc: 0.7371\n",
            "Epoch 1435/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.4981 - acc: 0.7386\n",
            "Epoch 1436/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5032 - acc: 0.7327\n",
            "Epoch 1437/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5129 - acc: 0.7342\n",
            "Epoch 1438/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5035 - acc: 0.7459\n",
            "Epoch 1439/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5056 - acc: 0.7518\n",
            "Epoch 1440/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.4925 - acc: 0.7371\n",
            "Epoch 1441/2000\n",
            "1358/1358 [==============================] - 0s 91us/step - loss: 0.5140 - acc: 0.7320\n",
            "Epoch 1442/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5009 - acc: 0.7312\n",
            "Epoch 1443/2000\n",
            "1358/1358 [==============================] - 0s 91us/step - loss: 0.5054 - acc: 0.7349\n",
            "Epoch 1444/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5035 - acc: 0.7305\n",
            "Epoch 1445/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5137 - acc: 0.7268\n",
            "Epoch 1446/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5199 - acc: 0.7172\n",
            "Epoch 1447/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5110 - acc: 0.7312\n",
            "Epoch 1448/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5241 - acc: 0.7239\n",
            "Epoch 1449/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5097 - acc: 0.7386\n",
            "Epoch 1450/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5094 - acc: 0.7209\n",
            "Epoch 1451/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5115 - acc: 0.7378\n",
            "Epoch 1452/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5201 - acc: 0.7378\n",
            "Epoch 1453/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5138 - acc: 0.7275\n",
            "Epoch 1454/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5071 - acc: 0.7356\n",
            "Epoch 1455/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5010 - acc: 0.7378\n",
            "Epoch 1456/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5075 - acc: 0.7283\n",
            "Epoch 1457/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5193 - acc: 0.7187\n",
            "Epoch 1458/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5246 - acc: 0.7231\n",
            "Epoch 1459/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.4960 - acc: 0.7423\n",
            "Epoch 1460/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5212 - acc: 0.7268\n",
            "Epoch 1461/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5139 - acc: 0.7334\n",
            "Epoch 1462/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5118 - acc: 0.7327\n",
            "Epoch 1463/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5190 - acc: 0.7224\n",
            "Epoch 1464/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5133 - acc: 0.7334\n",
            "Epoch 1465/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5152 - acc: 0.7216\n",
            "Epoch 1466/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5086 - acc: 0.7268\n",
            "Epoch 1467/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5259 - acc: 0.7349\n",
            "Epoch 1468/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5064 - acc: 0.7408\n",
            "Epoch 1469/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.4966 - acc: 0.7496\n",
            "Epoch 1470/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5091 - acc: 0.7386\n",
            "Epoch 1471/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5076 - acc: 0.7320\n",
            "Epoch 1472/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.4966 - acc: 0.7445\n",
            "Epoch 1473/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5120 - acc: 0.7371\n",
            "Epoch 1474/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5020 - acc: 0.7297\n",
            "Epoch 1475/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5104 - acc: 0.7327\n",
            "Epoch 1476/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5099 - acc: 0.7349\n",
            "Epoch 1477/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5255 - acc: 0.7275\n",
            "Epoch 1478/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5240 - acc: 0.7187\n",
            "Epoch 1479/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5020 - acc: 0.7415\n",
            "Epoch 1480/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5246 - acc: 0.7297\n",
            "Epoch 1481/2000\n",
            "1358/1358 [==============================] - 0s 91us/step - loss: 0.5016 - acc: 0.7297\n",
            "Epoch 1482/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5066 - acc: 0.7423\n",
            "Epoch 1483/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.4949 - acc: 0.7386\n",
            "Epoch 1484/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5034 - acc: 0.7327\n",
            "Epoch 1485/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5085 - acc: 0.7297\n",
            "Epoch 1486/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5122 - acc: 0.7386\n",
            "Epoch 1487/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5066 - acc: 0.7239\n",
            "Epoch 1488/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5164 - acc: 0.7320\n",
            "Epoch 1489/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5198 - acc: 0.7194\n",
            "Epoch 1490/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5124 - acc: 0.7408\n",
            "Epoch 1491/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.4905 - acc: 0.7437\n",
            "Epoch 1492/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5094 - acc: 0.7194\n",
            "Epoch 1493/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5031 - acc: 0.7364\n",
            "Epoch 1494/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.4951 - acc: 0.7408\n",
            "Epoch 1495/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5208 - acc: 0.7312\n",
            "Epoch 1496/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5209 - acc: 0.7283\n",
            "Epoch 1497/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5067 - acc: 0.7261\n",
            "Epoch 1498/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5104 - acc: 0.7283\n",
            "Epoch 1499/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5065 - acc: 0.7378\n",
            "Epoch 1500/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.4986 - acc: 0.7349\n",
            "Epoch 1501/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5127 - acc: 0.7312\n",
            "Epoch 1502/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5175 - acc: 0.7342\n",
            "Epoch 1503/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5083 - acc: 0.7305\n",
            "Epoch 1504/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5075 - acc: 0.7371\n",
            "Epoch 1505/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5154 - acc: 0.7261\n",
            "Epoch 1506/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5168 - acc: 0.7187\n",
            "Epoch 1507/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5073 - acc: 0.7445\n",
            "Epoch 1508/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5121 - acc: 0.7401\n",
            "Epoch 1509/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.4914 - acc: 0.7607\n",
            "Epoch 1510/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5103 - acc: 0.7364\n",
            "Epoch 1511/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5104 - acc: 0.7231\n",
            "Epoch 1512/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5139 - acc: 0.7261\n",
            "Epoch 1513/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5194 - acc: 0.7349\n",
            "Epoch 1514/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5048 - acc: 0.7437\n",
            "Epoch 1515/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5296 - acc: 0.7246\n",
            "Epoch 1516/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5100 - acc: 0.7312\n",
            "Epoch 1517/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5057 - acc: 0.7312\n",
            "Epoch 1518/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.4914 - acc: 0.7452\n",
            "Epoch 1519/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5178 - acc: 0.7261\n",
            "Epoch 1520/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5205 - acc: 0.7239\n",
            "Epoch 1521/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5225 - acc: 0.7290\n",
            "Epoch 1522/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.4988 - acc: 0.7423\n",
            "Epoch 1523/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5178 - acc: 0.7253\n",
            "Epoch 1524/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5221 - acc: 0.7342\n",
            "Epoch 1525/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5233 - acc: 0.7261\n",
            "Epoch 1526/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5217 - acc: 0.7231\n",
            "Epoch 1527/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5209 - acc: 0.7356\n",
            "Epoch 1528/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5202 - acc: 0.7268\n",
            "Epoch 1529/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5097 - acc: 0.7290\n",
            "Epoch 1530/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5121 - acc: 0.7356\n",
            "Epoch 1531/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5070 - acc: 0.7356\n",
            "Epoch 1532/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5110 - acc: 0.7312\n",
            "Epoch 1533/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5083 - acc: 0.7305\n",
            "Epoch 1534/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5140 - acc: 0.7378\n",
            "Epoch 1535/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5169 - acc: 0.7437\n",
            "Epoch 1536/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5179 - acc: 0.7312\n",
            "Epoch 1537/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5098 - acc: 0.7452\n",
            "Epoch 1538/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5043 - acc: 0.7342\n",
            "Epoch 1539/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5097 - acc: 0.7452\n",
            "Epoch 1540/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5204 - acc: 0.7393\n",
            "Epoch 1541/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5176 - acc: 0.7371\n",
            "Epoch 1542/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5189 - acc: 0.7261\n",
            "Epoch 1543/2000\n",
            "1358/1358 [==============================] - 0s 93us/step - loss: 0.5163 - acc: 0.7327\n",
            "Epoch 1544/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5138 - acc: 0.7349\n",
            "Epoch 1545/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5085 - acc: 0.7297\n",
            "Epoch 1546/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5162 - acc: 0.7187\n",
            "Epoch 1547/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.4923 - acc: 0.7423\n",
            "Epoch 1548/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5168 - acc: 0.7261\n",
            "Epoch 1549/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5135 - acc: 0.7320\n",
            "Epoch 1550/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5065 - acc: 0.7334\n",
            "Epoch 1551/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5012 - acc: 0.7445\n",
            "Epoch 1552/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5089 - acc: 0.7290\n",
            "Epoch 1553/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5183 - acc: 0.7239\n",
            "Epoch 1554/2000\n",
            "1358/1358 [==============================] - 0s 96us/step - loss: 0.5039 - acc: 0.7452\n",
            "Epoch 1555/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5072 - acc: 0.7327\n",
            "Epoch 1556/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.4999 - acc: 0.7415\n",
            "Epoch 1557/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5061 - acc: 0.7386\n",
            "Epoch 1558/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.4982 - acc: 0.7518\n",
            "Epoch 1559/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5096 - acc: 0.7356\n",
            "Epoch 1560/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.4937 - acc: 0.7467\n",
            "Epoch 1561/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5003 - acc: 0.7386\n",
            "Epoch 1562/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5208 - acc: 0.7283\n",
            "Epoch 1563/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5213 - acc: 0.7371\n",
            "Epoch 1564/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.4865 - acc: 0.7489\n",
            "Epoch 1565/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.4989 - acc: 0.7452\n",
            "Epoch 1566/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5016 - acc: 0.7327\n",
            "Epoch 1567/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5154 - acc: 0.7386\n",
            "Epoch 1568/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5195 - acc: 0.7437\n",
            "Epoch 1569/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.4993 - acc: 0.7386\n",
            "Epoch 1570/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5093 - acc: 0.7378\n",
            "Epoch 1571/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5109 - acc: 0.7378\n",
            "Epoch 1572/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5161 - acc: 0.7356\n",
            "Epoch 1573/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5065 - acc: 0.7393\n",
            "Epoch 1574/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5078 - acc: 0.7239\n",
            "Epoch 1575/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5030 - acc: 0.7459\n",
            "Epoch 1576/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5354 - acc: 0.7202\n",
            "Epoch 1577/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5172 - acc: 0.7342\n",
            "Epoch 1578/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5158 - acc: 0.7327\n",
            "Epoch 1579/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5206 - acc: 0.7305\n",
            "Epoch 1580/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5120 - acc: 0.7393\n",
            "Epoch 1581/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5042 - acc: 0.7526\n",
            "Epoch 1582/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5027 - acc: 0.7268\n",
            "Epoch 1583/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5186 - acc: 0.7452\n",
            "Epoch 1584/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5050 - acc: 0.7408\n",
            "Epoch 1585/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5033 - acc: 0.7290\n",
            "Epoch 1586/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5106 - acc: 0.7239\n",
            "Epoch 1587/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5213 - acc: 0.7467\n",
            "Epoch 1588/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5148 - acc: 0.7297\n",
            "Epoch 1589/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5192 - acc: 0.7239\n",
            "Epoch 1590/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.4958 - acc: 0.7526\n",
            "Epoch 1591/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5080 - acc: 0.7305\n",
            "Epoch 1592/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5029 - acc: 0.7437\n",
            "Epoch 1593/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5077 - acc: 0.7401\n",
            "Epoch 1594/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5198 - acc: 0.7231\n",
            "Epoch 1595/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5130 - acc: 0.7356\n",
            "Epoch 1596/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5105 - acc: 0.7239\n",
            "Epoch 1597/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5096 - acc: 0.7386\n",
            "Epoch 1598/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5185 - acc: 0.7349\n",
            "Epoch 1599/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5038 - acc: 0.7356\n",
            "Epoch 1600/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5104 - acc: 0.7209\n",
            "Epoch 1601/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5130 - acc: 0.7224\n",
            "Epoch 1602/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5030 - acc: 0.7408\n",
            "Epoch 1603/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5300 - acc: 0.7209\n",
            "Epoch 1604/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5139 - acc: 0.7180\n",
            "Epoch 1605/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5097 - acc: 0.7364\n",
            "Epoch 1606/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5159 - acc: 0.7268\n",
            "Epoch 1607/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5083 - acc: 0.7320\n",
            "Epoch 1608/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5185 - acc: 0.7297\n",
            "Epoch 1609/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5031 - acc: 0.7482\n",
            "Epoch 1610/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.4925 - acc: 0.7504\n",
            "Epoch 1611/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5045 - acc: 0.7342\n",
            "Epoch 1612/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5224 - acc: 0.7320\n",
            "Epoch 1613/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.4923 - acc: 0.7445\n",
            "Epoch 1614/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5111 - acc: 0.7209\n",
            "Epoch 1615/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5156 - acc: 0.7320\n",
            "Epoch 1616/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5024 - acc: 0.7320\n",
            "Epoch 1617/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5169 - acc: 0.7327\n",
            "Epoch 1618/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.4995 - acc: 0.7312\n",
            "Epoch 1619/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5169 - acc: 0.7423\n",
            "Epoch 1620/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5197 - acc: 0.7297\n",
            "Epoch 1621/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.4985 - acc: 0.7349\n",
            "Epoch 1622/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5079 - acc: 0.7349\n",
            "Epoch 1623/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5135 - acc: 0.7452\n",
            "Epoch 1624/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5024 - acc: 0.7393\n",
            "Epoch 1625/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.4938 - acc: 0.7511\n",
            "Epoch 1626/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.4998 - acc: 0.7305\n",
            "Epoch 1627/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5039 - acc: 0.7378\n",
            "Epoch 1628/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5138 - acc: 0.7275\n",
            "Epoch 1629/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5124 - acc: 0.7349\n",
            "Epoch 1630/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.4969 - acc: 0.7474\n",
            "Epoch 1631/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5072 - acc: 0.7423\n",
            "Epoch 1632/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5276 - acc: 0.7231\n",
            "Epoch 1633/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5103 - acc: 0.7364\n",
            "Epoch 1634/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5089 - acc: 0.7378\n",
            "Epoch 1635/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5061 - acc: 0.7445\n",
            "Epoch 1636/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5157 - acc: 0.7482\n",
            "Epoch 1637/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5197 - acc: 0.7305\n",
            "Epoch 1638/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.4981 - acc: 0.7393\n",
            "Epoch 1639/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5030 - acc: 0.7320\n",
            "Epoch 1640/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5151 - acc: 0.7275\n",
            "Epoch 1641/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5085 - acc: 0.7253\n",
            "Epoch 1642/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5154 - acc: 0.7401\n",
            "Epoch 1643/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5099 - acc: 0.7430\n",
            "Epoch 1644/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5062 - acc: 0.7482\n",
            "Epoch 1645/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5048 - acc: 0.7320\n",
            "Epoch 1646/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.4943 - acc: 0.7445\n",
            "Epoch 1647/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.4991 - acc: 0.7386\n",
            "Epoch 1648/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5102 - acc: 0.7371\n",
            "Epoch 1649/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5029 - acc: 0.7408\n",
            "Epoch 1650/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5107 - acc: 0.7526\n",
            "Epoch 1651/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.4948 - acc: 0.7467\n",
            "Epoch 1652/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5053 - acc: 0.7364\n",
            "Epoch 1653/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5119 - acc: 0.7297\n",
            "Epoch 1654/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5025 - acc: 0.7408\n",
            "Epoch 1655/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5217 - acc: 0.7239\n",
            "Epoch 1656/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5142 - acc: 0.7349\n",
            "Epoch 1657/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5167 - acc: 0.7261\n",
            "Epoch 1658/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5057 - acc: 0.7401\n",
            "Epoch 1659/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5045 - acc: 0.7401\n",
            "Epoch 1660/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5141 - acc: 0.7349\n",
            "Epoch 1661/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5193 - acc: 0.7239\n",
            "Epoch 1662/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5113 - acc: 0.7268\n",
            "Epoch 1663/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5193 - acc: 0.7334\n",
            "Epoch 1664/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5157 - acc: 0.7327\n",
            "Epoch 1665/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5024 - acc: 0.7378\n",
            "Epoch 1666/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5040 - acc: 0.7393\n",
            "Epoch 1667/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5085 - acc: 0.7430\n",
            "Epoch 1668/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5056 - acc: 0.7408\n",
            "Epoch 1669/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5131 - acc: 0.7386\n",
            "Epoch 1670/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5035 - acc: 0.7312\n",
            "Epoch 1671/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5098 - acc: 0.7401\n",
            "Epoch 1672/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5091 - acc: 0.7187\n",
            "Epoch 1673/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5151 - acc: 0.7334\n",
            "Epoch 1674/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5090 - acc: 0.7334\n",
            "Epoch 1675/2000\n",
            "1358/1358 [==============================] - 0s 90us/step - loss: 0.5132 - acc: 0.7320\n",
            "Epoch 1676/2000\n",
            "1358/1358 [==============================] - 0s 90us/step - loss: 0.5169 - acc: 0.7275\n",
            "Epoch 1677/2000\n",
            "1358/1358 [==============================] - 0s 90us/step - loss: 0.5088 - acc: 0.7349\n",
            "Epoch 1678/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5136 - acc: 0.7297\n",
            "Epoch 1679/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5049 - acc: 0.7415\n",
            "Epoch 1680/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5118 - acc: 0.7297\n",
            "Epoch 1681/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5213 - acc: 0.7371\n",
            "Epoch 1682/2000\n",
            "1358/1358 [==============================] - 0s 92us/step - loss: 0.5034 - acc: 0.7283\n",
            "Epoch 1683/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5138 - acc: 0.7408\n",
            "Epoch 1684/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.4937 - acc: 0.7327\n",
            "Epoch 1685/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5025 - acc: 0.7386\n",
            "Epoch 1686/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5025 - acc: 0.7364\n",
            "Epoch 1687/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5135 - acc: 0.7445\n",
            "Epoch 1688/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5177 - acc: 0.7283\n",
            "Epoch 1689/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5079 - acc: 0.7452\n",
            "Epoch 1690/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5026 - acc: 0.7482\n",
            "Epoch 1691/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5011 - acc: 0.7356\n",
            "Epoch 1692/2000\n",
            "1358/1358 [==============================] - 0s 92us/step - loss: 0.4977 - acc: 0.7467\n",
            "Epoch 1693/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5079 - acc: 0.7334\n",
            "Epoch 1694/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.4969 - acc: 0.7364\n",
            "Epoch 1695/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5067 - acc: 0.7393\n",
            "Epoch 1696/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5150 - acc: 0.7371\n",
            "Epoch 1697/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5222 - acc: 0.7327\n",
            "Epoch 1698/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5123 - acc: 0.7452\n",
            "Epoch 1699/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5098 - acc: 0.7445\n",
            "Epoch 1700/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5044 - acc: 0.7342\n",
            "Epoch 1701/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5018 - acc: 0.7386\n",
            "Epoch 1702/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5083 - acc: 0.7305\n",
            "Epoch 1703/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5130 - acc: 0.7312\n",
            "Epoch 1704/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5108 - acc: 0.7342\n",
            "Epoch 1705/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5150 - acc: 0.7349\n",
            "Epoch 1706/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.4977 - acc: 0.7541\n",
            "Epoch 1707/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5114 - acc: 0.7334\n",
            "Epoch 1708/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5094 - acc: 0.7364\n",
            "Epoch 1709/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5150 - acc: 0.7401\n",
            "Epoch 1710/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5143 - acc: 0.7283\n",
            "Epoch 1711/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5222 - acc: 0.7231\n",
            "Epoch 1712/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5143 - acc: 0.7445\n",
            "Epoch 1713/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5041 - acc: 0.7305\n",
            "Epoch 1714/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5145 - acc: 0.7320\n",
            "Epoch 1715/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5012 - acc: 0.7342\n",
            "Epoch 1716/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5117 - acc: 0.7356\n",
            "Epoch 1717/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5028 - acc: 0.7393\n",
            "Epoch 1718/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5213 - acc: 0.7356\n",
            "Epoch 1719/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5140 - acc: 0.7415\n",
            "Epoch 1720/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5073 - acc: 0.7378\n",
            "Epoch 1721/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5053 - acc: 0.7327\n",
            "Epoch 1722/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5133 - acc: 0.7253\n",
            "Epoch 1723/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5054 - acc: 0.7327\n",
            "Epoch 1724/2000\n",
            "1358/1358 [==============================] - 0s 68us/step - loss: 0.5137 - acc: 0.7312\n",
            "Epoch 1725/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.4985 - acc: 0.7312\n",
            "Epoch 1726/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5064 - acc: 0.7386\n",
            "Epoch 1727/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.4986 - acc: 0.7371\n",
            "Epoch 1728/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5141 - acc: 0.7268\n",
            "Epoch 1729/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5147 - acc: 0.7408\n",
            "Epoch 1730/2000\n",
            "1358/1358 [==============================] - 0s 92us/step - loss: 0.5038 - acc: 0.7283\n",
            "Epoch 1731/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5082 - acc: 0.7401\n",
            "Epoch 1732/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5209 - acc: 0.7312\n",
            "Epoch 1733/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5152 - acc: 0.7408\n",
            "Epoch 1734/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5100 - acc: 0.7283\n",
            "Epoch 1735/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5060 - acc: 0.7364\n",
            "Epoch 1736/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5013 - acc: 0.7393\n",
            "Epoch 1737/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5109 - acc: 0.7275\n",
            "Epoch 1738/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5166 - acc: 0.7283\n",
            "Epoch 1739/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5127 - acc: 0.7371\n",
            "Epoch 1740/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5229 - acc: 0.7415\n",
            "Epoch 1741/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5228 - acc: 0.7305\n",
            "Epoch 1742/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5086 - acc: 0.7393\n",
            "Epoch 1743/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.4998 - acc: 0.7356\n",
            "Epoch 1744/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5060 - acc: 0.7364\n",
            "Epoch 1745/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5088 - acc: 0.7356\n",
            "Epoch 1746/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5046 - acc: 0.7297\n",
            "Epoch 1747/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5080 - acc: 0.7371\n",
            "Epoch 1748/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5002 - acc: 0.7297\n",
            "Epoch 1749/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5082 - acc: 0.7467\n",
            "Epoch 1750/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.4995 - acc: 0.7401\n",
            "Epoch 1751/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5057 - acc: 0.7290\n",
            "Epoch 1752/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5029 - acc: 0.7401\n",
            "Epoch 1753/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5104 - acc: 0.7467\n",
            "Epoch 1754/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5234 - acc: 0.7474\n",
            "Epoch 1755/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5221 - acc: 0.7261\n",
            "Epoch 1756/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5048 - acc: 0.7437\n",
            "Epoch 1757/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5071 - acc: 0.7334\n",
            "Epoch 1758/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5163 - acc: 0.7467\n",
            "Epoch 1759/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5063 - acc: 0.7283\n",
            "Epoch 1760/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5124 - acc: 0.7378\n",
            "Epoch 1761/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5043 - acc: 0.7275\n",
            "Epoch 1762/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5032 - acc: 0.7467\n",
            "Epoch 1763/2000\n",
            "1358/1358 [==============================] - 0s 106us/step - loss: 0.5091 - acc: 0.7231\n",
            "Epoch 1764/2000\n",
            "1358/1358 [==============================] - 0s 122us/step - loss: 0.5166 - acc: 0.7378\n",
            "Epoch 1765/2000\n",
            "1358/1358 [==============================] - 0s 97us/step - loss: 0.5085 - acc: 0.7430\n",
            "Epoch 1766/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5151 - acc: 0.7415\n",
            "Epoch 1767/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5106 - acc: 0.7261\n",
            "Epoch 1768/2000\n",
            "1358/1358 [==============================] - 0s 107us/step - loss: 0.5064 - acc: 0.7393\n",
            "Epoch 1769/2000\n",
            "1358/1358 [==============================] - 0s 102us/step - loss: 0.4955 - acc: 0.7445\n",
            "Epoch 1770/2000\n",
            "1358/1358 [==============================] - 0s 99us/step - loss: 0.5134 - acc: 0.7356\n",
            "Epoch 1771/2000\n",
            "1358/1358 [==============================] - 0s 111us/step - loss: 0.5151 - acc: 0.7268\n",
            "Epoch 1772/2000\n",
            "1358/1358 [==============================] - 0s 94us/step - loss: 0.5085 - acc: 0.7364\n",
            "Epoch 1773/2000\n",
            "1358/1358 [==============================] - 0s 104us/step - loss: 0.5033 - acc: 0.7312\n",
            "Epoch 1774/2000\n",
            "1358/1358 [==============================] - 0s 95us/step - loss: 0.5082 - acc: 0.7489\n",
            "Epoch 1775/2000\n",
            "1358/1358 [==============================] - 0s 97us/step - loss: 0.5134 - acc: 0.7334\n",
            "Epoch 1776/2000\n",
            "1358/1358 [==============================] - 0s 100us/step - loss: 0.4981 - acc: 0.7401\n",
            "Epoch 1777/2000\n",
            "1358/1358 [==============================] - 0s 97us/step - loss: 0.5188 - acc: 0.7283\n",
            "Epoch 1778/2000\n",
            "1358/1358 [==============================] - 0s 107us/step - loss: 0.5068 - acc: 0.7342\n",
            "Epoch 1779/2000\n",
            "1358/1358 [==============================] - 0s 106us/step - loss: 0.5100 - acc: 0.7320\n",
            "Epoch 1780/2000\n",
            "1358/1358 [==============================] - 0s 99us/step - loss: 0.4902 - acc: 0.7526\n",
            "Epoch 1781/2000\n",
            "1358/1358 [==============================] - 0s 100us/step - loss: 0.5110 - acc: 0.7467\n",
            "Epoch 1782/2000\n",
            "1358/1358 [==============================] - 0s 92us/step - loss: 0.5093 - acc: 0.7320\n",
            "Epoch 1783/2000\n",
            "1358/1358 [==============================] - 0s 98us/step - loss: 0.5061 - acc: 0.7320\n",
            "Epoch 1784/2000\n",
            "1358/1358 [==============================] - 0s 92us/step - loss: 0.5019 - acc: 0.7401\n",
            "Epoch 1785/2000\n",
            "1358/1358 [==============================] - 0s 101us/step - loss: 0.5137 - acc: 0.7342\n",
            "Epoch 1786/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5092 - acc: 0.7386\n",
            "Epoch 1787/2000\n",
            "1358/1358 [==============================] - 0s 101us/step - loss: 0.5241 - acc: 0.7430\n",
            "Epoch 1788/2000\n",
            "1358/1358 [==============================] - 0s 93us/step - loss: 0.5023 - acc: 0.7349\n",
            "Epoch 1789/2000\n",
            "1358/1358 [==============================] - 0s 99us/step - loss: 0.4934 - acc: 0.7423\n",
            "Epoch 1790/2000\n",
            "1358/1358 [==============================] - 0s 91us/step - loss: 0.4901 - acc: 0.7437\n",
            "Epoch 1791/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5209 - acc: 0.7378\n",
            "Epoch 1792/2000\n",
            "1358/1358 [==============================] - 0s 96us/step - loss: 0.5198 - acc: 0.7386\n",
            "Epoch 1793/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5272 - acc: 0.7290\n",
            "Epoch 1794/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5020 - acc: 0.7423\n",
            "Epoch 1795/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5052 - acc: 0.7165\n",
            "Epoch 1796/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5090 - acc: 0.7437\n",
            "Epoch 1797/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5106 - acc: 0.7430\n",
            "Epoch 1798/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5086 - acc: 0.7364\n",
            "Epoch 1799/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5078 - acc: 0.7445\n",
            "Epoch 1800/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5069 - acc: 0.7371\n",
            "Epoch 1801/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5137 - acc: 0.7283\n",
            "Epoch 1802/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5118 - acc: 0.7378\n",
            "Epoch 1803/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5027 - acc: 0.7342\n",
            "Epoch 1804/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5087 - acc: 0.7415\n",
            "Epoch 1805/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5283 - acc: 0.7231\n",
            "Epoch 1806/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5120 - acc: 0.7342\n",
            "Epoch 1807/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5096 - acc: 0.7356\n",
            "Epoch 1808/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5133 - acc: 0.7423\n",
            "Epoch 1809/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.4990 - acc: 0.7386\n",
            "Epoch 1810/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5038 - acc: 0.7496\n",
            "Epoch 1811/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5095 - acc: 0.7371\n",
            "Epoch 1812/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.4905 - acc: 0.7401\n",
            "Epoch 1813/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5019 - acc: 0.7356\n",
            "Epoch 1814/2000\n",
            "1358/1358 [==============================] - 0s 93us/step - loss: 0.4998 - acc: 0.7401\n",
            "Epoch 1815/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5218 - acc: 0.7334\n",
            "Epoch 1816/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5056 - acc: 0.7430\n",
            "Epoch 1817/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5118 - acc: 0.7334\n",
            "Epoch 1818/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5141 - acc: 0.7283\n",
            "Epoch 1819/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5013 - acc: 0.7423\n",
            "Epoch 1820/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5064 - acc: 0.7334\n",
            "Epoch 1821/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5034 - acc: 0.7371\n",
            "Epoch 1822/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.4964 - acc: 0.7415\n",
            "Epoch 1823/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.4989 - acc: 0.7430\n",
            "Epoch 1824/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5107 - acc: 0.7445\n",
            "Epoch 1825/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.4977 - acc: 0.7408\n",
            "Epoch 1826/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5046 - acc: 0.7364\n",
            "Epoch 1827/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5125 - acc: 0.7312\n",
            "Epoch 1828/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5100 - acc: 0.7305\n",
            "Epoch 1829/2000\n",
            "1358/1358 [==============================] - 0s 90us/step - loss: 0.5002 - acc: 0.7526\n",
            "Epoch 1830/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.4994 - acc: 0.7290\n",
            "Epoch 1831/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5099 - acc: 0.7334\n",
            "Epoch 1832/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5116 - acc: 0.7401\n",
            "Epoch 1833/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5054 - acc: 0.7408\n",
            "Epoch 1834/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5178 - acc: 0.7334\n",
            "Epoch 1835/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5090 - acc: 0.7533\n",
            "Epoch 1836/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5049 - acc: 0.7356\n",
            "Epoch 1837/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5103 - acc: 0.7327\n",
            "Epoch 1838/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5203 - acc: 0.7386\n",
            "Epoch 1839/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.4911 - acc: 0.7459\n",
            "Epoch 1840/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5201 - acc: 0.7393\n",
            "Epoch 1841/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5054 - acc: 0.7327\n",
            "Epoch 1842/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5050 - acc: 0.7408\n",
            "Epoch 1843/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5173 - acc: 0.7364\n",
            "Epoch 1844/2000\n",
            "1358/1358 [==============================] - 0s 90us/step - loss: 0.4950 - acc: 0.7526\n",
            "Epoch 1845/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5118 - acc: 0.7334\n",
            "Epoch 1846/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5076 - acc: 0.7430\n",
            "Epoch 1847/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5079 - acc: 0.7378\n",
            "Epoch 1848/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5034 - acc: 0.7408\n",
            "Epoch 1849/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.4972 - acc: 0.7541\n",
            "Epoch 1850/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5104 - acc: 0.7437\n",
            "Epoch 1851/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5013 - acc: 0.7253\n",
            "Epoch 1852/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5036 - acc: 0.7511\n",
            "Epoch 1853/2000\n",
            "1358/1358 [==============================] - 0s 93us/step - loss: 0.5115 - acc: 0.7342\n",
            "Epoch 1854/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5058 - acc: 0.7511\n",
            "Epoch 1855/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5061 - acc: 0.7275\n",
            "Epoch 1856/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.4980 - acc: 0.7555\n",
            "Epoch 1857/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5020 - acc: 0.7349\n",
            "Epoch 1858/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.4974 - acc: 0.7430\n",
            "Epoch 1859/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5006 - acc: 0.7437\n",
            "Epoch 1860/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5138 - acc: 0.7305\n",
            "Epoch 1861/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5061 - acc: 0.7511\n",
            "Epoch 1862/2000\n",
            "1358/1358 [==============================] - 0s 90us/step - loss: 0.5212 - acc: 0.7209\n",
            "Epoch 1863/2000\n",
            "1358/1358 [==============================] - 0s 91us/step - loss: 0.5127 - acc: 0.7364\n",
            "Epoch 1864/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5239 - acc: 0.7231\n",
            "Epoch 1865/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.4940 - acc: 0.7430\n",
            "Epoch 1866/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5252 - acc: 0.7246\n",
            "Epoch 1867/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5101 - acc: 0.7371\n",
            "Epoch 1868/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5154 - acc: 0.7349\n",
            "Epoch 1869/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5046 - acc: 0.7415\n",
            "Epoch 1870/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5019 - acc: 0.7437\n",
            "Epoch 1871/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5072 - acc: 0.7246\n",
            "Epoch 1872/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5133 - acc: 0.7283\n",
            "Epoch 1873/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5046 - acc: 0.7356\n",
            "Epoch 1874/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5075 - acc: 0.7364\n",
            "Epoch 1875/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5041 - acc: 0.7401\n",
            "Epoch 1876/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5258 - acc: 0.7224\n",
            "Epoch 1877/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5019 - acc: 0.7408\n",
            "Epoch 1878/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5082 - acc: 0.7386\n",
            "Epoch 1879/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5002 - acc: 0.7364\n",
            "Epoch 1880/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5140 - acc: 0.7459\n",
            "Epoch 1881/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5029 - acc: 0.7430\n",
            "Epoch 1882/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5003 - acc: 0.7504\n",
            "Epoch 1883/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.4979 - acc: 0.7371\n",
            "Epoch 1884/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5031 - acc: 0.7423\n",
            "Epoch 1885/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5034 - acc: 0.7533\n",
            "Epoch 1886/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5056 - acc: 0.7378\n",
            "Epoch 1887/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5066 - acc: 0.7430\n",
            "Epoch 1888/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5026 - acc: 0.7437\n",
            "Epoch 1889/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5185 - acc: 0.7327\n",
            "Epoch 1890/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5016 - acc: 0.7518\n",
            "Epoch 1891/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5091 - acc: 0.7349\n",
            "Epoch 1892/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5103 - acc: 0.7305\n",
            "Epoch 1893/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5051 - acc: 0.7371\n",
            "Epoch 1894/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5144 - acc: 0.7312\n",
            "Epoch 1895/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5214 - acc: 0.7312\n",
            "Epoch 1896/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.4995 - acc: 0.7342\n",
            "Epoch 1897/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5135 - acc: 0.7430\n",
            "Epoch 1898/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5062 - acc: 0.7290\n",
            "Epoch 1899/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.4918 - acc: 0.7474\n",
            "Epoch 1900/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5129 - acc: 0.7231\n",
            "Epoch 1901/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5043 - acc: 0.7349\n",
            "Epoch 1902/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5092 - acc: 0.7548\n",
            "Epoch 1903/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.4971 - acc: 0.7401\n",
            "Epoch 1904/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.4941 - acc: 0.7430\n",
            "Epoch 1905/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5071 - acc: 0.7474\n",
            "Epoch 1906/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5323 - acc: 0.7224\n",
            "Epoch 1907/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5035 - acc: 0.7312\n",
            "Epoch 1908/2000\n",
            "1358/1358 [==============================] - 0s 90us/step - loss: 0.5191 - acc: 0.7393\n",
            "Epoch 1909/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5060 - acc: 0.7371\n",
            "Epoch 1910/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.4996 - acc: 0.7526\n",
            "Epoch 1911/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5073 - acc: 0.7378\n",
            "Epoch 1912/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.4996 - acc: 0.7378\n",
            "Epoch 1913/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.4963 - acc: 0.7415\n",
            "Epoch 1914/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5195 - acc: 0.7290\n",
            "Epoch 1915/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5108 - acc: 0.7312\n",
            "Epoch 1916/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.4996 - acc: 0.7334\n",
            "Epoch 1917/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.4978 - acc: 0.7474\n",
            "Epoch 1918/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.4978 - acc: 0.7526\n",
            "Epoch 1919/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5123 - acc: 0.7489\n",
            "Epoch 1920/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5226 - acc: 0.7261\n",
            "Epoch 1921/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5089 - acc: 0.7342\n",
            "Epoch 1922/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5103 - acc: 0.7378\n",
            "Epoch 1923/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.4999 - acc: 0.7467\n",
            "Epoch 1924/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5020 - acc: 0.7452\n",
            "Epoch 1925/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5169 - acc: 0.7320\n",
            "Epoch 1926/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5095 - acc: 0.7437\n",
            "Epoch 1927/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5214 - acc: 0.7194\n",
            "Epoch 1928/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5005 - acc: 0.7371\n",
            "Epoch 1929/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5000 - acc: 0.7452\n",
            "Epoch 1930/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5141 - acc: 0.7423\n",
            "Epoch 1931/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5043 - acc: 0.7415\n",
            "Epoch 1932/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5188 - acc: 0.7356\n",
            "Epoch 1933/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5136 - acc: 0.7393\n",
            "Epoch 1934/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5005 - acc: 0.7423\n",
            "Epoch 1935/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5082 - acc: 0.7526\n",
            "Epoch 1936/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.4982 - acc: 0.7437\n",
            "Epoch 1937/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.5159 - acc: 0.7297\n",
            "Epoch 1938/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5163 - acc: 0.7320\n",
            "Epoch 1939/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5246 - acc: 0.7349\n",
            "Epoch 1940/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5174 - acc: 0.7135\n",
            "Epoch 1941/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5106 - acc: 0.7393\n",
            "Epoch 1942/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5136 - acc: 0.7312\n",
            "Epoch 1943/2000\n",
            "1358/1358 [==============================] - 0s 88us/step - loss: 0.4901 - acc: 0.7401\n",
            "Epoch 1944/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5108 - acc: 0.7327\n",
            "Epoch 1945/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5071 - acc: 0.7386\n",
            "Epoch 1946/2000\n",
            "1358/1358 [==============================] - 0s 89us/step - loss: 0.5102 - acc: 0.7305\n",
            "Epoch 1947/2000\n",
            "1358/1358 [==============================] - 0s 81us/step - loss: 0.5065 - acc: 0.7320\n",
            "Epoch 1948/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5012 - acc: 0.7393\n",
            "Epoch 1949/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.4981 - acc: 0.7312\n",
            "Epoch 1950/2000\n",
            "1358/1358 [==============================] - 0s 80us/step - loss: 0.5033 - acc: 0.7290\n",
            "Epoch 1951/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.4953 - acc: 0.7342\n",
            "Epoch 1952/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.4882 - acc: 0.7563\n",
            "Epoch 1953/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5088 - acc: 0.7423\n",
            "Epoch 1954/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.5211 - acc: 0.7356\n",
            "Epoch 1955/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.5230 - acc: 0.7393\n",
            "Epoch 1956/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.5098 - acc: 0.7378\n",
            "Epoch 1957/2000\n",
            "1358/1358 [==============================] - 0s 70us/step - loss: 0.5006 - acc: 0.7378\n",
            "Epoch 1958/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5075 - acc: 0.7305\n",
            "Epoch 1959/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5116 - acc: 0.7268\n",
            "Epoch 1960/2000\n",
            "1358/1358 [==============================] - 0s 73us/step - loss: 0.5222 - acc: 0.7297\n",
            "Epoch 1961/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5189 - acc: 0.7305\n",
            "Epoch 1962/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5204 - acc: 0.7305\n",
            "Epoch 1963/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5048 - acc: 0.7401\n",
            "Epoch 1964/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5103 - acc: 0.7386\n",
            "Epoch 1965/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.4974 - acc: 0.7467\n",
            "Epoch 1966/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5074 - acc: 0.7430\n",
            "Epoch 1967/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.4960 - acc: 0.7526\n",
            "Epoch 1968/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5072 - acc: 0.7393\n",
            "Epoch 1969/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.4898 - acc: 0.7371\n",
            "Epoch 1970/2000\n",
            "1358/1358 [==============================] - 0s 86us/step - loss: 0.4964 - acc: 0.7423\n",
            "Epoch 1971/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5034 - acc: 0.7342\n",
            "Epoch 1972/2000\n",
            "1358/1358 [==============================] - 0s 79us/step - loss: 0.5099 - acc: 0.7334\n",
            "Epoch 1973/2000\n",
            "1358/1358 [==============================] - 0s 75us/step - loss: 0.5111 - acc: 0.7187\n",
            "Epoch 1974/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5093 - acc: 0.7297\n",
            "Epoch 1975/2000\n",
            "1358/1358 [==============================] - 0s 92us/step - loss: 0.5222 - acc: 0.7342\n",
            "Epoch 1976/2000\n",
            "1358/1358 [==============================] - 0s 69us/step - loss: 0.4907 - acc: 0.7401\n",
            "Epoch 1977/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5237 - acc: 0.7386\n",
            "Epoch 1978/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5131 - acc: 0.7231\n",
            "Epoch 1979/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5165 - acc: 0.7349\n",
            "Epoch 1980/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.4983 - acc: 0.7504\n",
            "Epoch 1981/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.4911 - acc: 0.7533\n",
            "Epoch 1982/2000\n",
            "1358/1358 [==============================] - 0s 84us/step - loss: 0.5171 - acc: 0.7327\n",
            "Epoch 1983/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.5072 - acc: 0.7474\n",
            "Epoch 1984/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5066 - acc: 0.7297\n",
            "Epoch 1985/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5079 - acc: 0.7474\n",
            "Epoch 1986/2000\n",
            "1358/1358 [==============================] - 0s 71us/step - loss: 0.5087 - acc: 0.7415\n",
            "Epoch 1987/2000\n",
            "1358/1358 [==============================] - 0s 72us/step - loss: 0.4903 - acc: 0.7474\n",
            "Epoch 1988/2000\n",
            "1358/1358 [==============================] - 0s 74us/step - loss: 0.4982 - acc: 0.7489\n",
            "Epoch 1989/2000\n",
            "1358/1358 [==============================] - 0s 83us/step - loss: 0.5093 - acc: 0.7393\n",
            "Epoch 1990/2000\n",
            "1358/1358 [==============================] - 0s 85us/step - loss: 0.5050 - acc: 0.7349\n",
            "Epoch 1991/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5028 - acc: 0.7349\n",
            "Epoch 1992/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.5097 - acc: 0.7437\n",
            "Epoch 1993/2000\n",
            "1358/1358 [==============================] - 0s 82us/step - loss: 0.4984 - acc: 0.7437\n",
            "Epoch 1994/2000\n",
            "1358/1358 [==============================] - 0s 87us/step - loss: 0.5054 - acc: 0.7356\n",
            "Epoch 1995/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.4952 - acc: 0.7437\n",
            "Epoch 1996/2000\n",
            "1358/1358 [==============================] - 0s 76us/step - loss: 0.5030 - acc: 0.7452\n",
            "Epoch 1997/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5200 - acc: 0.7320\n",
            "Epoch 1998/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5120 - acc: 0.7401\n",
            "Epoch 1999/2000\n",
            "1358/1358 [==============================] - 0s 77us/step - loss: 0.5092 - acc: 0.7474\n",
            "Epoch 2000/2000\n",
            "1358/1358 [==============================] - 0s 78us/step - loss: 0.5036 - acc: 0.7423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ho9WICAHvjBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing accuracy\n",
        "predict_prob_test1 = model.predict(X1_test.reshape(-1,13,1))\n",
        "test1_scores=np.argmax(predict_prob_test1,axis=1)\n",
        "(Y1_test==test1_scores).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2fI5Xu4wls2",
        "outputId": "2fb902a5-403e-441d-daa2-831bcb02bca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7935010482180294"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#validation accuracy\n",
        "predict_prob_val1 = model.predict(np.array(X1_val).reshape(-1,13,1))\n",
        "print(predict_prob_val1)\n",
        "val1_scores=np.argmax(predict_prob_val1,axis=1)\n",
        "(Y1_val==val1_scores).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMCOKmxKwnB3",
        "outputId": "f75e13fa-35d6-4176-b323-82e5bb26e400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.7494677  0.25053236]\n",
            " [0.67890656 0.32109347]\n",
            " [0.7086975  0.29130247]\n",
            " ...\n",
            " [0.08397299 0.916027  ]\n",
            " [0.64647907 0.35352084]\n",
            " [0.46493113 0.5350688 ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7422145328719724"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training accuracy\n",
        "predict_prob_train1 = model.predict(np.array(X1_train).reshape(-1,13,1))\n",
        "print(predict_prob_train1)\n",
        "train1_scores=np.argmax(predict_prob_train1,axis=1)\n",
        "(Y1_train==train1_scores).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z21IddJDy6D0",
        "outputId": "af7004c2-3547-467e-d190-9168a53ad24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.8065427  0.19345729]\n",
            " [0.70209867 0.29790133]\n",
            " [0.6367037  0.36329627]\n",
            " ...\n",
            " [0.73129565 0.2687044 ]\n",
            " [0.00478622 0.99521375]\n",
            " [0.00644024 0.9935597 ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7790868924889544"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(predict_prob_train1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9Z1DFxWztpJ",
        "outputId": "862eff41-b43b-48bf-c958-c6d857730f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_prob_train1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTWmw3av2qkw",
        "outputId": "9b6b9faa-cd74-494a-be2f-fe3c6ef6cdec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1358, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y1_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0o5E-Ne2tWH",
        "outputId": "036a63ce-c699-4c61-ef0e-63c5e7a5c2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1358,)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y1_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y9Myiak-BKC",
        "outputId": "c9cea5d4-8a86-4760-b7d9-b79ea8ee3d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(578,)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y1_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrkP8m9y_5fV",
        "outputId": "3b432955-6824-4a1f-f585-2b50242df182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(954,)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_prob_train1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgKgXe-u-4rr",
        "outputId": "49d21d42-42a3-411a-97b3-1f0696840e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1358, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_prob_test1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ILZ07el_V6Y",
        "outputId": "92a7c693-f745-41cf-88ad-7d0df09ffcbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(954, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_prob_val1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiR4teHCJnNq",
        "outputId": "43e89d92-1e87-4ca5-d538-12f20a26b416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(578, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_prob_val1.reshape(578,2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkLJnBVo_Y6I",
        "outputId": "968dc052-6f61-443c-8562-5a3dde683711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(578, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in predict_prob_val1.flatten():\n",
        "  print(i.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI45e382_fGu",
        "outputId": "1ef67a16-4ee7-420e-8a16-3444511b627b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n",
            "()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URj3LozGAEZC",
        "outputId": "bcecf75e-9983-4c85-d906-1b831ca880d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1358, 13, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yPgT1rOCJL5",
        "outputId": "3fe229c1-f5c6-473f-af60-ca73308fce45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2312, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1_val.to_numpy().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-DMwMIaCLgz",
        "outputId": "f6cbc059-4773-4b3e-dff7-30da76d38321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(578, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkUbnT9KCOTm",
        "outputId": "e1283ea2-8ec8-437b-b403-7794c2f40a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(578, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-layer Preceptron"
      ],
      "metadata": {
        "id": "0NfOdkNGz0Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "hMhN_jigz81N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLPClassifier()"
      ],
      "metadata": {
        "id": "4VnVSpYM0MQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2 = data.drop(['Unnamed: 0','Unnamed: 0.1.1','Unnamed: 0.1','final_tweets','label'],axis=1)\n",
        "y2 = data['label']"
      ],
      "metadata": {
        "id": "9aTVvGpT0oNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2,y2, test_size=0.33, random_state=14)"
      ],
      "metadata": {
        "id": "oUndEhTD0q7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2= X2.to_numpy()\n",
        "y2= y2.to_numpy() \n",
        "y2= y2.flatten()"
      ],
      "metadata": {
        "id": "IKgUJBCc1CTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5, random_state=25, shuffle=True)\n",
        "CV_score_array  =[]\n",
        "for train_index_2, test_index_2 in kf.split(X):\n",
        "    X2_train, X2_valid = X2[train_index], X2[test_index]\n",
        "    y2_train, y2_valid = y2[train_index], y2[test_index]\n",
        "    mlp=MLPClassifier(activation=\"relu\",alpha=0.00025,hidden_layer_sizes=(200,150,100,50,25),learning_rate='adaptive',solver='adam', max_iter=200, random_state=25)\n",
        "    mlp.fit(X2_train,y2_train)  "
      ],
      "metadata": {
        "id": "Y0nhSOOC0Pkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2_valid_pred=m.predict(X2_valid)"
      ],
      "metadata": {
        "id": "hVLVRRqM1Jlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2_valid_prob=m.predict_proba(X2_valid)"
      ],
      "metadata": {
        "id": "bRZFBxiH_8NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X2_valid_pred==y2_valid).mean()   #validation accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4kRQ8mV1Lh0",
        "outputId": "71bda7c8-9d84-4b18-ba2e-017c8789c980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7629757785467128"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X2_test_pred=m.predict(X2_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf0kevk21Xae",
        "outputId": "f9137150-e7ac-4025-9d06-ed264388e152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X2_test_prob=m.predict_proba(X2_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7giDSv9qAR_q",
        "outputId": "d236b4e8-f970-45c7-f27f-902ec1ccb6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X2_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU5cmwf0CVa8",
        "outputId": "c75cfc9d-ce8b-489d-adf0-4ef7b77d4d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(954, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X2_valid_test==y2_test).mean()    #test accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcJ17EvT1RFE",
        "outputId": "de04103b-84a7-4a0b-efdb-fa4bc10d1f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8438155136268344"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X2_train_pred=m.predict(X2_train) "
      ],
      "metadata": {
        "id": "NNXehxUN1ZqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hTOf91RaCPUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2_train_prob=m.predict_proba(X2_train) "
      ],
      "metadata": {
        "id": "Bg__sr_kAuSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X2_train_pred==y2_train).mean() #training accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQwHXmJV1f2k",
        "outputId": "f7f51061-b000-4d7c-f345-d612efee4122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8213667820069204"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OEBlHF_T_mvF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}